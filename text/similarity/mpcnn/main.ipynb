{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MPCNN_Final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ruQyuKKg8rqx",
        "colab_type": "code",
        "outputId": "8adff172-2166-4c69-b28d-0fa37db40f59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2253
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.0.0\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import abc\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "import codecs\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "from __future__ import print_function\n",
        "import time\n",
        "import os\n",
        "import datetime\n",
        "from tensorflow.python import debug as tf_debug\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "from sklearn.utils import shuffle\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import logging\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\",force_remount=True)\n",
        "\n",
        "\n",
        "\n",
        "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
        "    data = np.array(data)\n",
        "    data_size = len(data)\n",
        "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle the data at each epoch\n",
        "        if shuffle:\n",
        "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "            shuffled_data = data[shuffle_indices]\n",
        "        else:\n",
        "            shuffled_data = data\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "            yield shuffled_data[start_index:end_index]\n",
        "class Embedder(object):\n",
        "    \"\"\" Generic embedding interface.\n",
        "    Required:\n",
        "      * w: dict mapping tokens to indices\n",
        "      * g: matrix with one row per token index\n",
        "      * N: embedding dimensionality\n",
        "    \"\"\"\n",
        "\n",
        "    def map_tokens(self, tokens, ndim=2):\n",
        "        \"\"\" for the given list of tokens, return a list of GloVe embeddings,\n",
        "        or a single plain bag-of-words average embedding if ndim=1.\n",
        "        Unseen words (that's actually *very* rare) are mapped to 0-vectors. \"\"\"\n",
        "        gtokens = [self.g[self.w[t]] for t in tokens if t in self.w]\n",
        "        if not gtokens:\n",
        "            return np.zeros((1, self.N)) if ndim == 2 else np.zeros(self.N)\n",
        "        gtokens = np.array(gtokens)\n",
        "        if ndim == 2:\n",
        "            return gtokens\n",
        "        else:\n",
        "            return gtokens.mean(axis=0)\n",
        "\n",
        "    def map_set(self, ss, ndim=2):\n",
        "        \"\"\" apply map_tokens on a whole set of sentences \"\"\"\n",
        "        return [self.map_tokens(s, ndim=ndim) for s in ss]\n",
        "\n",
        "    def map_jset(self, sj):\n",
        "        \"\"\" for a set of sentence emb indices, get per-token embeddings \"\"\"\n",
        "        return self.g[sj]\n",
        "\n",
        "    def pad_set(self, ss, spad, N=None):\n",
        "        \"\"\" Given a set of sentences transformed to per-word embeddings\n",
        "        (using glove.map_set()), convert them to a 3D matrix with fixed\n",
        "        sentence sizes - padded or trimmed to spad embeddings per sentence.\n",
        "        Output is a tensor of shape (len(ss), spad, N).\n",
        "        To determine spad, use something like\n",
        "            np.sort([np.shape(s) for s in s0], axis=0)[-1000]\n",
        "        so that typically everything fits, but you don't go to absurd lengths\n",
        "        to accomodate outliers.\n",
        "        \"\"\"\n",
        "        ss2 = []\n",
        "        if N is None:\n",
        "            N = self.N\n",
        "        for s in ss:\n",
        "            if spad > s.shape[0]:\n",
        "                if s.ndim == 2:\n",
        "                    s = np.vstack((s, np.zeros((spad - s.shape[0], N))))\n",
        "                else:  # pad non-embeddings (e.g. toklabels) too\n",
        "                    s = np.hstack((s, np.zeros(spad - s.shape[0])))\n",
        "            elif spad < s.shape[0]:\n",
        "                s = s[:spad]\n",
        "            ss2.append(s)\n",
        "        return np.array(ss2)\n",
        "\n",
        "\n",
        "\n",
        "logger = logging.getLogger('mylogger')\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "  # 创建一个handler，用于写入日志文件\n",
        "timestamp = str(int(time.time()))\n",
        "fh = logging.FileHandler('./log_' + timestamp +'.txt')\n",
        "fh.setLevel(logging.DEBUG)\n",
        "\n",
        "  # 再创建一个handler，用于输出到控制台\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.DEBUG)\n",
        "\n",
        "  # 定义handler的输出格式\n",
        "formatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\n",
        "fh.setFormatter(formatter)\n",
        "ch.setFormatter(formatter)\n",
        "\n",
        "  # 给logger添加handler\n",
        "logger.addHandler(fh)\n",
        "logger.addHandler(ch)\n",
        "\n",
        "\n",
        "class GloVe(Embedder):\n",
        "    \"\"\" A GloVe dictionary and the associated N-dimensional vector space \"\"\"\n",
        "    def __init__(self, N=50, glovepath='/content/drive/My Drive/Colab Notebooks/MPCNN/glove.6B/glove.6B.%dd.txt'):\n",
        "        \"\"\" Load GloVe dictionary from the standard distributed text file.\n",
        "        Glovepath should contain %d, which is substituted for the embedding\n",
        "        dimension N. \"\"\"\n",
        "        self.N = N\n",
        "        self.w = dict()\n",
        "        self.g = []\n",
        "        self.glovepath = glovepath % (N,)\n",
        "\n",
        "        # [0] must be a zero vector\n",
        "        self.g.append(np.zeros(self.N))\n",
        "\n",
        "        with open(self.glovepath, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                l = line.split()\n",
        "                word = l[0]\n",
        "                self.w[word] = len(self.g)\n",
        "                self.g.append(np.array(l[1:]).astype(float))\n",
        "        self.w['UKNOW'] = len(self.g)\n",
        "        self.g.append(np.zeros(self.N))\n",
        "        self.g = np.array(self.g, dtype='float32')\n",
        "\n",
        "#class TextSemanticSimilarity1(TextSemanticSimilarity):\n",
        "class TextSemanticSimilarity(abc.ABC):  \n",
        "  \n",
        "  \n",
        "  def load_sts(self,dsfile, glove, skip_unlabeled=True):\n",
        "    \"\"\" load a dataset in the sts tsv format \"\"\"\n",
        "    s0 = []\n",
        "    s1 = []\n",
        "    labels = []\n",
        "    with codecs.open(dsfile, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            line = line.rstrip()\n",
        "            label, s0x, s1x = line.split('\\t')\n",
        "            if label == '':\n",
        "                continue\n",
        "            else:\n",
        "                score_int = int(round(float(label)))\n",
        "                y = [0] * 6\n",
        "                y[score_int] = 1\n",
        "                labels.append(np.array(y))\n",
        "            for i, ss in enumerate([s0x, s1x]):\n",
        "                words = word_tokenize(ss)\n",
        "                index = []\n",
        "                for word in words:\n",
        "                    word = word.lower()\n",
        "                    if word in glove.w:\n",
        "                        index.append(glove.w[word])\n",
        "                    else:\n",
        "                        index.append(glove.w['UKNOW'])\n",
        "                left = 100 - len(words)\n",
        "                pad = [0]*left\n",
        "                index.extend(pad)\n",
        "                if i == 0:\n",
        "                    s0.append(np.array(index))\n",
        "                else:\n",
        "                    s1.append(np.array(index))\n",
        "            #s0.append(word_tokenize(s0x))\n",
        "            #s1.append(word_tokenize(s1x))\n",
        "    #print(len(s0))\n",
        "    return (s0, s1, labels)\n",
        "  \n",
        "  def concat_datasets(self,datasets):\n",
        "    \"\"\" Concatenate multiple loaded datasets into a single large one.\n",
        "    Example: s0, s1, lab = concat_datasets([load_sts(d) for glob.glob('data/sts/semeval-sts/all/201[0-4]*')]) \"\"\"\n",
        "    s0 = []\n",
        "    s1 = []\n",
        "    labels = []\n",
        "    for s0x, s1x, labelsx in datasets:\n",
        "        s0 += s0x\n",
        "        s1 += s1x\n",
        "        labels += labelsx\n",
        "    return (np.array(s0), np.array(s1), np.array(labels))\n",
        "  \n",
        "  def load_embedded(self,glove, s0, s1, labels, ndim=0, s0pad=25, s1pad=60):\n",
        "    \"\"\" Post-process loaded (s0, s1, labels) by mapping it to embeddings,\n",
        "    plus optionally balancing (if labels are binary) and optionally not\n",
        "    averaging but padding and returning full-sequence matrices.\n",
        "    Note that this is now deprecated, especially if you use Keras - use the\n",
        "    vocab.Vocabulary class. \"\"\"\n",
        "\n",
        "    if ndim == 1:\n",
        "        # for averaging:\n",
        "        e0 = np.array(glove.map_set(s0, ndim=1))\n",
        "        e1 = np.array(glove.map_set(s1, ndim=1))\n",
        "    else:\n",
        "        # for padding and sequences (e.g. keras RNNs):\n",
        "        # print('(%s) s0[-1000]: %d tokens' % (globmask, np.sort([np.shape(s) for s in s0], axis=0)[-1000]))\n",
        "        # print('(%s) s1[-1000]: %d tokens' % (globmask, np.sort([np.shape(s) for s in s1], axis=0)[-1000]))\n",
        "        e0 = glove.pad_set(glove.map_set(s0), s0pad)\n",
        "        e1 = glove.pad_set(glove.map_set(s1), s1pad)\n",
        "    return (e0, e1, s0, s1, labels)\n",
        "\n",
        "  def load_set(self,glove, path):\n",
        "    files = []\n",
        "    for file in os.listdir(path):\n",
        "        if os.path.isfile(path + '/' + file):\n",
        "            files.append(path + '/' + file)\n",
        "    s0, s1, labels = self.concat_datasets([self.load_sts(d, glove) for d in files])\n",
        "    #s0, s1, labels = np.array(s0), np.array(s1), np.array(labels)\n",
        "    print('(%s) Loaded dataset: %d' % (path, len(s0)))\n",
        "    #e0, e1, s0, s1, labels = load_embedded(glove, s0, s1, labels)\n",
        "    return ([s0, s1], labels)\n",
        "  \n",
        "  def get_embedding(self):\n",
        "    gfile_path = os.path.join(\"/content/drive/My Drive/Colab Notebooks/MPCNN/glove.6B\", \"glove.6B.300d.txt\")\n",
        "    f = open(gfile_path, 'r')\n",
        "    embeddings = {}\n",
        "    for line in f:\n",
        "        sp_value = line.split()\n",
        "        word = sp_value[0]\n",
        "        embedding = [float(value) for value in sp_value[1:]]\n",
        "        embeddings[word] = embedding\n",
        "    print(\"read word2vec finished!\")\n",
        "    f.close()\n",
        "    return embeddings\n",
        "  \n",
        "  \n",
        "  \n",
        "   \n",
        "\n",
        "  \n",
        "  def get_cosine(self,vec1, vec2):\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
        "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "\n",
        "  def text_to_vector(self,text):\n",
        "      word = re.compile(r'\\w+')\n",
        "      words = word.findall(text)\n",
        "      return Counter(words)\n",
        "    \n",
        "  \n",
        "    \n",
        "  def __init__(self):\n",
        "    \n",
        "    pass\n",
        "  \n",
        "  \n",
        "    \n",
        "  #@abc.abstractmethod\n",
        "  def read_dataset(self, file_name):\n",
        "    \"\"\"\n",
        "\t\tReads a dataset that is a CSV/Excel File.\n",
        "\t\tArgs:\n",
        "\t\t\tfile_name : With it's absolute path\n",
        "\t\tReturns:\n",
        "\t\t\ttraining_data_list : List of Lists that containes 2 sentences and it's similarity score \n",
        "\t\t\tNote :\n",
        "\t\t\t\tFormat of the output : [[S1,S2,Sim_score],[T1,T2,Sim_score]....]\n",
        "\t\tRaises:\n",
        "\t\t\tNone\n",
        "\t\t\"\"\"\n",
        "    X,Y=self.load_set(glove, path=file_name)\n",
        "    return X,Y\n",
        "    \n",
        "\n",
        "  #@abc.abstractmethod\n",
        "  def train(self, Xtrain, ytrain,Xtest, ytest):\n",
        "    tf.app.flags.DEFINE_integer('embedding_dim', 100, 'The dimension of the word embedding')\n",
        "    tf.app.flags.DEFINE_integer('num_filters_A', 50, 'The number of filters in block A')\n",
        "    tf.app.flags.DEFINE_integer('num_filters_B', 50, 'The number of filters in block B')\n",
        "    tf.app.flags.DEFINE_integer('n_hidden', 150, 'number of hidden units in the fully connected layer')#150\n",
        "    tf.app.flags.DEFINE_integer('sentence_length', 100, 'max size of sentence')\n",
        "    tf.app.flags.DEFINE_integer('num_classes', 6, 'num of the labels')\n",
        "    tf.flags.DEFINE_float(\"l2_reg_lambda\", 1, \"L2 regularization lambda (default: 0.0)\")\n",
        "\n",
        "    tf.app.flags.DEFINE_integer('num_epochs', 85, 'Number of epochs to be trained')#85\n",
        "    tf.app.flags.DEFINE_integer('batch_size', 64, 'size of mini batch')#64\n",
        "\n",
        "    tf.app.flags.DEFINE_integer(\"display_step\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
        "    tf.app.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
        "    tf.app.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
        "    tf.app.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
        "\n",
        "    tf.app.flags.DEFINE_float('lr', 1e-3, 'learning rate')\n",
        "\n",
        "    tf.app.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
        "    tf.app.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
        "    filter_size = [1, 2, 100]\n",
        "    conf = tf.app.flags.FLAGS\n",
        "    conf._parse_flags()\n",
        "    \n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      print(\"I have just started\")\n",
        "      input_1 = tf.placeholder(tf.int32, [None, conf.sentence_length], name=\"input_x1\")\n",
        "      input_2 = tf.placeholder(tf.int32, [None, conf.sentence_length], name=\"input_x2\")\n",
        "      input_3 = tf.placeholder(tf.float32, [None, conf.num_classes], name=\"input_y\")\n",
        "      dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
        "      with tf.name_scope(\"embendding\"):\n",
        "          s0_embed = tf.nn.embedding_lookup(glove.g, input_1)\n",
        "          s1_embed = tf.nn.embedding_lookup(glove.g, input_2)\n",
        "\n",
        "      with tf.name_scope(\"reshape\"):\n",
        "          input_x1 = tf.reshape(s0_embed, [-1, conf.sentence_length, conf.embedding_dim, 1])\n",
        "          input_x2 = tf.reshape(s1_embed, [-1, conf.sentence_length, conf.embedding_dim, 1])\n",
        "          input_y = tf.reshape(input_3, [-1, conf.num_classes])\n",
        "\n",
        "\n",
        "      \n",
        "      setence_model = MPCNN_Layer(conf.num_classes, conf.embedding_dim, filter_size,\n",
        "                                  [conf.num_filters_A, conf.num_filters_B], conf.n_hidden,\n",
        "                                  input_x1, input_x2, input_y, dropout_keep_prob, conf.l2_reg_lambda)\n",
        "    \n",
        "      global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "      \n",
        "      setence_model.similarity_measure_layer()\n",
        "      optimizer = tf.train.AdamOptimizer(conf.lr)\n",
        "      grads_and_vars = optimizer.compute_gradients(setence_model.loss)\n",
        "      train_step = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "      \n",
        "      timestamp = str(int(time.time()))\n",
        "      out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
        "      print(\"Writing to {}\\n\".format(out_dir))\n",
        "      #\n",
        "      loss_summary = tf.summary.scalar(\"loss\", setence_model.loss)\n",
        "      acc_summary = tf.summary.scalar(\"accuracy\", setence_model.accuracy)\n",
        "      \n",
        "      #\n",
        "      train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "      train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "      train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "      #\n",
        "      dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "      dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "      dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
        "      print(\"Checkpoint 3\")\n",
        "      def train1(x1_batch, x2_batch, y_batch):\n",
        "        \"\"\"\n",
        "        A single training step\n",
        "        \"\"\"\n",
        "        feed_dict = {\n",
        "          input_1: x1_batch,\n",
        "          input_2: x2_batch,\n",
        "          input_3: y_batch,\n",
        "          dropout_keep_prob: 0.5\n",
        "        }\n",
        "        _, step, summaries, batch_loss, accuracy = sess.run(\n",
        "            [train_step, global_step, train_summary_op, setence_model.loss, setence_model.accuracy],\n",
        "            feed_dict)\n",
        "        time_str = datetime.datetime.now().isoformat()\n",
        "        logger.info(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, batch_loss, accuracy))\n",
        "        train_summary_writer.add_summary(summaries, step)\n",
        "        \n",
        "      def dev_step(x1_batch, x2_batch, y_batch, writer=None):\n",
        "            \"\"\"\n",
        "            Evaluates model on a dev set\n",
        "            \"\"\"\n",
        "            feed_dict = {\n",
        "              input_1: x1_batch,\n",
        "              input_2: x2_batch,\n",
        "              input_3: y_batch,\n",
        "              dropout_keep_prob: 1\n",
        "            }\n",
        "            _, step, summaries, batch_loss, accuracy = sess.run(\n",
        "                [train_step, global_step, dev_summary_op, setence_model.loss, setence_model.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            dev_summary_writer.add_summary(summaries, step)\n",
        "            if writer:\n",
        "                 writer.add_summary(summaries, step)\n",
        "\n",
        "            return batch_loss, accuracy\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      print(\"Checkpoint 3\")\n",
        "      batches = batch_iter(list(zip(Xtrain[0], Xtrain[1], ytrain)),conf.batch_size,conf.num_epochs)\n",
        "      for batch in batches:\n",
        "          x1_batch, x2_batch, y_batch = zip(*batch)\n",
        "          train1(x1_batch, x2_batch, y_batch)\n",
        "          current_step = tf.train.global_step(sess, global_step)\n",
        "          if current_step % conf.evaluate_every == 0:\n",
        "              total_dev_loss = 0.0\n",
        "              total_dev_accuracy = 0.0\n",
        "\n",
        "              logger.info(\"\\nEvaluation:\")\n",
        "              print(\"Checkpoint 5 iteration\")\n",
        "              dev_batches = batch_iter(list(zip(Xtest[0], Xtest[1], ytest)), conf.batch_size, 1)\n",
        "              for dev_batch in dev_batches:\n",
        "                  x1_dev_batch, x2_dev_batch, y_dev_batch = zip(*dev_batch)\n",
        "                  dev_loss, dev_accuracy = dev_step(x1_dev_batch, x2_dev_batch, y_dev_batch)\n",
        "                  total_dev_loss += dev_loss\n",
        "                  total_dev_accuracy += dev_accuracy\n",
        "              total_dev_accuracy = total_dev_accuracy / (len(ytest) / conf.batch_size)\n",
        "              logger.info(\"dev_loss {:g}, dev_acc {:g}, num_dev_batches {:g}\".format(total_dev_loss, total_dev_accuracy,\n",
        "                                                                               len(ytest) / conf.batch_size))\n",
        "\n",
        "      logger.info(\"Optimization Finished!\")\n",
        "      \n",
        "  #@abc.abstractmethod\n",
        "  def predict(self, data_X, data_Y):\n",
        "    \"\"\"\n",
        "\t\tPredicts the similarity score on the given input data(2 sentences). Assumes model has been trained with train()\n",
        "\t\tArgs:\n",
        "\t\t\tdata_X: Sentence 1(Non Tokenized).\n",
        "\t\t\tdata_Y: Sentence 2(Non Tokenized)\n",
        "\t\tReturns:\n",
        "\t\t\tprediction_score: Similarity Score ( Float ) \n",
        "\t\t\t\t\n",
        "\t\tRaises:\n",
        "\t\t\tNone\n",
        "\t\t\"\"\"\n",
        "    X=data_X\n",
        "    Y=data_Y\n",
        "    vector1 = self.text_to_vector(X)\n",
        "    vector2 = self.text_to_vector(Y)\n",
        "    result = self.get_cosine(vector1, vector2)\n",
        "    return result\n",
        "\n",
        "  \n",
        "\n",
        "  #@abc.abstractmethod\n",
        "  def evaluate(self, actual_values, predicted_values):\n",
        "    \"\"\"\n",
        "\t\tReturns the correlation score(0-1) between the actual and predicted similarity scores\n",
        "\t\tArgs:\n",
        "\t\t\tactual_values : List of actual similarity scores\n",
        "\t\t\tpredicted_values : List of predicted similarity scores\n",
        "\t\tReturns:\n",
        "\t\t\tcorrelation_coefficient : Value between 0-1 to show the correlation between the values(actual and predicted)\n",
        "\t\tRaises:\n",
        "\t\t\tNone\n",
        "      \"\"\"\n",
        "    x = np.array(actual_values)\n",
        "    y = np.array(predicted_values)\n",
        "    precision=precision_score(actual_values, predicted_values, average='samples')\n",
        "    recall=recall_score(actual_values, predicted_values, average='samples')\n",
        "    f1=f1_score(actual_values, predicted_values, average='samples')\n",
        "    \n",
        "    r, p = pearsonr(x, y)\n",
        "    evaluation_score = r\n",
        "    return precision,recall,f1\n",
        "\n",
        "  #@abc.abstractmethod\n",
        "  def save_model(self, file):\n",
        "    \"\"\"\n",
        "\t\t:param file: Where to save the model - Optional function\n",
        "\t\t:return:\n",
        "    \"\"\"\n",
        "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "         os.makedirs(checkpoint_dir)\n",
        "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=conf.num_checkpoints)\n",
        "    return\n",
        "\n",
        "  #@abc.abstractmethod\n",
        "  def load_model(self, file):\n",
        "    \"\"\"\n",
        "\t\t:param file: From where to load the model - Optional function\n",
        "\t\t:return:\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "#Created a folder for each dataset like sick,sem2017,sem2014 etc for training and testing and that will be given as a path \n",
        "#Input will read a txt file which contains sentence1 sentence 2\n",
        "print('loading glove...')\n",
        "glove = GloVe(N=100)\n",
        "model=TextSemanticSimilarity()\n",
        "Xtrain, ytrain =model.read_dataset('/content/drive/My Drive/Colab Notebooks/MPCNN/sts/semeval-sts/all')\n",
        "Xtrain[0], Xtrain[1], ytrain = shuffle(Xtrain[0], Xtrain[1], ytrain)\n",
        "Xtest, ytest =model.read_dataset('/content/drive/My Drive/Colab Notebooks/MPCNN/sts/semeval-sts/2016')\n",
        "Xtest[0], Xtest[1], ytest = shuffle(Xtest[0], Xtest[1], ytest)\n",
        "#train will train and evaluate at the same time\n",
        "model.train(Xtrain, ytrain,Xtest, ytest)\n",
        "predictions=[]\n",
        "\n",
        "#P,R,F1 = myModel.evaluate(predictions, test_Y)  # calculate Precision, Recall, F1\n",
        "#print('Precision: %s, Recall: %s, F1: %s'%(P,R,F1))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.0.0 in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.16.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (0.33.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->tensorflow==1.0.0) (41.0.1)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Mounted at /content/drive\n",
            "loading glove...\n",
            "(/content/drive/My Drive/Colab Notebooks/MPCNN/sts/semeval-sts/all) Loaded dataset: 450\n",
            "(/content/drive/My Drive/Colab Notebooks/MPCNN/sts/semeval-sts/2016) Loaded dataset: 254\n",
            "I have just started\n",
            "Writing to /content/runs/1556763904\n",
            "\n",
            "Checkpoint 3\n",
            "Checkpoint 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[2019-05-02 02:27:30,686][INFO] ## 2019-05-02T02:27:30.686732: step 1, loss 75.057, acc 0.03125\n",
            "[2019-05-02 02:27:30,686][INFO] ## 2019-05-02T02:27:30.686732: step 1, loss 75.057, acc 0.03125\n",
            "[2019-05-02 02:28:24,306][INFO] ## 2019-05-02T02:28:24.306303: step 2, loss 73.4256, acc 0.21875\n",
            "[2019-05-02 02:28:24,306][INFO] ## 2019-05-02T02:28:24.306303: step 2, loss 73.4256, acc 0.21875\n",
            "[2019-05-02 02:29:13,310][INFO] ## 2019-05-02T02:29:13.310076: step 3, loss 70.1697, acc 0.265625\n",
            "[2019-05-02 02:29:13,310][INFO] ## 2019-05-02T02:29:13.310076: step 3, loss 70.1697, acc 0.265625\n",
            "[2019-05-02 02:30:03,705][INFO] ## 2019-05-02T02:30:03.705542: step 4, loss 70.447, acc 0.265625\n",
            "[2019-05-02 02:30:03,705][INFO] ## 2019-05-02T02:30:03.705542: step 4, loss 70.447, acc 0.265625\n",
            "[2019-05-02 02:30:53,479][INFO] ## 2019-05-02T02:30:53.479448: step 5, loss 70.5924, acc 0.15625\n",
            "[2019-05-02 02:30:53,479][INFO] ## 2019-05-02T02:30:53.479448: step 5, loss 70.5924, acc 0.15625\n",
            "[2019-05-02 02:31:42,850][INFO] ## 2019-05-02T02:31:42.850677: step 6, loss 72.3254, acc 0.28125\n",
            "[2019-05-02 02:31:42,850][INFO] ## 2019-05-02T02:31:42.850677: step 6, loss 72.3254, acc 0.28125\n",
            "[2019-05-02 02:32:33,067][INFO] ## 2019-05-02T02:32:33.066960: step 7, loss 72.4686, acc 0.203125\n",
            "[2019-05-02 02:32:33,067][INFO] ## 2019-05-02T02:32:33.066960: step 7, loss 72.4686, acc 0.203125\n",
            "[2019-05-02 02:32:34,859][INFO] ## 2019-05-02T02:32:34.859255: step 8, loss 21.8344, acc 0\n",
            "[2019-05-02 02:32:34,859][INFO] ## 2019-05-02T02:32:34.859255: step 8, loss 21.8344, acc 0\n",
            "[2019-05-02 02:33:25,233][INFO] ## 2019-05-02T02:33:25.233329: step 9, loss 69.0898, acc 0.359375\n",
            "[2019-05-02 02:33:25,233][INFO] ## 2019-05-02T02:33:25.233329: step 9, loss 69.0898, acc 0.359375\n",
            "[2019-05-02 02:34:15,441][INFO] ## 2019-05-02T02:34:15.441154: step 10, loss 70.2108, acc 0.3125\n",
            "[2019-05-02 02:34:15,441][INFO] ## 2019-05-02T02:34:15.441154: step 10, loss 70.2108, acc 0.3125\n",
            "[2019-05-02 02:35:05,519][INFO] ## 2019-05-02T02:35:05.519547: step 11, loss 70.9307, acc 0.25\n",
            "[2019-05-02 02:35:05,519][INFO] ## 2019-05-02T02:35:05.519547: step 11, loss 70.9307, acc 0.25\n",
            "[2019-05-02 02:35:54,176][INFO] ## 2019-05-02T02:35:54.176146: step 12, loss 68.5631, acc 0.328125\n",
            "[2019-05-02 02:35:54,176][INFO] ## 2019-05-02T02:35:54.176146: step 12, loss 68.5631, acc 0.328125\n",
            "[2019-05-02 02:36:44,207][INFO] ## 2019-05-02T02:36:44.207279: step 13, loss 73.6029, acc 0.1875\n",
            "[2019-05-02 02:36:44,207][INFO] ## 2019-05-02T02:36:44.207279: step 13, loss 73.6029, acc 0.1875\n",
            "[2019-05-02 02:37:34,042][INFO] ## 2019-05-02T02:37:34.042724: step 14, loss 71.1288, acc 0.28125\n",
            "[2019-05-02 02:37:34,042][INFO] ## 2019-05-02T02:37:34.042724: step 14, loss 71.1288, acc 0.28125\n",
            "[2019-05-02 02:38:24,971][INFO] ## 2019-05-02T02:38:24.971018: step 15, loss 70.5912, acc 0.203125\n",
            "[2019-05-02 02:38:24,971][INFO] ## 2019-05-02T02:38:24.971018: step 15, loss 70.5912, acc 0.203125\n",
            "[2019-05-02 02:38:26,770][INFO] ## 2019-05-02T02:38:26.769985: step 16, loss 21.7786, acc 0.5\n",
            "[2019-05-02 02:38:26,770][INFO] ## 2019-05-02T02:38:26.769985: step 16, loss 21.7786, acc 0.5\n",
            "[2019-05-02 02:39:16,834][INFO] ## 2019-05-02T02:39:16.833904: step 17, loss 72.1358, acc 0.296875\n",
            "[2019-05-02 02:39:16,834][INFO] ## 2019-05-02T02:39:16.833904: step 17, loss 72.1358, acc 0.296875\n",
            "[2019-05-02 02:40:05,373][INFO] ## 2019-05-02T02:40:05.373735: step 18, loss 69.129, acc 0.34375\n",
            "[2019-05-02 02:40:05,373][INFO] ## 2019-05-02T02:40:05.373735: step 18, loss 69.129, acc 0.34375\n",
            "[2019-05-02 02:40:55,415][INFO] ## 2019-05-02T02:40:55.415557: step 19, loss 68.5494, acc 0.34375\n",
            "[2019-05-02 02:40:55,415][INFO] ## 2019-05-02T02:40:55.415557: step 19, loss 68.5494, acc 0.34375\n",
            "[2019-05-02 02:41:45,309][INFO] ## 2019-05-02T02:41:45.309812: step 20, loss 69.5257, acc 0.25\n",
            "[2019-05-02 02:41:45,309][INFO] ## 2019-05-02T02:41:45.309812: step 20, loss 69.5257, acc 0.25\n",
            "[2019-05-02 02:42:34,607][INFO] ## 2019-05-02T02:42:34.607559: step 21, loss 70.8599, acc 0.328125\n",
            "[2019-05-02 02:42:34,607][INFO] ## 2019-05-02T02:42:34.607559: step 21, loss 70.8599, acc 0.328125\n",
            "[2019-05-02 02:43:26,077][INFO] ## 2019-05-02T02:43:26.077692: step 22, loss 69.9993, acc 0.234375\n",
            "[2019-05-02 02:43:26,077][INFO] ## 2019-05-02T02:43:26.077692: step 22, loss 69.9993, acc 0.234375\n",
            "[2019-05-02 02:44:14,879][INFO] ## 2019-05-02T02:44:14.879504: step 23, loss 69.8181, acc 0.3125\n",
            "[2019-05-02 02:44:14,879][INFO] ## 2019-05-02T02:44:14.879504: step 23, loss 69.8181, acc 0.3125\n",
            "[2019-05-02 02:44:17,001][INFO] ## 2019-05-02T02:44:17.001365: step 24, loss 21.1471, acc 0.5\n",
            "[2019-05-02 02:44:17,001][INFO] ## 2019-05-02T02:44:17.001365: step 24, loss 21.1471, acc 0.5\n",
            "[2019-05-02 02:45:06,191][INFO] ## 2019-05-02T02:45:06.191511: step 25, loss 68.7775, acc 0.3125\n",
            "[2019-05-02 02:45:06,191][INFO] ## 2019-05-02T02:45:06.191511: step 25, loss 68.7775, acc 0.3125\n",
            "[2019-05-02 02:45:56,271][INFO] ## 2019-05-02T02:45:56.271243: step 26, loss 68.4711, acc 0.3125\n",
            "[2019-05-02 02:45:56,271][INFO] ## 2019-05-02T02:45:56.271243: step 26, loss 68.4711, acc 0.3125\n",
            "[2019-05-02 02:46:44,755][INFO] ## 2019-05-02T02:46:44.754931: step 27, loss 69.1346, acc 0.34375\n",
            "[2019-05-02 02:46:44,755][INFO] ## 2019-05-02T02:46:44.754931: step 27, loss 69.1346, acc 0.34375\n",
            "[2019-05-02 02:47:35,554][INFO] ## 2019-05-02T02:47:35.554211: step 28, loss 70.6171, acc 0.234375\n",
            "[2019-05-02 02:47:35,554][INFO] ## 2019-05-02T02:47:35.554211: step 28, loss 70.6171, acc 0.234375\n",
            "[2019-05-02 02:48:25,509][INFO] ## 2019-05-02T02:48:25.509165: step 29, loss 69.4591, acc 0.28125\n",
            "[2019-05-02 02:48:25,509][INFO] ## 2019-05-02T02:48:25.509165: step 29, loss 69.4591, acc 0.28125\n",
            "[2019-05-02 02:49:15,456][INFO] ## 2019-05-02T02:49:15.456434: step 30, loss 69.9415, acc 0.28125\n",
            "[2019-05-02 02:49:15,456][INFO] ## 2019-05-02T02:49:15.456434: step 30, loss 69.9415, acc 0.28125\n",
            "[2019-05-02 02:50:05,463][INFO] ## 2019-05-02T02:50:05.462948: step 31, loss 69.7028, acc 0.359375\n",
            "[2019-05-02 02:50:05,463][INFO] ## 2019-05-02T02:50:05.462948: step 31, loss 69.7028, acc 0.359375\n",
            "[2019-05-02 02:50:07,263][INFO] ## 2019-05-02T02:50:07.263636: step 32, loss 20.8681, acc 0.5\n",
            "[2019-05-02 02:50:07,263][INFO] ## 2019-05-02T02:50:07.263636: step 32, loss 20.8681, acc 0.5\n",
            "[2019-05-02 02:50:56,052][INFO] ## 2019-05-02T02:50:56.052477: step 33, loss 69.5448, acc 0.265625\n",
            "[2019-05-02 02:50:56,052][INFO] ## 2019-05-02T02:50:56.052477: step 33, loss 69.5448, acc 0.265625\n",
            "[2019-05-02 02:51:45,638][INFO] ## 2019-05-02T02:51:45.638310: step 34, loss 68.9506, acc 0.296875\n",
            "[2019-05-02 02:51:45,638][INFO] ## 2019-05-02T02:51:45.638310: step 34, loss 68.9506, acc 0.296875\n",
            "[2019-05-02 02:52:35,766][INFO] ## 2019-05-02T02:52:35.766397: step 35, loss 68.7921, acc 0.28125\n",
            "[2019-05-02 02:52:35,766][INFO] ## 2019-05-02T02:52:35.766397: step 35, loss 68.7921, acc 0.28125\n",
            "[2019-05-02 02:53:24,784][INFO] ## 2019-05-02T02:53:24.784594: step 36, loss 70.8028, acc 0.265625\n",
            "[2019-05-02 02:53:24,784][INFO] ## 2019-05-02T02:53:24.784594: step 36, loss 70.8028, acc 0.265625\n",
            "[2019-05-02 02:54:15,269][INFO] ## 2019-05-02T02:54:15.269114: step 37, loss 67.8593, acc 0.359375\n",
            "[2019-05-02 02:54:15,269][INFO] ## 2019-05-02T02:54:15.269114: step 37, loss 67.8593, acc 0.359375\n",
            "[2019-05-02 02:55:05,262][INFO] ## 2019-05-02T02:55:05.262200: step 38, loss 68.5959, acc 0.328125\n",
            "[2019-05-02 02:55:05,262][INFO] ## 2019-05-02T02:55:05.262200: step 38, loss 68.5959, acc 0.328125\n",
            "[2019-05-02 02:55:53,764][INFO] ## 2019-05-02T02:55:53.764245: step 39, loss 68.7448, acc 0.34375\n",
            "[2019-05-02 02:55:53,764][INFO] ## 2019-05-02T02:55:53.764245: step 39, loss 68.7448, acc 0.34375\n",
            "[2019-05-02 02:55:55,554][INFO] ## 2019-05-02T02:55:55.554683: step 40, loss 20.4103, acc 1\n",
            "[2019-05-02 02:55:55,554][INFO] ## 2019-05-02T02:55:55.554683: step 40, loss 20.4103, acc 1\n",
            "[2019-05-02 02:56:45,602][INFO] ## 2019-05-02T02:56:45.601927: step 41, loss 70.3558, acc 0.25\n",
            "[2019-05-02 02:56:45,602][INFO] ## 2019-05-02T02:56:45.601927: step 41, loss 70.3558, acc 0.25\n",
            "[2019-05-02 02:57:33,930][INFO] ## 2019-05-02T02:57:33.930822: step 42, loss 67.6951, acc 0.328125\n",
            "[2019-05-02 02:57:33,930][INFO] ## 2019-05-02T02:57:33.930822: step 42, loss 67.6951, acc 0.328125\n",
            "[2019-05-02 02:58:23,976][INFO] ## 2019-05-02T02:58:23.976010: step 43, loss 70.331, acc 0.25\n",
            "[2019-05-02 02:58:23,976][INFO] ## 2019-05-02T02:58:23.976010: step 43, loss 70.331, acc 0.25\n",
            "[2019-05-02 02:59:15,244][INFO] ## 2019-05-02T02:59:15.244850: step 44, loss 69.5327, acc 0.28125\n",
            "[2019-05-02 02:59:15,244][INFO] ## 2019-05-02T02:59:15.244850: step 44, loss 69.5327, acc 0.28125\n",
            "[2019-05-02 03:00:03,630][INFO] ## 2019-05-02T03:00:03.630743: step 45, loss 67.4522, acc 0.3125\n",
            "[2019-05-02 03:00:03,630][INFO] ## 2019-05-02T03:00:03.630743: step 45, loss 67.4522, acc 0.3125\n",
            "[2019-05-02 03:00:53,523][INFO] ## 2019-05-02T03:00:53.523280: step 46, loss 66.4941, acc 0.453125\n",
            "[2019-05-02 03:00:53,523][INFO] ## 2019-05-02T03:00:53.523280: step 46, loss 66.4941, acc 0.453125\n",
            "[2019-05-02 03:01:42,515][INFO] ## 2019-05-02T03:01:42.515433: step 47, loss 68.8816, acc 0.234375\n",
            "[2019-05-02 03:01:42,515][INFO] ## 2019-05-02T03:01:42.515433: step 47, loss 68.8816, acc 0.234375\n",
            "[2019-05-02 03:01:44,617][INFO] ## 2019-05-02T03:01:44.617776: step 48, loss 20.3317, acc 0.5\n",
            "[2019-05-02 03:01:44,617][INFO] ## 2019-05-02T03:01:44.617776: step 48, loss 20.3317, acc 0.5\n",
            "[2019-05-02 03:02:33,537][INFO] ## 2019-05-02T03:02:33.536559: step 49, loss 69.1614, acc 0.328125\n",
            "[2019-05-02 03:02:33,537][INFO] ## 2019-05-02T03:02:33.536559: step 49, loss 69.1614, acc 0.328125\n",
            "[2019-05-02 03:03:24,219][INFO] ## 2019-05-02T03:03:24.219209: step 50, loss 68.4375, acc 0.34375\n",
            "[2019-05-02 03:03:24,219][INFO] ## 2019-05-02T03:03:24.219209: step 50, loss 68.4375, acc 0.34375\n",
            "[2019-05-02 03:04:14,156][INFO] ## 2019-05-02T03:04:14.156564: step 51, loss 67.7371, acc 0.28125\n",
            "[2019-05-02 03:04:14,156][INFO] ## 2019-05-02T03:04:14.156564: step 51, loss 67.7371, acc 0.28125\n",
            "[2019-05-02 03:05:03,885][INFO] ## 2019-05-02T03:05:03.885105: step 52, loss 66.6361, acc 0.359375\n",
            "[2019-05-02 03:05:03,885][INFO] ## 2019-05-02T03:05:03.885105: step 52, loss 66.6361, acc 0.359375\n",
            "[2019-05-02 03:05:53,648][INFO] ## 2019-05-02T03:05:53.648495: step 53, loss 66.1592, acc 0.390625\n",
            "[2019-05-02 03:05:53,648][INFO] ## 2019-05-02T03:05:53.648495: step 53, loss 66.1592, acc 0.390625\n",
            "[2019-05-02 03:06:41,995][INFO] ## 2019-05-02T03:06:41.995351: step 54, loss 69.5693, acc 0.1875\n",
            "[2019-05-02 03:06:41,995][INFO] ## 2019-05-02T03:06:41.995351: step 54, loss 69.5693, acc 0.1875\n",
            "[2019-05-02 03:07:31,829][INFO] ## 2019-05-02T03:07:31.828917: step 55, loss 69.0925, acc 0.265625\n",
            "[2019-05-02 03:07:31,829][INFO] ## 2019-05-02T03:07:31.828917: step 55, loss 69.0925, acc 0.265625\n",
            "[2019-05-02 03:07:33,610][INFO] ## 2019-05-02T03:07:33.610302: step 56, loss 20.2341, acc 0\n",
            "[2019-05-02 03:07:33,610][INFO] ## 2019-05-02T03:07:33.610302: step 56, loss 20.2341, acc 0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "jBS9vEhMFM5c",
        "colab_type": "code",
        "outputId": "69ba9f1a-0be7-48f9-e1c3-f45d93b4a38e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/My Drive/Colab Notebooks/MPCNN/TextSemanticSimilarity_test_input.txt\", \"r\") as ins:\n",
        "    for line in ins:\n",
        "        text1,text2=line.split(\",\")\n",
        "        prediction=model.predict(text1,text2)\n",
        "        s=line+\",\"+str(prediction)\n",
        "        #print(prediction)\n",
        "        predictions.append(prediction)\n",
        "        with open('/content/drive/My Drive/Colab Notebooks/MPCNN/TextSemanticSimilarity_test_output.txt', 'a+') as f:\n",
        "          f.write(s+\"\\n\")\n",
        "          print(s)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A group of kids is playing in a yard and an old man is standing in the background,A group of boys in a yard is playing and a man is standing in the background\n",
            ",0.889108448948774\n",
            "A group of children is playing in the house and there is no man standing in the background,A group of kids is playing in a yard and an old man is standing in the background\n",
            ",0.7833494518006403\n",
            "The young boys are playing outdoors and the man is smiling nearby,The kids are playing outdoors near a man with a smile \n",
            ",0.4003203845127179\n",
            "The young boys are playing outdoors and the man is smiling nearby,There is no boy playing outdoors and there is no man smiling \n",
            ",0.5051814855409226\n",
            "The kids are playing outdoors near a man with a smile,A group of kids is playing in a yard and an old man is standing in the background \n",
            ",0.2956561979945413\n",
            "There is no boy playing outdoors and there is no man smiling,A group of kids is playing in a yard and an old man is standing in the background \n",
            ",0.3731012536223182\n",
            "A group of boys in a yard is playing and a man is standing in the background,The young boys are playing outdoors and the man is smiling nearby \n",
            ",0.4213504858001923\n",
            "A group of children is playing in the house and there is no man standing in the background,The young boys are playing outdoors and the man is smiling nearby \n",
            ",0.41247895569215276\n",
            "The young boys are playing outdoors and the man is smiling nearby,A group of kids is playing in a yard and an old man is standing in the background \n",
            ",0.3692744729379982\n",
            "A brown dog is attacking another animal in front of the tall man in pants,A brown dog is attacking another animal in front of the man in pants ,0.9701425001453319\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2KLExlEebsWZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MPCNN_Layer():\n",
        "    def compute_l1_distance(self,x, y):\n",
        "      with tf.name_scope('l1_distance'):\n",
        "          d = tf.reduce_sum(tf.abs(tf.subtract(x, y)), axis=1)\n",
        "          return d\n",
        "      \n",
        "    def compute_euclidean_distance(self,x, y):\n",
        "      with tf.name_scope('euclidean_distance'):\n",
        "          d = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(x, y)), axis=1))\n",
        "          return d\n",
        "\n",
        "    def compute_pearson_distance(self,x, y):\n",
        "        with tf.name_scope(\"pearson\"):\n",
        "            mid1 = tf.reduce_mean(x * y, axis=1) - \\\n",
        "                        tf.reduce_mean(x, axis=1) * tf.reduce_mean(y, axis=1)\n",
        "            mid2 = tf.sqrt(tf.reduce_mean(tf.square(x), axis=1) - tf.square(tf.reduce_mean(x, axis=1))) * \\\n",
        "                   tf.sqrt(tf.reduce_mean(tf.square(y), axis=1) - tf.square(tf.reduce_mean(y, axis=1)))\n",
        "            return mid1 / mid2\n",
        "\n",
        "    def compute_cosine_distance(self,x, y):\n",
        "        with tf.name_scope('cosine_distance'):\n",
        "            x_norm = tf.sqrt(tf.reduce_sum(tf.square(x), axis=1))\n",
        "            y_norm = tf.sqrt(tf.reduce_sum(tf.square(y), axis=1))\n",
        "            x_y = tf.reduce_sum(tf.multiply(x, y), axis=1)\n",
        "            d = tf.divide(x_y, tf.multiply(x_norm, y_norm))\n",
        "            return d\n",
        "\n",
        "    def comU1(self,x, y):\n",
        "        result = [self.compute_cosine_distance(x, y), self.compute_l1_distance(x, y)]\n",
        "        # result = [compute_euclidean_distance(x, y), compute_euclidean_distance(x, y), compute_euclidean_distance(x, y)]\n",
        "        return tf.stack(result, axis=1)\n",
        "    def comU2(self,x, y):\n",
        "    # result = [compute_cosine_distance(x, y), compute_euclidean_distance(x, y)]\n",
        "    # return tf.stack(result, axis=1)\n",
        "      return tf.expand_dims(self.compute_cosine_distance(x, y), -1)\n",
        "    def __init__(self, num_classes, embedding_size, filter_sizes, num_filters, n_hidden,\n",
        "                 input_x1, input_x2, input_y, dropout_keep_prob, l2_reg_lambda):\n",
        "        '''\n",
        "        :param sequence_length:\n",
        "        :param num_classes:\n",
        "        :param embedding_size:\n",
        "        :param filter_sizes:\n",
        "        :param num_filters:\n",
        "        '''\n",
        "        self.embedding_size = embedding_size\n",
        "        self.filter_sizes = filter_sizes\n",
        "        self .num_filters = num_filters\n",
        "        self.num_classes = num_classes\n",
        "        self.poolings = [tf.reduce_max, tf.reduce_min, tf.reduce_mean]\n",
        "\n",
        "        self.input_x1 = input_x1\n",
        "        self.input_x2 = input_x2\n",
        "        self.input_y = input_y\n",
        "        self.dropout_keep_prob = dropout_keep_prob\n",
        "        self.l2_loss = tf.constant(0.0)\n",
        "        self.l2_reg_lambda = l2_reg_lambda\n",
        "        self.W1 = [self.init_weight([filter_sizes[0], embedding_size, 1, num_filters[0]], \"W1_0\"),\n",
        "                   self.init_weight([filter_sizes[1], embedding_size, 1, num_filters[0]], \"W1_1\"),\n",
        "                   self.init_weight([filter_sizes[2], embedding_size, 1, num_filters[0]], \"W1_2\")]\n",
        "        self.b1 = [tf.Variable(tf.constant(0.1, shape=[num_filters[0]]), \"b1_0\"),\n",
        "                   tf.Variable(tf.constant(0.1, shape=[num_filters[0]]), \"b1_1\"),\n",
        "                   tf.Variable(tf.constant(0.1, shape=[num_filters[0]]), \"b1_2\")]\n",
        "\n",
        "        self.W2 = [self.init_weight([filter_sizes[0], embedding_size, 1, num_filters[1]], \"W2_0\"),\n",
        "                   self.init_weight([filter_sizes[1], embedding_size, 1, num_filters[1]], \"W2_1\")]\n",
        "        self.b2 = [tf.Variable(tf.constant(0.1, shape=[num_filters[1], embedding_size]), \"b2_0\"),\n",
        "                   tf.Variable(tf.constant(0.1, shape=[num_filters[1], embedding_size]), \"b2_1\")]\n",
        "        self.h = num_filters[0]*len(self.poolings)*2 + \\\n",
        "                 num_filters[1]*(len(self.poolings)-1)*(len(filter_sizes)-1)*3 + \\\n",
        "                 len(self.poolings)*len(filter_sizes)*len(filter_sizes)*3\n",
        "        self.Wh = tf.Variable(tf.random_normal([604, n_hidden], stddev=0.01), name='Wh')\n",
        "        self.bh = tf.Variable(tf.constant(0.1, shape=[n_hidden]), name=\"bh\")\n",
        "\n",
        "        self.Wo = tf.Variable(tf.random_normal([n_hidden, num_classes], stddev=0.01), name='Wo')\n",
        "        self.bo = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"bo\")\n",
        "\n",
        "    def init_weight(self,shape, name):\n",
        "      var = tf.Variable(tf.truncated_normal(shape, mean=0, stddev=1.0), name=name)\n",
        "      return var\n",
        "  \n",
        "    def attention(self):\n",
        "        sent1_unstack = tf.unstack(self.input_x1, axis=1)\n",
        "        sent2_unstack = tf.unstack(self.input_x2, axis=1)\n",
        "        D = []\n",
        "        for i in range(len(sent1_unstack)):\n",
        "            d = []\n",
        "            for j in range(len(sent2_unstack)):\n",
        "                dis = self.compute_cosine_distance(sent1_unstack[i], sent2_unstack[j])\n",
        "                #dis:[batch_size, 1(channels)]\n",
        "                d.append(dis)\n",
        "            D.append(d)\n",
        "            print(i)\n",
        "        D = tf.reshape(D, [-1, len(sent1_unstack), len(sent2_unstack), 1])\n",
        "        A = [tf.nn.softmax(tf.expand_dims(tf.reduce_sum(D, axis=i), 2)) for i in [2, 1]]\n",
        "        atten_embed = []\n",
        "        atten_embed.append(tf.concat([self.input_x1, A[0] * self.input_x1], 2))\n",
        "        atten_embed.append(tf.concat([self.input_x2, A[1] * self.input_x2], 2))\n",
        "        return atten_embed\n",
        "\n",
        "    def per_dim_conv_layer(self, x, w, b, pooling):\n",
        "        '''\n",
        "        :param input: [batch_size, sentence_length, embed_size, 1]\n",
        "        :param w: [ws, embedding_size, 1, num_filters]\n",
        "        :param b: [num_filters, embedding_size]\n",
        "        :param pooling:\n",
        "        :return:\n",
        "        '''\n",
        "        # unpcak the input in the dim of embed_dim\n",
        "        input_unstack = tf.unstack(x, axis=2)\n",
        "        w_unstack = tf.unstack(w, axis=1)\n",
        "        b_unstack = tf.unstack(b, axis=1)\n",
        "        convs = []\n",
        "        for i in range(x.get_shape()[2]):\n",
        "            conv = tf.nn.conv1d(input_unstack[i], w_unstack[i], stride=1, padding=\"VALID\")\n",
        "            conv = slim.batch_norm(inputs=conv, activation_fn=tf.nn.tanh, is_training=self.is_training)\n",
        "            convs.append(conv)\n",
        "        conv = tf.stack(convs, axis=2)\n",
        "        pool = pooling(conv, axis=1)\n",
        "\n",
        "        return pool\n",
        "\n",
        "    def bulit_block_A(self, x):\n",
        "        #bulid block A and cal the similarity according to algorithm 1\n",
        "        out = []\n",
        "        with tf.name_scope(\"bulid_block_A\"):\n",
        "            for pooling in self.poolings:\n",
        "                pools = []\n",
        "                for i, ws in enumerate(self.filter_sizes):\n",
        "                    with tf.name_scope(\"conv-pool-%s\" %ws):\n",
        "                        conv = tf.nn.conv2d(x, self.W1[i], strides=[1, 1, 1, 1], padding=\"VALID\")\n",
        "                        conv = slim.batch_norm(inputs=conv, activation_fn=tf.nn.tanh, is_training=self.is_training)\n",
        "                        pool = pooling(conv, axis=1)\n",
        "                    pools.append(pool)\n",
        "                out.append(pools)\n",
        "            return out\n",
        "\n",
        "    def bulid_block_B(self, x):\n",
        "        out = []\n",
        "        with tf.name_scope(\"bulid_block_B\"):\n",
        "            for pooling in self.poolings[:-1]:\n",
        "                pools = []\n",
        "                with tf.name_scope(\"conv-pool\"):\n",
        "                    for i, ws in enumerate(self.filter_sizes[:-1]):\n",
        "                        with tf.name_scope(\"per_conv-pool-%s\" % ws):\n",
        "                            pool = self.per_dim_conv_layer(x, self.W2[i], self.b2[i], pooling)\n",
        "                        pools.append(pool)\n",
        "                    out.append(pools)\n",
        "            return out\n",
        "\n",
        "\n",
        "    def similarity_sentence_layer(self):\n",
        "        # atten = self.attention() #[batch_size, length, 2*embedding, 1]\n",
        "        sent1 = self.bulit_block_A(self.input_x1)\n",
        "        sent2 = self.bulit_block_A(self.input_x2)\n",
        "        fea_h = []\n",
        "        with tf.name_scope(\"cal_dis_with_alg1\"):\n",
        "            for i in range(3):\n",
        "                regM1 = tf.concat(sent1[i], 1)\n",
        "                regM2 = tf.concat(sent2[i], 1)\n",
        "                for k in range(self.num_filters[0]):\n",
        "                    fea_h.append(self.comU2(regM1[:, :, k], regM2[:, :, k]))\n",
        "\n",
        "        #self.fea_h = fea_h\n",
        "\n",
        "        fea_a = []\n",
        "        with tf.name_scope(\"cal_dis_with_alg2_2-9\"):\n",
        "            for i in range(3):\n",
        "                for j in range(len(self.filter_sizes)):\n",
        "                    for k in range(len(self.filter_sizes)):\n",
        "                        fea_a.append(self.comU1(sent1[i][j][:, 0, :], sent2[i][k][:, 0, :]))\n",
        "        #\n",
        "        sent1 = self.bulid_block_B(self.input_x1)\n",
        "        sent2 = self.bulid_block_B(self.input_x2)\n",
        "\n",
        "        fea_b = []\n",
        "        with tf.name_scope(\"cal_dis_with_alg2_last\"):\n",
        "            for i in range(len(self.poolings)-1):\n",
        "                for j in range(len(self.filter_sizes)-1):\n",
        "                    for k in range(self.num_filters[1]):\n",
        "                        fea_b.append(self.comU1(sent1[i][j][:, :, k], sent2[i][j][:, :, k]))\n",
        "        #self.fea_b = fea_b\n",
        "        return tf.concat(fea_h + fea_a + fea_b, 1)\n",
        "\n",
        "\n",
        "    def similarity_measure_layer(self, is_training=True):\n",
        "        self.is_training = is_training\n",
        "        fea = self.similarity_sentence_layer()\n",
        "        self.h_drop = tf.nn.dropout(fea, self.dropout_keep_prob)\n",
        "        # fea_h.extend(fea_a)\n",
        "        # fea_h.extend(fea_b)\n",
        "        #print len(fea_h), fea_h\n",
        "        #fea = tf.concat(fea_h+fea_a+fea_b, 1)\n",
        "        #print fea.get_shape()\n",
        "        with tf.name_scope(\"full_connect_layer\"):\n",
        "            h = tf.nn.tanh(tf.matmul(fea, self.Wh) + self.bh)\n",
        "            # h = tf.nn.dropout(h, self.dropout_keep_prob)\n",
        "            self.scores = tf.matmul(h, self.Wo) + self.bo\n",
        "            self.output = tf.nn.softmax(self.scores)\n",
        "        #     return o\n",
        "\n",
        "        # CalculateMean cross-entropy loss\n",
        "        reg = tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(1e-4), tf.trainable_variables())\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            # self.loss = -tf.reduce_sum(self.input_y * tf.log(self.output))\n",
        "            self.loss = tf.reduce_sum(tf.square(tf.subtract(self.input_y, self.output))) + reg\n",
        "\n",
        "            # self.loss = tf.reduce_mean(\n",
        "            #     tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y))\n",
        "            # self.loss = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss\n",
        "\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.input_y, 1), tf.argmax(self.scores, 1)), tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}