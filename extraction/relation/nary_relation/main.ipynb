{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph_LSTM",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "KNOpgCs1ygGq",
        "colab_type": "code",
        "outputId": "0e4baf9d-540d-4793-b0d0-e23d6cde8671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/tagucci/pythonrouge.git \n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import re\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import copy\n",
        "import sys, math\n",
        "from collections import defaultdict\n",
        "from tensorflow.python.ops import rnn\n",
        "import pdb\n",
        "import cPickle as pickle\n",
        "from tensorflow.python.ops import variable_scope\n",
        "import random\n",
        "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
        "from os.path import basename\n",
        "from pythonrouge.pythonrouge import Pythonrouge\n",
        "import codecs\n",
        "import platform\n",
        "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu\n",
        "tf.logging.set_verbosity(tf.logging.ERROR) # DEBUG, INFO, WARN, ERROR, and FATAL\n",
        "# import math\n",
        "class Vocab(object):\n",
        "    def __init__(self, vec_path=None, dim=100, fileformat='bin',voc=None, word2id=None, word_vecs=None, unk_mapping_path=None):\n",
        "        self.unk_label = 'UNK'\n",
        "        self.stoplist = None\n",
        "        if fileformat == 'bin':\n",
        "            self.fromBinary(vec_path,voc=voc)\n",
        "        elif fileformat == 'txt':\n",
        "            self.fromText(vec_path,voc=voc)\n",
        "        elif fileformat == 'txt2':\n",
        "            self.fromText_format2(vec_path,voc=voc,pre_word_vecs=word_vecs)\n",
        "        elif fileformat == 'txt3':\n",
        "            self.fromText_format3(vec_path,voc=voc)\n",
        "        elif fileformat == 'map':\n",
        "            self.fromMap(word2id, word_vecs, word_dim=dim)\n",
        "        else: # build a vocabulary with a word set\n",
        "            self.fromVocabualry(voc, dim=dim)\n",
        "\n",
        "        self.__unk_mapping = None\n",
        "        if unk_mapping_path is not None:\n",
        "            self.__unk_mapping = {}\n",
        "            in_file = open(unk_mapping_path, 'rt')\n",
        "            for line in in_file:\n",
        "                items = re.split('\\t', line)\n",
        "                self.__unk_mapping[items[0]] = items[1]\n",
        "            in_file.close()\n",
        "\n",
        "\n",
        "    def get_anony_ids(self):\n",
        "        self.anony_ids = set([y for x, y in self.word2id.iteritems() if re.search('_[0-9]+', x) != None])\n",
        "        self.anony_toks = [x for x, y in self.word2id.iteritems() if y in self.anony_ids]\n",
        "        print(self.anony_toks)\n",
        "\n",
        "    def fromVocabualry(self, voc, dim=100):\n",
        "        assert '#pad#' not in voc\n",
        "        assert 'UNK' not in voc\n",
        "        # load freq table and build index for each word\n",
        "        self.word2id = {'#pad#':0,'UNK':1,}\n",
        "        self.id2word = {0:'#pad#',1:'UNK',}\n",
        "\n",
        "        for word in voc:\n",
        "            cur_index = len(self.word2id)\n",
        "            self.word2id[word] = cur_index\n",
        "            self.id2word[cur_index] = word\n",
        "\n",
        "        self.vocab_size = len(self.word2id)\n",
        "        assert self.vocab_size == len(voc)+2\n",
        "        self.word_dim = dim\n",
        "        zero_vecs = np.zeros((1, self.word_dim), dtype=np.float32)\n",
        "        shape = (self.vocab_size-1, self.word_dim)\n",
        "        scale = 0.05\n",
        "        normal_vecs = np.array(np.random.uniform(low=-scale, high=scale, size=shape), dtype=np.float32)\n",
        "        self.word_vecs = np.concatenate((zero_vecs,normal_vecs,), axis=0)\n",
        "\n",
        "\n",
        "    def fromMap(self, word2id, word_vecs, word_dim=100):\n",
        "        assert False, 'not in use'\n",
        "        self.word2id = word2id\n",
        "        self.id2word = dict(zip(word2id.values(),word2id.keys()))\n",
        "\n",
        "        self.vocab_size = len(word2id)\n",
        "        self.word_dim = word_dim\n",
        "        self.word_vecs = word_vecs\n",
        "\n",
        "\n",
        "    def fromText(self, vec_path,voc=None):\n",
        "        assert False, 'not in use'\n",
        "        # load freq table and build index for each word\n",
        "        self.word2id = {}\n",
        "        self.id2word = {}\n",
        "\n",
        "        vec_file = open(vec_path, 'rt')\n",
        "        header = vec_file.readline()\n",
        "        self.vocab_size, self.word_dim = map(int, header.split())\n",
        "        word_vecs = {}\n",
        "        for line in vec_file:\n",
        "            line = line.decode('utf-8').strip()\n",
        "            parts = line.split(' ')\n",
        "            word = parts[0]\n",
        "            if (voc is not None) and (word not in voc): continue\n",
        "            vector = np.array(parts[1:], dtype='float32')\n",
        "            cur_index = len(self.word2id)\n",
        "            self.word2id[word] = cur_index\n",
        "            self.id2word[cur_index] = word\n",
        "            word_vecs[cur_index] = vector\n",
        "        vec_file.close()\n",
        "\n",
        "        self.vocab_size = len(self.word2id)\n",
        "        self.word_vecs = np.zeros((self.vocab_size+1, self.word_dim), dtype=np.float32) # the last dimension is all zero\n",
        "        for cur_index in xrange(self.vocab_size):\n",
        "            self.word_vecs[cur_index] = word_vecs[cur_index]\n",
        "\n",
        "\n",
        "    def fromText_format2(self, vec_path,voc=None,pre_word_vecs=None):\n",
        "        # load freq table and build index for each word\n",
        "        self.word2id = {}\n",
        "        self.id2word = {}\n",
        "\n",
        "        vec_file = open(vec_path, 'rt')\n",
        "        word_vecs = {}\n",
        "        for line in vec_file:\n",
        "            line = line.decode('utf-8').strip()\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) > 3:\n",
        "                parts = parts[:2] + parts[-1:]\n",
        "            cur_index = int(parts[0])\n",
        "            word = parts[1]\n",
        "            vector = np.array(map(float,re.split('\\\\s+', parts[2])), dtype='float32')\n",
        "            assert word not in self.word2id, word\n",
        "            self.word2id[word] = cur_index\n",
        "            self.id2word[cur_index] = word\n",
        "            word_vecs[cur_index] = vector\n",
        "            self.word_dim = vector.size\n",
        "        vec_file.close()\n",
        "\n",
        "        self.vocab_size = len(self.word2id)\n",
        "\n",
        "        if pre_word_vecs is not None:\n",
        "            self.word_vecs = pre_word_vecs\n",
        "        else:\n",
        "            self.word_vecs = np.zeros((self.vocab_size, self.word_dim), dtype=np.float32) # the last dimension is all zero\n",
        "            for cur_index in xrange(self.vocab_size):\n",
        "                self.word_vecs[cur_index] = word_vecs[cur_index]\n",
        "\n",
        "\n",
        "    def fromText_format3(self, vec_path,voc=None):\n",
        "        assert False, 'not in use'\n",
        "        # load freq table and build index for each word\n",
        "        self.word2id = {}\n",
        "        self.id2word = {}\n",
        "\n",
        "        self.word2id['<s>'] = 0\n",
        "        self.id2word[0] = '<s>'\n",
        "        self.word2id['</s>'] = 1\n",
        "        self.id2word[1] = '</s>'\n",
        "\n",
        "        vec_file = open(vec_path, 'rt')\n",
        "        word_vecs = {}\n",
        "        for line in vec_file:\n",
        "            line = line.decode('utf-8').strip()\n",
        "            parts = line.split(' ')\n",
        "            word = parts[0]\n",
        "            self.word_dim = len(parts[1:])\n",
        "            if (voc is not None) and (word not in voc): continue\n",
        "            vector = np.array(parts[1:], dtype='float32')\n",
        "            if self.word2id.has_key(word): cur_index = self.word2id[word]\n",
        "            else: cur_index = len(self.word2id)\n",
        "            self.word2id[word] = cur_index\n",
        "            self.id2word[cur_index] = word\n",
        "            word_vecs[cur_index] = vector\n",
        "        vec_file.close()\n",
        "\n",
        "        self.vocab_size = len(self.word2id)\n",
        "        scale = 0.05\n",
        "        self.word_vecs = np.random.uniform(low=-scale, high=scale, size=(self.vocab_size+1, self.word_dim))\n",
        "        for cur_index in xrange(self.vocab_size):\n",
        "            if not word_vecs.has_key(cur_index): continue\n",
        "            self.word_vecs[cur_index] = word_vecs[cur_index]\n",
        "\n",
        "\n",
        "\n",
        "    def fromText_bak(self, vec_path,voc=None):\n",
        "        # load freq table and build index for each word\n",
        "        self.word2id = {}\n",
        "        self.id2word = {}\n",
        "\n",
        "        vec_file = open(vec_path, 'rt')\n",
        "        header = vec_file.readline()\n",
        "        self.vocab_size, self.word_dim = map(int, header.split())\n",
        "        self.word_vecs = np.zeros((self.vocab_size+1, self.word_dim), dtype=np.float32) # the last dimension is all zero\n",
        "        for line in vec_file:\n",
        "            line = line.decode('utf-8').strip()\n",
        "            parts = line.split(' ')\n",
        "            word = parts[0]\n",
        "            if (voc is not None) and (word not in voc): continue\n",
        "            vector = np.array(parts[1:], dtype='float32')\n",
        "            cur_index = len(self.word2id)\n",
        "            self.word2id[word] = cur_index\n",
        "            self.id2word[cur_index] = word\n",
        "            self.word_vecs[cur_index] = vector\n",
        "        vec_file.close()\n",
        "\n",
        "    def fromBinary_with_voc(self, fname, voc, scale=0.05, stop_num=50):\n",
        "        self.stoplist = voc[0:stop_num]\n",
        "        voc = voc[stop_num:]\n",
        "        voc.append(self.unk_label)\n",
        "        self.word2id = {}\n",
        "        self.id2word = {}\n",
        "        for word in voc:\n",
        "            curIndex = len(self.word2id)\n",
        "            self.word2id[word] = curIndex\n",
        "            self.id2word[curIndex] = word\n",
        "\n",
        "        with open(fname, \"rb\") as f:\n",
        "            header = f.readline()\n",
        "            cur_vocab_size, self.word_dim = map(int, header.split())\n",
        "            word_vecs = {}\n",
        "            binary_len = np.dtype('float32').itemsize * self.word_dim\n",
        "            for idx in xrange(cur_vocab_size):\n",
        "                word = []\n",
        "                while True:\n",
        "                    ch = f.read(1)\n",
        "                    if ch == ' ':\n",
        "                        word = ''.join(word)\n",
        "                        break\n",
        "                    if ch != '\\n':\n",
        "                        word.append(ch)\n",
        "                if word in self.word2id.keys():\n",
        "                    curIndex = self.word2id[word]\n",
        "                else:\n",
        "                    curIndex = len(self.word2id)\n",
        "                    self.word2id[word] = curIndex\n",
        "                    self.id2word[curIndex] = word\n",
        "                word_vecs[curIndex] = np.fromstring(f.read(binary_len), dtype='float32')\n",
        "\n",
        "        self.vocab_size = len(self.word2id)\n",
        "        self.word_vecs = np.random.uniform(low=-scale, high=scale, size=(self.vocab_size+1, self.word_dim)).astype('float32')\n",
        "        self.word_vecs[self.vocab_size] = self.word_vecs[self.vocab_size] * 0.0\n",
        "        for cur_index in word_vecs.keys():\n",
        "            self.word_vecs[cur_index] = word_vecs[cur_index]\n",
        "\n",
        "    def fromBinary(self, fname, scale=0.05, voc=None):\n",
        "        self.word2id = {}\n",
        "        self.id2word = {}\n",
        "        self.word2id[self.unk_label] = 0\n",
        "        self.id2word[0] = self.unk_label\n",
        "        # load word vector\n",
        "        with open(fname, \"rb\") as f:\n",
        "            header = f.readline()\n",
        "            self.vocab_size, self.word_dim = map(int, header.split())\n",
        "            word_vecs = {}\n",
        "            binary_len = np.dtype('float32').itemsize * self.word_dim\n",
        "            for idx in xrange(self.vocab_size):\n",
        "                word = []\n",
        "                while True:\n",
        "                    ch = f.read(1)\n",
        "                    if ch == ' ':\n",
        "                        word = ''.join(word)\n",
        "                        break\n",
        "                    if ch != '\\n':\n",
        "                        word.append(ch)\n",
        "                if word == '': continue\n",
        "                curIndex = len(self.word2id)\n",
        "                self.word2id[word] = curIndex\n",
        "                self.id2word[curIndex] = word\n",
        "                word_vecs[curIndex] = np.fromstring(f.read(binary_len), dtype='float32')\n",
        "\n",
        "        # add unkwords\n",
        "        if voc is not None:\n",
        "            for word in voc:\n",
        "                if word == '': continue\n",
        "                if self.word2id.has_key(word): continue\n",
        "                curIndex = len(self.word2id)\n",
        "                self.word2id[word] = curIndex\n",
        "                self.id2word[curIndex] = word\n",
        "                word_vecs[curIndex] = np.random.uniform(low=-scale, high=scale, size=(self.word_dim,)).astype('float32')\n",
        "\n",
        "        self.vocab_size = len(self.word2id)\n",
        "        self.word_vecs = np.zeros((self.vocab_size+1, self.word_dim), dtype=np.float32) # the last dimension is all zero\n",
        "        for cur_index in xrange(self.vocab_size):\n",
        "            if cur_index ==0 : continue\n",
        "            self.word_vecs[cur_index] = word_vecs[cur_index]\n",
        "        self.word_vecs[0] = np.random.uniform(low=-scale, high=scale, size=(self.word_dim,)).astype('float32')\n",
        "\n",
        "    def setWordvec(self,word_vecs):\n",
        "        self.word_vecs = word_vecs\n",
        "\n",
        "    def hasWord(self, word):\n",
        "        return self.word2id.has_key(word)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.word2id)\n",
        "\n",
        "    def getIndex(self, word):\n",
        "        if self.stoplist is not None:\n",
        "            if word in self.stoplist:\n",
        "                return None\n",
        "        if(self.word2id.has_key(word)):\n",
        "            return self.word2id.get(word)\n",
        "        else:\n",
        "            return self.word2id.get('UNK')\n",
        "\n",
        "    def getWord(self, idx):\n",
        "        assert idx < self.vocab_size\n",
        "        return self.id2word.get(idx)\n",
        "\n",
        "    def getVector(self, word):\n",
        "        if(self.word2id.has_key(word)):\n",
        "            idx = self.word2id.get(word)\n",
        "            return self.word_vecs[idx]\n",
        "        return None\n",
        "\n",
        "    def getLexical(self, sout):\n",
        "        end_id = self.getIndex('</s>')\n",
        "        try:\n",
        "            k = sout.index(end_id)\n",
        "            sout = sout[:k+1]\n",
        "        except ValueError:\n",
        "            pass\n",
        "        slex = ' '.join([self.getWord(x) for x in sout])\n",
        "        return sout, slex\n",
        "\n",
        "    def to_index_sequence(self, sentence):\n",
        "        sentence = sentence.strip()\n",
        "        seq = []\n",
        "        for word in re.split('\\\\s+', sentence):\n",
        "            idx = self.getIndex(word)\n",
        "            if idx == None and self.__unk_mapping is not None and self.__unk_mapping.has_key(word):\n",
        "                simWord = self.__unk_mapping[word]\n",
        "                idx = self.getIndex(simWord)\n",
        "            if idx == None: idx = self.vocab_size\n",
        "            seq.append(idx)\n",
        "        return seq\n",
        "\n",
        "    def to_index_sequence_for_list(self, words):\n",
        "        seq = []\n",
        "        for word in words:\n",
        "            idx = self.getIndex(word)\n",
        "            if idx == None and self.__unk_mapping is not None and self.__unk_mapping.has_key(word):\n",
        "                simWord = self.__unk_mapping[word]\n",
        "                idx = self.getIndex(simWord)\n",
        "            if idx == None: idx = self.vocab_size\n",
        "            seq.append(idx)\n",
        "        return seq\n",
        "\n",
        "    def to_character_matrix(self, sentence, max_char_per_word=-1):\n",
        "        sentence = sentence.strip()\n",
        "        return self.to_character_matrix_for_list(re.split('\\\\s+', sentence), max_char_per_word)\n",
        "\n",
        "    def to_character_matrix_for_list(self, sentence, max_char_per_word=-1):\n",
        "        seq = []\n",
        "        for word in sentence:\n",
        "            cur_seq = []\n",
        "            for i in xrange(len(word)):\n",
        "                cur_char = word[i]\n",
        "                idx = self.getIndex(cur_char)\n",
        "                if idx == None and self.__unk_mapping is not None and self.__unk_mapping.has_key(cur_char):\n",
        "                    simWord = self.__unk_mapping[cur_char]\n",
        "                    idx = self.getIndex(simWord)\n",
        "                if idx == None: idx = self.vocab_size\n",
        "                cur_seq.append(idx)\n",
        "            if max_char_per_word!=-1 and len(cur_seq) > max_char_per_word:\n",
        "                cur_seq = cur_seq[:max_char_per_word]\n",
        "            seq.append(cur_seq)\n",
        "        return seq\n",
        "\n",
        "    def to_index_sequence4binary_features(self, sentence):\n",
        "        sentence = sentence.strip().lower()\n",
        "        seq = []\n",
        "        for word in re.split(' ', sentence):\n",
        "            idx = self.getIndex(word)\n",
        "            if idx == None: continue\n",
        "            seq.append(idx)\n",
        "        return seq\n",
        "\n",
        "    def to_char_ngram_index_sequence(self, sentence):\n",
        "        sentence = sentence.strip().lower()\n",
        "        seq = []\n",
        "        words = re.split(' ', sentence)\n",
        "        for word in words:\n",
        "            sub_words = collect_char_ngram(word)\n",
        "            for sub_word in sub_words:\n",
        "                idx = self.getIndex(sub_word)\n",
        "                if idx == None: continue\n",
        "                seq.append(idx)\n",
        "        return seq\n",
        "\n",
        "    def to_sparse_feature_sequence(self, sentence1, sentence2):\n",
        "        words1 = set(re.split(' ', sentence1.strip().lower()))\n",
        "        words2 = set(re.split(' ', sentence2.strip().lower()))\n",
        "        intersection_words = words1.intersection(words2)\n",
        "        seq = []\n",
        "        for word in intersection_words:\n",
        "            idx = self.getIndex(word)\n",
        "            if idx == None: continue\n",
        "            seq.append(idx)\n",
        "        return seq\n",
        "\n",
        "    def get_sentence_vector(self, sentence):\n",
        "        sent_vec = np.zeros((self.word_dim,), dtype='float32')\n",
        "        sentence = sentence.strip().lower()\n",
        "        total = 0.0\n",
        "        for word in re.split(' ', sentence):\n",
        "            cur_vec = self.getVector(word)\n",
        "            if cur_vec is None: continue\n",
        "            sent_vec += cur_vec\n",
        "            total += 1.0\n",
        "        if total != 0.0: sent_vec /= total\n",
        "        return sent_vec\n",
        "\n",
        "    def dump_to_txt2(self, outpath):\n",
        "        outfile = open(outpath, 'wt')\n",
        "        for word in self.word2id.keys():\n",
        "            cur_id = self.word2id[word]\n",
        "            cur_vector = self.getVector(word)\n",
        "#             print(word)\n",
        "            word= word.encode('utf-8')\n",
        "            outline = \"{}\\t{}\\t{}\".format(cur_id, word, vec2string(cur_vector))\n",
        "            outfile.write(outline + \"\\n\")\n",
        "        outfile.close()\n",
        "\n",
        "    def dump_to_txt3(self, outpath):\n",
        "        outfile = open(outpath, 'wt')\n",
        "        for word in self.word2id.keys():\n",
        "            cur_vector = self.getVector(word)\n",
        "            word= word.encode('utf-8')\n",
        "            outline = word + \" {}\".format(vec2string(cur_vector))\n",
        "            outfile.write(outline + \"\\n\")\n",
        "        outfile.close()\n",
        "\n",
        "def vec2string(val):\n",
        "    result = \"\"\n",
        "    for v in val:\n",
        "        result += \" {}\".format(v)\n",
        "    return result.strip()\n",
        "\n",
        "\n",
        "def collect_all_ngram(words, n=2):\n",
        "    all_ngrams = set()\n",
        "    for i in xrange(len(words)-n):\n",
        "        cur_ngram = words[i:i+n]\n",
        "        all_ngrams.add(' '.join(cur_ngram))\n",
        "    return all_ngrams\n",
        "\n",
        "def collect_char_ngram(word, n=3):\n",
        "    all_words = []\n",
        "    if len(word)<=n: all_words.append(word)\n",
        "    else:\n",
        "        for i in xrange(len(word)-n+1):\n",
        "            cur_word = word[i:i+3]\n",
        "            all_words.append(cur_word)\n",
        "    return all_words\n",
        "\n",
        "def to_char_ngram_sequence(sentence, n=3):\n",
        "    seq = []\n",
        "    words = re.split(' ', sentence)\n",
        "    for word in words:\n",
        "        sub_words = collect_char_ngram(word)\n",
        "        seq.extend(sub_words)\n",
        "    return ' '.join(seq)\n",
        "\n",
        "def collectVoc(trainpath):\n",
        "    vocab = set()\n",
        "    inputFile = file(trainpath, 'rt')\n",
        "    for line in inputFile:\n",
        "        line = line.strip()\n",
        "        label, sentence = re.split('\\t', line)\n",
        "        sentence = sentence.lower()\n",
        "        for word in re.split(' ', sentence):\n",
        "            vocab.add(word)\n",
        "    inputFile.close()\n",
        "    return vocab\n",
        "\n",
        "def collect_word_count(sentences, unk_num=1):\n",
        "    word_count_map = {}\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip().lower()\n",
        "        for word in re.split(' ', sentence):\n",
        "            cur_count = 0\n",
        "            if word_count_map.has_key(word):\n",
        "                cur_count = word_count_map.get(word)\n",
        "            word_count_map[word] = cur_count + 1\n",
        "    word_count_list = []\n",
        "    for word in word_count_map.keys():\n",
        "        count = word_count_map.get(word)\n",
        "        word_count_list.append((count, word))\n",
        "\n",
        "    word_count_list = sorted(word_count_list,key=(lambda a:a[0]), reverse=True)\n",
        "#     for i in xrange(50):\n",
        "#         word, count = word_count_list[i]\n",
        "#         print('{}\\t{}'.format(word, count))\n",
        "#     return word_count_list\n",
        "    return [word for count, word in word_count_list if count>unk_num ]\n",
        "\n",
        "def collect_word_count_with_max_vocab(sentences, max_vocab=600000):\n",
        "    word_count_map = {}\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip().lower()\n",
        "        for word in re.split(' ', sentence):\n",
        "            cur_count = 0\n",
        "            if word_count_map.has_key(word):\n",
        "                cur_count = word_count_map.get(word)\n",
        "            word_count_map[word] = cur_count + 1\n",
        "    word_count_list = []\n",
        "    for word in word_count_map.keys():\n",
        "        count = word_count_map.get(word)\n",
        "        word_count_list.append((count, word))\n",
        "\n",
        "    word_count_list = sorted(word_count_list,key=(lambda a:a[0]), reverse=True)\n",
        "#     for i in xrange(50):\n",
        "#         word, count = word_count_list[i]\n",
        "#         print('{}\\t{}'.format(word, count))\n",
        "#     return word_count_list\n",
        "#     return [word for count, word in word_count_list if count>unk_num ]\n",
        "    if len(word_count_list)<max_vocab: max_vocab = len(word_count_list)\n",
        "    return [word for count, word in word_count_list[:max_vocab]]\n",
        "\n",
        "def read_all_sentences(inpath):\n",
        "    all_sentences = []\n",
        "    in_file = file(inpath, 'rt')\n",
        "    for line in in_file:\n",
        "        if line.startswith('<'): continue\n",
        "        line = line.strip().lower()\n",
        "        sentences = re.split('\\t', line)\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            all_sentences.append(sentence)\n",
        "    in_file.close()\n",
        "    return all_sentences\n",
        "\n",
        "def read_sparse_features(inpath, threshold=0.0):\n",
        "    sparse_features = []\n",
        "    in_file = file(inpath, 'rt')\n",
        "    for line in in_file:\n",
        "        line = line.strip().lower()\n",
        "        items = re.split('\\t', line)\n",
        "        if len(items)!=2: continue\n",
        "        (sparse_feature, count) = items\n",
        "        count = float(count)\n",
        "        if count< threshold: continue\n",
        "        sparse_features.append(sparse_feature)\n",
        "    in_file.close()\n",
        "    return sparse_features\n",
        "\n",
        "def build_word_index_file(word_vec_path, out_path):\n",
        "    print('Loading word vectors ... ')\n",
        "    vocab = Vocab(word_vec_path)\n",
        "    print('Word_vecs shape: ', vocab.word_vecs.shape)\n",
        "    word2id = vocab.word2id\n",
        "    out_file = open(out_path,'wt')\n",
        "    out_file.write('{}\\t{}\\n'.format(len(word2id), vocab.word_dim))\n",
        "    for word in word2id.keys():\n",
        "        wid = word2id[word]\n",
        "        out_file.write('{}\\t{}\\n'.format(word, wid))\n",
        "    out_file.close()\n",
        "def load_word_index(index_path):\n",
        "    word2id = {}\n",
        "    in_file = open(index_path, 'rt')\n",
        "    started = False\n",
        "    for line in in_file:\n",
        "        items = re.split('\\t', line)\n",
        "        if not started:\n",
        "            started = True\n",
        "            vocab_size = int(items[0])\n",
        "            word_dim = int(items[1])\n",
        "        else:\n",
        "            if len(items)<2:\n",
        "                word = ''\n",
        "                word_id = int(items[0])\n",
        "            else:\n",
        "                word, word_id = items\n",
        "            word2id[word] = int(word_id)\n",
        "    in_file.close()\n",
        "    return (vocab_size, word_dim, word2id)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/tagucci/pythonrouge.git\n",
            "  Cloning https://github.com/tagucci/pythonrouge.git to /tmp/pip-req-build-MnbmQC\n",
            "  Running command git clone -q https://github.com/tagucci/pythonrouge.git /tmp/pip-req-build-MnbmQC\n",
            "Requirement already satisfied (use --upgrade to upgrade): pythonrouge==0.2 from git+https://github.com/tagucci/pythonrouge.git in /usr/local/lib/python2.7/dist-packages\n",
            "Building wheels for collected packages: pythonrouge\n",
            "  Building wheel for pythonrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-AWRkFD/wheels/fd/ff/be/6716935d513fa8656ab185cb0aa70aed382b72dda42bf09c95\n",
            "Successfully built pythonrouge\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bw7qQre50PJK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_batches(size, batch_size):\n",
        "    nb_batch = int(np.ceil(size/float(batch_size)))\n",
        "    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)] # zgwang: starting point of each batch\n",
        "\n",
        "def pad_2d_vals_no_size(in_vals, dtype=np.int32):\n",
        "    size1 = len(in_vals)\n",
        "    size2 = np.max([len(x) for x in in_vals])\n",
        "    return pad_2d_vals(in_vals, size1, size2, dtype=dtype)\n",
        "\n",
        "def pad_2d_vals(in_vals, dim1_size, dim2_size, dtype=np.int32):\n",
        "    out_val = np.zeros((dim1_size, dim2_size), dtype=dtype)\n",
        "    if dim1_size > len(in_vals): dim1_size = len(in_vals)\n",
        "    for i in xrange(dim1_size):\n",
        "        cur_in_vals = in_vals[i]\n",
        "        cur_dim2_size = dim2_size\n",
        "        if cur_dim2_size > len(cur_in_vals): cur_dim2_size = len(cur_in_vals)\n",
        "        out_val[i,:cur_dim2_size] = cur_in_vals[:cur_dim2_size]\n",
        "    return out_val\n",
        "\n",
        "def pad_3d_vals_no_size(in_vals, dtype=np.int32):\n",
        "    size1 = len(in_vals)\n",
        "    size2 = np.max([len(x) for x in in_vals])\n",
        "    size3 = 0\n",
        "    for val in in_vals:\n",
        "        cur_size3 = np.max([len(x) for x in val])\n",
        "        if size3<cur_size3: size3 = cur_size3\n",
        "    return pad_3d_vals(in_vals, size1, size2, size3, dtype=dtype)\n",
        "\n",
        "def pad_3d_vals(in_vals, dim1_size, dim2_size, dim3_size, dtype=np.int32):\n",
        "    out_val = np.zeros((dim1_size, dim2_size, dim3_size), dtype=dtype)\n",
        "    if dim1_size > len(in_vals): dim1_size = len(in_vals)\n",
        "    for i in xrange(dim1_size):\n",
        "        in_vals_i = in_vals[i]\n",
        "        cur_dim2_size = dim2_size\n",
        "        if cur_dim2_size > len(in_vals_i): cur_dim2_size = len(in_vals_i)\n",
        "        for j in xrange(cur_dim2_size):\n",
        "            in_vals_ij = in_vals_i[j]\n",
        "            cur_dim3_size = dim3_size\n",
        "            if cur_dim3_size > len(in_vals_ij): cur_dim3_size = len(in_vals_ij)\n",
        "            out_val[i, j, :cur_dim3_size] = in_vals_ij[:cur_dim3_size]\n",
        "    return out_val\n",
        "\n",
        "def pad_4d_vals(in_vals, dim1_size, dim2_size, dim3_size, dim4_size, dtype=np.int32):\n",
        "    out_val = np.zeros((dim1_size, dim2_size, dim3_size, dim4_size), dtype=dtype)\n",
        "    if dim1_size > len(in_vals): dim1_size = len(in_vals)\n",
        "    for i in xrange(dim1_size):\n",
        "        in_vals_i = in_vals[i]\n",
        "        cur_dim2_size = dim2_size\n",
        "        if cur_dim2_size > len(in_vals_i): cur_dim2_size = len(in_vals_i)\n",
        "        for j in xrange(cur_dim2_size):\n",
        "            in_vals_ij = in_vals_i[j]\n",
        "            cur_dim3_size = dim3_size\n",
        "            if cur_dim3_size > len(in_vals_ij): cur_dim3_size = len(in_vals_ij)\n",
        "            for k in xrange(cur_dim3_size):\n",
        "                in_vals_ijk = in_vals_ij[k]\n",
        "                cur_dim4_size = dim4_size\n",
        "                if cur_dim4_size > len(in_vals_ijk): cur_dim4_size = len(in_vals_ijk)\n",
        "                out_val[i, j, k, :cur_dim4_size] = in_vals_ijk[:cur_dim4_size]\n",
        "    return out_val\n",
        "\n",
        "def pad_target_labels(in_val, max_length, dtype=np.float32):\n",
        "    batch_size = len(in_val)\n",
        "    out_val = np.zeros((batch_size, max_length), dtype=dtype)\n",
        "    for i in xrange(batch_size):\n",
        "        for index in in_val[i]:\n",
        "            out_val[i,index] = 1.0\n",
        "    return out_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dqRysW5s0Ut4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Bunch(object):\n",
        "    def __init__(self, adict):\n",
        "        self.__dict__.update(adict)\n",
        "\n",
        "def save_namespace(FLAGS, out_path):\n",
        "    FLAGS_dict = vars(FLAGS)\n",
        "    with open(out_path, 'w') as fp:\n",
        "        json.dump(FLAGS_dict, fp)\n",
        "        \n",
        "def load_namespace(in_path):\n",
        "    with open(in_path, 'r') as fp:\n",
        "        FLAGS_dict = json.load(fp)\n",
        "    return Bunch(FLAGS_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MqBZ5SN30iwt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def precook(s, n=4, out=False):\n",
        "    \"\"\"Takes a string as input and returns an object that can be given to\n",
        "    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n",
        "    can take string arguments as well.\"\"\"\n",
        "    words = s.split()\n",
        "    counts = defaultdict(int)\n",
        "    for k in xrange(1,n+1):\n",
        "        for i in xrange(len(words)-k+1):\n",
        "            ngram = tuple(words[i:i+k])\n",
        "            counts[ngram] += 1\n",
        "    return (len(words), counts)\n",
        "\n",
        "def cook_refs(refs, eff=None, n=4): ## lhuang: oracle will call with \"average\"\n",
        "    '''Takes a list of reference sentences for a single segment\n",
        "    and returns an object that encapsulates everything that BLEU\n",
        "    needs to know about them.'''\n",
        "\n",
        "    reflen = []\n",
        "    maxcounts = {}\n",
        "    for ref in refs:\n",
        "        rl, counts = precook(ref, n)\n",
        "        reflen.append(rl)\n",
        "        for (ngram,count) in counts.iteritems():\n",
        "            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n",
        "\n",
        "    # Calculate effective reference sentence length.\n",
        "    if eff == \"shortest\":\n",
        "        reflen = min(reflen)\n",
        "    elif eff == \"average\":\n",
        "        reflen = float(sum(reflen))/len(reflen)\n",
        "\n",
        "    ## lhuang: N.B.: leave reflen computaiton to the very end!!\n",
        "\n",
        "    ## lhuang: N.B.: in case of \"closest\", keep a list of reflens!! (bad design)\n",
        "\n",
        "    return (reflen, maxcounts)\n",
        "\n",
        "def cook_test(test, (reflen, refmaxcounts), eff=None, n=4):\n",
        "    '''Takes a test sentence and returns an object that\n",
        "    encapsulates everything that BLEU needs to know about it.'''\n",
        "\n",
        "    testlen, counts = precook(test, n, True)\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    # Calculate effective reference sentence length.\n",
        "\n",
        "    if eff == \"closest\":\n",
        "        result[\"reflen\"] = min((abs(l-testlen), l) for l in reflen)[1]\n",
        "    else: ## i.e., \"average\" or \"shortest\" or None\n",
        "        result[\"reflen\"] = reflen\n",
        "\n",
        "    result[\"testlen\"] = testlen\n",
        "\n",
        "    result[\"guess\"] = [max(0,testlen-k+1) for k in xrange(1,n+1)]\n",
        "\n",
        "    result['correct'] = [0]*n\n",
        "    for (ngram, count) in counts.iteritems():\n",
        "        result[\"correct\"][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n",
        "\n",
        "    return result\n",
        "\n",
        "class BleuScorer(object):\n",
        "    \"\"\"Bleu scorer.\n",
        "    \"\"\"\n",
        "\n",
        "    __slots__ = \"n\", \"crefs\", \"ctest\", \"_score\", \"_ratio\", \"_testlen\", \"_reflen\", \"special_reflen\"\n",
        "    # special_reflen is used in oracle (proportional effective ref len for a node).\n",
        "\n",
        "    def copy(self):\n",
        "        ''' copy the refs.'''\n",
        "        new = BleuScorer(n=self.n)\n",
        "        new.ctest = copy.copy(self.ctest)\n",
        "        new.crefs = copy.copy(self.crefs)\n",
        "        new._score = None\n",
        "        return new\n",
        "\n",
        "    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n",
        "        ''' singular instance '''\n",
        "\n",
        "        self.n = n\n",
        "        self.crefs = []\n",
        "        self.ctest = []\n",
        "        self.cook_append(test, refs)\n",
        "        self.special_reflen = special_reflen\n",
        "\n",
        "    def cook_append(self, test, refs):\n",
        "        '''called by constructor and __iadd__ to avoid creating new instances.'''\n",
        "\n",
        "        if refs is not None:\n",
        "            self.crefs.append(cook_refs(refs))\n",
        "            if test is not None:\n",
        "                cooked_test = cook_test(test, self.crefs[-1])\n",
        "                self.ctest.append(cooked_test) ## N.B.: -1\n",
        "            else:\n",
        "                self.ctest.append(None) # lens of crefs and ctest have to match\n",
        "\n",
        "        self._score = None ## need to recompute\n",
        "\n",
        "    def ratio(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._ratio\n",
        "\n",
        "    def score_ratio(self, option=None):\n",
        "        '''return (bleu, len_ratio) pair'''\n",
        "        return (self.fscore(option=option), self.ratio(option=option))\n",
        "\n",
        "    def score_ratio_str(self, option=None):\n",
        "        return \"%.4f (%.2f)\" % self.score_ratio(option)\n",
        "\n",
        "    def reflen(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._reflen\n",
        "\n",
        "    def testlen(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._testlen\n",
        "\n",
        "    def retest(self, new_test):\n",
        "        if type(new_test) is str:\n",
        "            new_test = [new_test]\n",
        "        assert len(new_test) == len(self.crefs), new_test\n",
        "        self.ctest = []\n",
        "        for t, rs in zip(new_test, self.crefs):\n",
        "            self.ctest.append(cook_test(t, rs))\n",
        "        self._score = None\n",
        "\n",
        "        return self\n",
        "\n",
        "    def rescore(self, new_test):\n",
        "        ''' replace test(s) with new test(s), and returns the new score.'''\n",
        "\n",
        "        return self.retest(new_test).compute_score()\n",
        "\n",
        "    def size(self):\n",
        "        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n",
        "        return len(self.crefs)\n",
        "\n",
        "    def __iadd__(self, other):\n",
        "        '''add an instance (e.g., from another sentence).'''\n",
        "\n",
        "        if type(other) is tuple:\n",
        "            ## avoid creating new BleuScorer instances\n",
        "            self.cook_append(other[0], other[1])\n",
        "        else:\n",
        "            assert self.compatible(other), \"incompatible BLEUs.\"\n",
        "            self.ctest.extend(other.ctest)\n",
        "            self.crefs.extend(other.crefs)\n",
        "            self._score = None ## need to recompute\n",
        "\n",
        "        return self\n",
        "\n",
        "    def compatible(self, other):\n",
        "        return isinstance(other, BleuScorer) and self.n == other.n\n",
        "\n",
        "    def single_reflen(self, option=\"average\"):\n",
        "        return self._single_reflen(self.crefs[0][0], option)\n",
        "\n",
        "    def _single_reflen(self, reflens, option=None, testlen=None):\n",
        "\n",
        "        if option == \"shortest\":\n",
        "            reflen = min(reflens)\n",
        "        elif option == \"average\":\n",
        "            reflen = float(sum(reflens))/len(reflens)\n",
        "        elif option == \"closest\":\n",
        "            reflen = min((abs(l-testlen), l) for l in reflens)[1]\n",
        "        else:\n",
        "            assert False, \"unsupported reflen option %s\" % option\n",
        "\n",
        "        return reflen\n",
        "\n",
        "    def recompute_score(self, option=None, verbose=0):\n",
        "        self._score = None\n",
        "        return self.compute_score(option, verbose)\n",
        "\n",
        "    def compute_score(self, option=None, verbose=0):\n",
        "        n = self.n\n",
        "        small = 1e-9\n",
        "        tiny = 1e-15 ## so that if guess is 0 still return 0\n",
        "        bleu_list = [[] for _ in range(n)]\n",
        "\n",
        "        if self._score is not None:\n",
        "            return self._score\n",
        "\n",
        "        if option is None:\n",
        "            option = \"average\" if len(self.crefs) == 1 else \"closest\"\n",
        "\n",
        "        self._testlen = 0\n",
        "        self._reflen = 0\n",
        "        totalcomps = {'testlen':0, 'reflen':0, 'guess':[0]*n, 'correct':[0]*n}\n",
        "\n",
        "        # for each sentence\n",
        "        for comps in self.ctest:\n",
        "            testlen = comps['testlen']\n",
        "            self._testlen += testlen\n",
        "\n",
        "            if self.special_reflen is None: ## need computation\n",
        "                reflen = self._single_reflen(comps['reflen'], option, testlen)\n",
        "            else:\n",
        "                reflen = self.special_reflen\n",
        "\n",
        "            self._reflen += reflen\n",
        "\n",
        "            for key in ['guess','correct']:\n",
        "                for k in xrange(n):\n",
        "                    totalcomps[key][k] += comps[key][k]\n",
        "\n",
        "            # append per image bleu score\n",
        "            bleu = 1.\n",
        "            for k in xrange(n):\n",
        "                bleu *= (float(comps['correct'][k]) + tiny) \\\n",
        "                        /(float(comps['guess'][k]) + small)\n",
        "                bleu_list[k].append(bleu ** (1./(k+1)))\n",
        "            ratio = (testlen + tiny) / (reflen + small) ## N.B.: avoid zero division\n",
        "            if ratio < 1:\n",
        "                for k in xrange(n):\n",
        "                    bleu_list[k][-1] *= math.exp(1 - 1/ratio)\n",
        "\n",
        "            if verbose > 1:\n",
        "                print (comps, reflen)\n",
        "\n",
        "        totalcomps['reflen'] = self._reflen\n",
        "        totalcomps['testlen'] = self._testlen\n",
        "\n",
        "        bleus = []\n",
        "        bleu = 1.\n",
        "        for k in xrange(n):\n",
        "            bleu *= float(totalcomps['correct'][k] + tiny) \\\n",
        "                    / (totalcomps['guess'][k] + small)\n",
        "            bleus.append(bleu ** (1./(k+1)))\n",
        "        ratio = (self._testlen + tiny) / (self._reflen + small) ## N.B.: avoid zero division\n",
        "        if ratio < 1:\n",
        "            for k in xrange(n):\n",
        "                bleus[k] *= math.exp(1 - 1/ratio)\n",
        "\n",
        "        if verbose > 0:\n",
        "            print (totalcomps)\n",
        "            print (\"ratio:\", ratio)\n",
        "\n",
        "        self._score = bleus\n",
        "        return self._score, bleu_list\n",
        "\n",
        "class Bleu:\n",
        "    def __init__(self, n=4):\n",
        "        # default compute Blue score up to 4\n",
        "        self._n = n\n",
        "        self._hypo_for_image = {}\n",
        "        self.ref_for_image = {}\n",
        "\n",
        "    def compute_score(self, gts, res):\n",
        "\n",
        "        assert(gts.keys() == res.keys())\n",
        "        imgIds = gts.keys()\n",
        "\n",
        "        bleu_scorer = BleuScorer(n=self._n)\n",
        "        for id in imgIds:\n",
        "            hypo = res[id]\n",
        "            ref = gts[id]\n",
        "\n",
        "            # Sanity check.\n",
        "            assert(type(hypo) is list)\n",
        "            assert(len(hypo) == 1)\n",
        "            assert(type(ref) is list)\n",
        "            assert(len(ref) >= 1)\n",
        "\n",
        "            bleu_scorer += (hypo[0], ref)\n",
        "\n",
        "        #score, scores = bleu_scorer.compute_score(option='shortest')\n",
        "        score, scores = bleu_scorer.compute_score(option='closest', verbose=0)\n",
        "        #score, scores = bleu_scorer.compute_score(option='average', verbose=1)\n",
        "\n",
        "        # return (bleu, bleu_info)\n",
        "        return score, scores\n",
        "\n",
        "    def method(self):\n",
        "        return \"Bleu\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cSwjrYMO1TmA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def my_lcs(string, sub):\n",
        "    \"\"\"\n",
        "    Calculates longest common subsequence for a pair of tokenized strings\n",
        "    :param string : list of str : tokens from a string split using whitespace\n",
        "    :param sub : list of str : shorter string, also split using whitespace\n",
        "    :returns: length (list of int): length of the longest common subsequence between the two strings\n",
        "    Note: my_lcs only gives length of the longest common subsequence, not the actual LCS\n",
        "    \"\"\"\n",
        "    if(len(string)< len(sub)):\n",
        "        sub, string = string, sub\n",
        "\n",
        "    lengths = [[0 for i in range(0,len(sub)+1)] for j in range(0,len(string)+1)]\n",
        "\n",
        "    for j in range(1,len(sub)+1):\n",
        "        for i in range(1,len(string)+1):\n",
        "            if(string[i-1] == sub[j-1]):\n",
        "                lengths[i][j] = lengths[i-1][j-1] + 1\n",
        "            else:\n",
        "                lengths[i][j] = max(lengths[i-1][j] , lengths[i][j-1])\n",
        "\n",
        "    return lengths[len(string)][len(sub)]\n",
        "\n",
        "class Rouge():\n",
        "    '''\n",
        "    Class for computing ROUGE-L score for a set of candidate sentences for the MS COCO test set\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        # vrama91: updated the value below based on discussion with Hovey\n",
        "        self.beta = 1.2\n",
        "\n",
        "    def calc_score(self, candidate, refs):\n",
        "        \"\"\"\n",
        "        Compute ROUGE-L score given one candidate and references for an image\n",
        "        :param candidate: str : candidate sentence to be evaluated\n",
        "        :param refs: list of str : COCO reference sentences for the particular image to be evaluated\n",
        "        :returns score: int (ROUGE-L score for the candidate evaluated against references)\n",
        "        \"\"\"\n",
        "        assert(len(candidate)==1)    \n",
        "        assert(len(refs)>0)         \n",
        "        prec = []\n",
        "        rec = []\n",
        "\n",
        "        # split into tokens\n",
        "        token_c = candidate[0].split(\" \")\n",
        "        \n",
        "        for reference in refs:\n",
        "            # split into tokens\n",
        "            token_r = reference.split(\" \")\n",
        "            # compute the longest common subsequence\n",
        "            lcs = my_lcs(token_r, token_c)\n",
        "            prec.append(lcs/float(len(token_c)))\n",
        "            rec.append(lcs/float(len(token_r)))\n",
        "\n",
        "        prec_max = max(prec)\n",
        "        rec_max = max(rec)\n",
        "\n",
        "        if(prec_max!=0 and rec_max !=0):\n",
        "            score = ((1 + self.beta**2)*prec_max*rec_max)/float(rec_max + self.beta**2*prec_max)\n",
        "        else:\n",
        "            score = 0.0\n",
        "        return score\n",
        "\n",
        "    def compute_score(self, gts, res):\n",
        "        \"\"\"\n",
        "        Computes Rouge-L score given a set of reference and candidate sentences for the dataset\n",
        "        Invoked by evaluate_captions.py \n",
        "        :param hypo_for_image: dict : candidate / test sentences with \"image name\" key and \"tokenized sentences\" as values \n",
        "        :param ref_for_image: dict : reference MS-COCO sentences with \"image name\" key and \"tokenized sentences\" as values\n",
        "        :returns: average_score: float (mean ROUGE-L score computed by averaging scores for all the images)\n",
        "        \"\"\"\n",
        "        assert(gts.keys() == res.keys())\n",
        "        imgIds = gts.keys()\n",
        "\n",
        "        score = []\n",
        "        for id in imgIds:\n",
        "            hypo = res[id]\n",
        "            ref  = gts[id]\n",
        "\n",
        "            score.append(self.calc_score(hypo, ref))\n",
        "\n",
        "            # Sanity check.\n",
        "            assert(type(hypo) is list)\n",
        "            assert(len(hypo) == 1)\n",
        "            assert(type(ref) is list)\n",
        "            assert(len(ref) > 0)\n",
        "\n",
        "        average_score = np.mean(np.array(score))\n",
        "        return average_score, np.array(score)\n",
        "\n",
        "    def method(self):\n",
        "        return \"Rouge\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yqkDSIgs1Xe3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def score_all(ref, hypo):\n",
        "    scorers = [\n",
        "        (Bleu(4),[\"Bleu_1\",\"Bleu_2\",\"Bleu_3\",\"Bleu_4\"]),\n",
        "        (Rouge(),\"ROUGE_L\"),\n",
        "    ]\n",
        "    final_scores = {}\n",
        "    for scorer,method in scorers:\n",
        "        score,scores = scorer.compute_score(ref,hypo)\n",
        "        if type(score)==list:\n",
        "            for m,s in zip(method,score):\n",
        "                final_scores[m] = s\n",
        "        else:\n",
        "            final_scores[method] = score\n",
        "\n",
        "    return final_scores\n",
        "\n",
        "def score(ref, hypo):\n",
        "    scorers = [\n",
        "        (Bleu(4),[\"Bleu_1\",\"Bleu_2\",\"Bleu_3\",\"Bleu_4\"])\n",
        "\n",
        "    ]\n",
        "    final_scores = {}\n",
        "    for scorer,method in scorers:\n",
        "        score,scores = scorer.compute_score(ref,hypo)\n",
        "        if type(score)==list:\n",
        "            for m,s in zip(method,score):\n",
        "                final_scores[m] = s\n",
        "        else:\n",
        "            final_scores[method] = score\n",
        "\n",
        "    return final_scores\n",
        "\n",
        "def evaluate_captions(ref,cand):\n",
        "    hypo = {}\n",
        "    refe = {}\n",
        "    for i, caption in enumerate(cand):\n",
        "        hypo[i] = [caption,]\n",
        "        refe[i] = ref[i]\n",
        "    final_scores = score(refe, hypo)\n",
        "    return 1*final_scores['Bleu_4'] + 1*final_scores['Bleu_3'] + 0.5*final_scores['Bleu_1'] + 0.5*final_scores['Bleu_2']\n",
        "\n",
        "def evaluate(data_path='./data', split='val', get_scores=False):\n",
        "    reference_path = os.path.join(data_path, \"%s/%s.references.pkl\" %(split, split))\n",
        "    candidate_path = os.path.join(data_path, \"%s/%s.candidate.captions.pkl\" %(split, split))\n",
        "\n",
        "    # load caption data\n",
        "    with open(reference_path, 'rb') as f:\n",
        "        ref = pickle.load(f)\n",
        "    with open(candidate_path, 'rb') as f:\n",
        "        cand = pickle.load(f)\n",
        "\n",
        "    # make dictionary\n",
        "    hypo = {}\n",
        "    for i, caption in enumerate(cand):\n",
        "        hypo[i] = [caption]\n",
        "\n",
        "    # compute bleu score\n",
        "    final_scores = score_all(ref, hypo)\n",
        "\n",
        "    # print out scores\n",
        "    print ('Bleu_1:\\t',final_scores['Bleu_1'])\n",
        "    print ('Bleu_2:\\t',final_scores['Bleu_2'])\n",
        "    print ('Bleu_3:\\t',final_scores['Bleu_3'])\n",
        "    print ('Bleu_4:\\t',final_scores['Bleu_4'])\n",
        "    print ('METEOR:\\t',final_scores['METEOR'])\n",
        "    print ('ROUGE_L:',final_scores['ROUGE_L'])\n",
        "    print ('CIDEr:\\t',final_scores['CIDEr'])\n",
        "\n",
        "    if get_scores:\n",
        "        return final_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aI3VtZQj1ug6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eps = 1e-6\n",
        "def cosine_distance(y1,y2):\n",
        "    # y1 [....,a, 1, d]\n",
        "    # y2 [....,1, b, d]\n",
        "    cosine_numerator = tf.reduce_sum(tf.multiply(y1, y2), axis=-1)\n",
        "    y1_norm = tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(y1), axis=-1), eps))\n",
        "    y2_norm = tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(y2), axis=-1), eps))\n",
        "    return cosine_numerator / y1_norm / y2_norm\n",
        "\n",
        "\n",
        "def euclidean_distance(y1,y2):\n",
        "    distance = tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(y1 - y2), axis=-1), eps))\n",
        "    return distance\n",
        "\n",
        "def match_same_row_matrix(matrix1, matrix2, feature_dim, options):\n",
        "    # matrix1: [num_rows, feature_dim]\n",
        "    # matrix2: [num_rows, feature_dim]\n",
        "    input_shape = tf.shape(matrix1)\n",
        "    num_rows = input_shape[0]\n",
        "    matching_result = []\n",
        "    matching_dim = 0\n",
        "    if options.has_key('cosine'):\n",
        "        cosine_value = cosine_distance(matrix1, matrix2)\n",
        "        cosine_value = tf.reshape(cosine_value, [num_rows, 1])\n",
        "        matching_result.append(cosine_value)\n",
        "        matching_dim += 1\n",
        "\n",
        "    if options.has_key('euclidean'):\n",
        "        euclidean_value = euclidean_distance(matrix1, matrix2)\n",
        "        euclidean_value = tf.reshape(euclidean_value, [num_rows, 1])\n",
        "        matching_result.append(euclidean_value)\n",
        "        matching_dim += 1\n",
        "\n",
        "    if options.has_key('subtract'):\n",
        "        cur_matching = matrix1-matrix2\n",
        "#         cur_matching = tf.reshape(cur_matching, [num_rows, feature_dim])\n",
        "        matching_result.append(cur_matching)\n",
        "        matching_dim += feature_dim\n",
        "\n",
        "    if options.has_key('multiply'):\n",
        "        cur_matching = matrix1*matrix2\n",
        "#         cur_matching = tf.reshape(cur_matching, [num_rows, feature_dim])\n",
        "        matching_result.append(cur_matching)\n",
        "        matching_dim += feature_dim\n",
        "\n",
        "    if options.has_key('nn'):\n",
        "        (nn_dim, w, b) = options['nn']\n",
        "#         with tf.variable_scope(name_scope + \"-nn\"):\n",
        "#             w = tf.get_variable(\"w\", [2*feature_dim, nn_dim], dtype=tf.float32)\n",
        "#             b = tf.get_variable(\"b\", [nn_dim], dtype=tf.float32)\n",
        "        NN_input = tf.concat(1, [matrix1, matrix2])\n",
        "        NN_input = tf.reshape(NN_input, [num_rows, 2*feature_dim])\n",
        "        NN_matching = tf.tanh(tf.matmul(NN_input, w) + b)\n",
        "        NN_matching = tf.reshape(NN_matching, [num_rows, nn_dim])\n",
        "        matching_result.append(NN_matching)\n",
        "        matching_dim += nn_dim\n",
        "\n",
        "    matrix1_tmp = tf.expand_dims(matrix1, axis=1) #[num_rows, 'x', feature_dim]\n",
        "    matrix2_tmp = tf.expand_dims(matrix2, axis=1) #[num_rows, 'x', feature_dim]\n",
        "    if options.has_key('mp-cosine'):\n",
        "        (cosine_MP_dim, mp_cosine_params) = options['mp-cosine']\n",
        "#         mp_cosine_params = tf.get_variable(name_scope + \"-mp-cosine\", shape=[cosine_MP_dim, feature_dim], dtype=tf.float32)\n",
        "        mp_cosine_params_tmp = tf.expand_dims(mp_cosine_params, axis=0) # ['x', cosine_MP_dim, feature_dim]\n",
        "        mp_cosine_matching = cosine_distance(tf.multiply(matrix1_tmp, mp_cosine_params_tmp), tf.multiply(matrix2_tmp, mp_cosine_params_tmp))\n",
        "#         mp_cosine_matching = tf.reshape(mp_cosine_matching, [num_rows, cosine_MP_dim])\n",
        "        matching_result.append(mp_cosine_matching)\n",
        "        matching_dim += cosine_MP_dim\n",
        "\n",
        "    if options.has_key('mp-euclidean'):\n",
        "        (euclidean_MP_dim, mp_euclidean_params) = options['mp-euclidean']\n",
        "#         mp_euclidean_params = tf.get_variable(name_scope + \"-mp-euclidean\", shape=[euclidean_MP_dim, feature_dim], dtype=tf.float32)\n",
        "        mp_euclidean_params_tmp = tf.expand_dims(mp_euclidean_params, axis=0) # ['x', euclidean_MP_dim, feature_dim]\n",
        "        mp_euclidean_matching = euclidean_distance(tf.multiply(matrix1_tmp, mp_euclidean_params_tmp), tf.multiply(matrix2_tmp, mp_euclidean_params_tmp))\n",
        "#         mp_euclidean_matching = tf.reshape(mp_euclidean_matching, [num_rows, euclidean_MP_dim])\n",
        "        matching_result.append(mp_euclidean_matching)\n",
        "        matching_dim += euclidean_MP_dim\n",
        "\n",
        "    matching_result = tf.concat(1, matching_result)\n",
        "    return (matching_result, matching_dim)\n",
        "\n",
        "def match_matrix_bak(matrix1, matrix2, feature_dim, options):\n",
        "    # matrix1: [num_rows1, feature_dim]\n",
        "    # matrix2: [num_rows2, feature_dim]\n",
        "    num_rows1 = tf.shape(matrix1)[0]\n",
        "    num_rows2 = tf.shape(matrix2)[0]\n",
        "\n",
        "    matrix1_tmp = tf.expand_dims(matrix1, axis=1) # [num_rows1, 'x', feature_dim]\n",
        "    matrix1_tmp = tf.tile(matrix1_tmp, [1, num_rows2, 1], name=None)# [num_rows1, num_rows2, feature_dim]\n",
        "    matrix1_tmp = tf.reshape(matrix1_tmp, [num_rows1*num_rows2, feature_dim])\n",
        "\n",
        "    matrix2_tmp = tf.expand_dims(matrix2, axis=0) # ['x', num_rows1, feature_dim]\n",
        "    matrix2_tmp = tf.tile(matrix2_tmp, [num_rows1, 1, 1], name=None)# [num_rows1, num_rows2, feature_dim]\n",
        "    matrix2_tmp = tf.reshape(matrix2_tmp, [num_rows1*num_rows2, feature_dim])\n",
        "\n",
        "    (matching_result, matching_dim) = match_same_row_matrix(matrix1_tmp, matrix2_tmp, feature_dim, options)\n",
        "    matching_result = tf.reshape(matching_result, [num_rows1, num_rows2, matching_dim])\n",
        "    return (matching_result, matching_dim)\n",
        "\n",
        "def match_matrix_bak2(matrix1, matrix2, feature_dim, options):\n",
        "    # matrix1: [num_rows1, feature_dim]\n",
        "    # matrix2: [num_rows2, feature_dim]\n",
        "    num_rows2 = tf.shape(matrix2)[0]\n",
        "    def singel_instance(x):\n",
        "        # x: [feature_dim]\n",
        "        x = tf.reshape(x, [1, feature_dim]) # ['x', feature_dim]\n",
        "        x = tf.tile(x, [num_rows2, 1])# [num_rows2, feature_dim]\n",
        "        (cur_matching_result, _) = match_same_row_matrix(x, matrix2, feature_dim, options)\n",
        "        return cur_matching_result\n",
        "    matching_result = tf.map_fn(singel_instance, matrix1, dtype=tf.float32) # [num_rows1, num_rows2, matching_dim]\n",
        "    matching_dim = options['matching_dim']\n",
        "    return (matching_result, matching_dim)\n",
        "\n",
        "def match_matrix(matrix1, matrix2, feature_dim, options):\n",
        "    # matrix1: [num_rows1, feature_dim]\n",
        "    # matrix2: [num_rows2, feature_dim]\n",
        "    num_rows1 = tf.shape(matrix1)[0]\n",
        "    num_rows2 = tf.shape(matrix2)[0]\n",
        "    def singel_instance(x):\n",
        "        # x: [feature_dim]\n",
        "        x = tf.reshape(x, [1, feature_dim]) # ['x', feature_dim]\n",
        "        def single_word(y):\n",
        "            # y: {features_dim}\n",
        "            y = tf.reshape(y, [1, feature_dim]) # ['x', feature_dim]\n",
        "            (cur_matching_result, _) = match_same_row_matrix(x, y, feature_dim, options)\n",
        "            return cur_matching_result\n",
        "        return tf.map_fn(single_word, matrix2, dtype=tf.float32) # [num_rows1, num_rows2, matching_dim]\n",
        "    matching_dim = options['matching_dim']\n",
        "    matching_result = tf.map_fn(singel_instance, matrix1, dtype=tf.float32) # [num_rows1, num_rows2, matching_dim]\n",
        "    matching_result = tf.reshape(matching_result, [num_rows1, num_rows2, matching_dim])\n",
        "    return (matching_result, matching_dim)\n",
        "\n",
        "\n",
        "def create_matching_params(feature_dim, options, name_scope):\n",
        "    options_with_params = {}\n",
        "    matching_dim = 0\n",
        "    if options.with_cosine:\n",
        "        options_with_params['cosine'] = 'cosine'\n",
        "        matching_dim += 1\n",
        "\n",
        "    if options.with_euclidean:\n",
        "        options_with_params['euclidean'] = 'euclidean'\n",
        "        matching_dim += 1\n",
        "\n",
        "    if options.with_subtract:\n",
        "        options_with_params['subtract'] = 'subtract'\n",
        "        matching_dim += feature_dim\n",
        "\n",
        "    if options.with_multiply:\n",
        "        options_with_params['multiply'] = 'multiply'\n",
        "        matching_dim += feature_dim\n",
        "\n",
        "    if options.with_nn_match:\n",
        "        nn_dim = options.nn_match_dim\n",
        "        with tf.variable_scope(name_scope + \"-nn\"):\n",
        "            w = tf.get_variable(\"w_nn_match\", [2*feature_dim, nn_dim], dtype=tf.float32)\n",
        "            b = tf.get_variable(\"b_nn_match\", [nn_dim], dtype=tf.float32)\n",
        "        options_with_params['nn'] = (nn_dim, w, b)\n",
        "        matching_dim += nn_dim\n",
        "\n",
        "    if options.with_mp_cosine:\n",
        "        cosine_MP_dim = options.cosine_MP_dim\n",
        "        mp_cosine_params = tf.get_variable(name_scope + \"-mp-cosine\", shape=[cosine_MP_dim, feature_dim], dtype=tf.float32)\n",
        "        options_with_params['mp-cosine'] = (cosine_MP_dim, mp_cosine_params)\n",
        "        matching_dim += cosine_MP_dim\n",
        "\n",
        "    if options.with_mp_euclidean:\n",
        "        euclidean_MP_dim = options.euclidean_MP_dim\n",
        "        mp_euclidean_params = tf.get_variable(name_scope + \"-mp-euclidean\", shape=[euclidean_MP_dim, feature_dim], dtype=tf.float32)\n",
        "        options_with_params['mp-euclidean'] = (euclidean_MP_dim, mp_euclidean_params)\n",
        "        matching_dim += euclidean_MP_dim\n",
        "    options_with_params['matching_dim'] = matching_dim\n",
        "    return options_with_params\n",
        "\n",
        "def calculate_full_matching(passage_representation, full_question_representation, feature_dim, options, name_scope):\n",
        "    # passage_representation: [batch_size, passage_len, feature_dim]\n",
        "    # full_question_representation: [batch_size, feature_dim]\n",
        "\n",
        "    # create parameters\n",
        "    options_with_params = create_matching_params(feature_dim, options, name_scope)\n",
        "    matching_dim = options_with_params['matching_dim']\n",
        "\n",
        "    in_shape = tf.shape(passage_representation)\n",
        "    batch_size = in_shape[0]\n",
        "    passage_len = in_shape[1]\n",
        "\n",
        "    def singel_instance(x):\n",
        "        p = x[0]\n",
        "        q = x[1]\n",
        "        # p: [pasasge_len, feature_dim], q: [feature_dim]\n",
        "        q = tf.expand_dims(q, axis=0) # ['x', feature_dim]\n",
        "        q = tf.tile(q, [passage_len, 1])# [pasasge_len, feature_dim]\n",
        "        (cur_matching_result, _) = match_same_row_matrix(p, q, feature_dim, options_with_params)\n",
        "        return cur_matching_result\n",
        "    elems = (passage_representation, full_question_representation)\n",
        "    matching_result = tf.map_fn(singel_instance, elems, dtype=tf.float32) # [batch_size, passage_len, decompse_dim]\n",
        "#     matching_result = tf.reshape(matching_result, [batch_size, passage_len, matching_dim])\n",
        "    return (matching_result, matching_dim)\n",
        "\n",
        "def calculate_maxpooling_matching(passage_rep, question_rep, feature_dim, options, name_scope):\n",
        "    # passage_representation: [batch_size, passage_len, dim]\n",
        "    # qusetion_representation: [batch_size, question_len, dim]\n",
        "\n",
        "    # create parameters\n",
        "    options_with_params = create_matching_params(feature_dim, options, name_scope)\n",
        "    matching_dim = options_with_params['matching_dim']\n",
        "\n",
        "    in_shape = tf.shape(passage_rep)\n",
        "    batch_size = in_shape[0]\n",
        "    passage_len = in_shape[1]\n",
        "\n",
        "    def singel_instance(x):\n",
        "        p = x[0] # p: [pasasge_len, dim]\n",
        "        q = x[1] # q: [question_len, dim]\n",
        "        (cur_matching_result, _) = match_matrix(p, q, feature_dim, options_with_params)\n",
        "        return cur_matching_result # [pasasge_len, question_len, matching_dim]\n",
        "    elems = (passage_rep, question_rep)\n",
        "    matching_result = tf.map_fn(singel_instance, elems, dtype=tf.float32) # [batch_size, passage_len, question_len, matching_dim]\n",
        "    matching_result = tf.concat(2, [tf.reduce_max(matching_result, axis=2), tf.reduce_mean(matching_result, axis=2)])\n",
        "#     matching_result = tf.reshape(matching_result, [batch_size, passage_len, 2*matching_dim])\n",
        "    return (matching_result, 2*matching_dim)\n",
        "\n",
        "def calculate_attentive_matching(passage_rep, att_question_rep, feature_dim, options, name_scope):\n",
        "    # passage_rep: [batch_size, passage_len, dim]\n",
        "    # att_question_rep: [batch_size, passage_len, dim]\n",
        "\n",
        "    # create parameters\n",
        "    options_with_params = create_matching_params(feature_dim, options, name_scope)\n",
        "    matching_dim = options_with_params['matching_dim']\n",
        "\n",
        "    in_shape = tf.shape(passage_rep)\n",
        "    batch_size = in_shape[0]\n",
        "    passage_len = in_shape[1]\n",
        "\n",
        "    def singel_instance(x):\n",
        "        p = x[0] # p: [pasasge_len, dim]\n",
        "        q = x[1] # q: [pasasge_len, dim]\n",
        "        (cur_matching_result, _) = match_same_row_matrix(p, q, feature_dim, options_with_params)\n",
        "        return cur_matching_result\n",
        "\n",
        "    elems = (passage_rep, att_question_rep)\n",
        "    matching_result = tf.map_fn(singel_instance, elems, dtype=tf.float32)\n",
        "#     matching_result = tf.reshape(matching_result, [batch_size, passage_len, matching_dim])\n",
        "    return (matching_result, matching_dim)\n",
        "\n",
        "def calculate_cosine_weighted_question_representation(question_representation, cosine_matrix, normalize=False):\n",
        "    # question_representation: [batch_size, question_len, dim]\n",
        "    # cosine_matrix: [batch_size, passage_len, question_len]\n",
        "    if normalize: cosine_matrix = tf.nn.softmax(cosine_matrix)\n",
        "    expanded_cosine_matrix = tf.expand_dims(cosine_matrix, axis=-1) # [batch_size, passage_len, question_len, 'x']\n",
        "    weighted_question_words = tf.expand_dims(question_representation, axis=1) # [batch_size, 'x', question_len, dim]\n",
        "    weighted_question_words = tf.reduce_sum(tf.multiply(weighted_question_words, expanded_cosine_matrix), axis=2)# [batch_size, passage_len, dim]\n",
        "    if not normalize:\n",
        "        weighted_question_words = tf.div(weighted_question_words, tf.expand_dims(tf.add(tf.reduce_sum(cosine_matrix, axis=-1),eps),axis=-1))\n",
        "    return weighted_question_words # [batch_size, passage_len, dim]\n",
        "\n",
        "def calculate_max_question_representation(question_representation, cosine_matrix):\n",
        "    # question_representation: [batch_size, question_len, dim]\n",
        "    # cosine_matrix: [batch_size, passage_len, question_len]\n",
        "    question_index = tf.arg_max(cosine_matrix, 2) # [batch_size, passage_len]\n",
        "    def singel_instance(x):\n",
        "        q = x[0]\n",
        "        c = x[1]\n",
        "        return tf.gather(q, c)\n",
        "    elems = (question_representation, question_index)\n",
        "    return tf.map_fn(singel_instance, elems, dtype=tf.float32) # [batch_size, passage_len, decompse_dim]\n",
        "\n",
        "def calculate_local_question_representation(question_representation, cosine_matrix, win_size):\n",
        "    # question_representation: [batch_size, question_len, dim]\n",
        "    # cosine_matrix: [batch_size, passage_len, question_len]\n",
        "    in_shape = tf.shape(question_representation)\n",
        "#     batch_size = in_shape[0]\n",
        "    question_len = tf.cast(in_shape[1], tf.int64)\n",
        "    question_index = tf.arg_max(cosine_matrix, 2) # [batch_size, passage_len]\n",
        "    def singel_instance(x):\n",
        "        q = x[0] # question_representation: [question_len, dim]\n",
        "        c = x[1] # question_index: [question_len]\n",
        "        result = tf.gather(q, c)\n",
        "        for i in xrange(win_size):\n",
        "            cur_index = tf.subtract(c, i+1)\n",
        "            cur_index = tf.maximum(cur_index, 0)\n",
        "            result = result + tf.gather(q, cur_index)\n",
        "\n",
        "        for i in xrange(1, win_size):\n",
        "            cur_index = tf.add(c, i+1)\n",
        "            cur_index = tf.minimum(cur_index, question_len-1)\n",
        "            result = result + tf.gather(q, cur_index)\n",
        "\n",
        "        return result / (2*win_size + 1)\n",
        "    elems = (question_representation, question_index)\n",
        "    return tf.map_fn(singel_instance, elems, dtype=tf.float32) # [batch_size, passage_len, decompse_dim]\n",
        "\n",
        "def cal_linear_decomposition_representation(passage_representation, passage_lengths, cosine_matrix,is_training,\n",
        "                                            lex_decompsition_dim, dropout_rate):\n",
        "    # passage_representation: [batch_size, passage_len, dim]\n",
        "    # cosine_matrix: [batch_size, passage_len, question_len]\n",
        "    passage_similarity = tf.reduce_max(cosine_matrix, 2)# [batch_size, passage_len]\n",
        "    similar_weights = tf.expand_dims(passage_similarity, -1) # [batch_size, passage_len, 1]\n",
        "    dissimilar_weights = tf.subtract(1.0, similar_weights)\n",
        "    similar_component = tf.multiply(passage_representation, similar_weights)\n",
        "    dissimilar_component = tf.multiply(passage_representation, dissimilar_weights)\n",
        "    all_component = tf.concat(2, [similar_component, dissimilar_component])\n",
        "    if lex_decompsition_dim==-1:\n",
        "        return all_component\n",
        "    with tf.variable_scope('lex_decomposition'):\n",
        "        lex_lstm_cell_fw = tf.nn.rnn_cell.BasicLSTMCell(lex_decompsition_dim)\n",
        "        lex_lstm_cell_bw = tf.nn.rnn_cell.BasicLSTMCell(lex_decompsition_dim)\n",
        "        if is_training:\n",
        "            lex_lstm_cell_fw = tf.nn.rnn_cell.DropoutWrapper(lex_lstm_cell_fw, output_keep_prob=(1 - dropout_rate))\n",
        "            lex_lstm_cell_bw = tf.nn.rnn_cell.DropoutWrapper(lex_lstm_cell_bw, output_keep_prob=(1 - dropout_rate))\n",
        "        lex_lstm_cell_fw = tf.nn.rnn_cell.MultiRNNCell([lex_lstm_cell_fw])\n",
        "        lex_lstm_cell_bw = tf.nn.rnn_cell.MultiRNNCell([lex_lstm_cell_bw])\n",
        "\n",
        "        (lex_features_fw, lex_features_bw), _ = rnn.bidirectional_dynamic_rnn(\n",
        "                    lex_lstm_cell_fw, lex_lstm_cell_bw, all_component, dtype=tf.float32, sequence_length=passage_lengths)\n",
        "\n",
        "        lex_features = tf.concat(2, [lex_features_fw, lex_features_bw])\n",
        "    return lex_features\n",
        "\n",
        "\n",
        "\n",
        "def cal_relevancy_matrix(in_question_repres, in_passage_repres):\n",
        "    in_question_repres_tmp = tf.expand_dims(in_question_repres, 1) # [batch_size, 1, question_len, dim]\n",
        "    in_passage_repres_tmp = tf.expand_dims(in_passage_repres, 2) # [batch_size, passage_len, 1, dim]\n",
        "    relevancy_matrix = cosine_distance(in_question_repres_tmp,in_passage_repres_tmp) # [batch_size, passage_len, question_len]\n",
        "    return relevancy_matrix\n",
        "\n",
        "def mask_relevancy_matrix(relevancy_matrix, question_mask, passage_mask):\n",
        "    # relevancy_matrix: [batch_size, passage_len, question_len]\n",
        "    # question_mask: [batch_size, question_len]\n",
        "    # passage_mask: [batch_size, passsage_len]\n",
        "    if question_mask is not None:\n",
        "        relevancy_matrix = tf.multiply(relevancy_matrix, tf.expand_dims(question_mask, 1))\n",
        "    relevancy_matrix = tf.multiply(relevancy_matrix, tf.expand_dims(passage_mask, 2))\n",
        "    return relevancy_matrix\n",
        "\n",
        "def match_passage_with_question(passage_context_representation_fw, passage_context_representation_bw, mask,\n",
        "                                question_context_representation_fw, question_context_representation_bw,question_mask,\n",
        "                                context_lstm_dim, with_full_matching=True, with_attentive_matching=True,\n",
        "                                with_max_attentive_matching=True, with_maxpooling_matching=True,\n",
        "                                with_forward_match=True, with_backward_match=True, match_options=None,\n",
        "                                with_local_attentive_matching=False, win_size=3):\n",
        "    all_question_aware_representatins = []\n",
        "    dim = 0\n",
        "\n",
        "    if with_forward_match:\n",
        "        if question_mask is not None:\n",
        "            question_context_representation_fw = tf.multiply(question_context_representation_fw, tf.expand_dims(question_mask,-1))\n",
        "        passage_context_representation_fw = tf.multiply(passage_context_representation_fw, tf.expand_dims(mask,-1))\n",
        "\n",
        "    if with_backward_match:\n",
        "        if question_mask is not None:\n",
        "            question_context_representation_bw = tf.multiply(question_context_representation_bw, tf.expand_dims(question_mask,-1))\n",
        "        passage_context_representation_bw = tf.multiply(passage_context_representation_bw, tf.expand_dims(mask,-1))\n",
        "\n",
        "    if with_full_matching:\n",
        "        # forward full matching\n",
        "        if with_forward_match:\n",
        "            fw_question_full_rep = question_context_representation_fw[:,-1,:]\n",
        "            (fw_full_match_rep, matching_dim) = calculate_full_matching(passage_context_representation_fw, fw_question_full_rep,\n",
        "                                                                         context_lstm_dim, match_options, 'fw_full_match')\n",
        "            all_question_aware_representatins.append(fw_full_match_rep)\n",
        "            dim += matching_dim\n",
        "\n",
        "        # backward full matching\n",
        "        if with_backward_match:\n",
        "            bw_question_full_rep = question_context_representation_bw[:,0,:]\n",
        "            (bw_full_match_rep, matching_dim) = calculate_full_matching(passage_context_representation_bw, bw_question_full_rep,\n",
        "                                                                         context_lstm_dim, match_options, 'bw_full_match')\n",
        "            all_question_aware_representatins.append(bw_full_match_rep)\n",
        "            dim += matching_dim\n",
        "\n",
        "    if with_maxpooling_matching:\n",
        "        # forward Maxpooling-Matching\n",
        "        if with_forward_match:\n",
        "            (fw_maxpooling_rep, matching_dim) = calculate_maxpooling_matching(passage_context_representation_fw,\n",
        "                                        question_context_representation_fw, context_lstm_dim, match_options, 'fw_maxpooling_match')\n",
        "            all_question_aware_representatins.append(fw_maxpooling_rep)\n",
        "            dim += matching_dim\n",
        "\n",
        "        # backward Maxpooling-Matching\n",
        "        if with_backward_match:\n",
        "            (bw_maxpooling_rep, matching_dim) = calculate_maxpooling_matching(passage_context_representation_bw,\n",
        "                                    question_context_representation_bw, context_lstm_dim, match_options, 'bw_maxpooling_match')\n",
        "            all_question_aware_representatins.append(bw_maxpooling_rep)\n",
        "            dim += matching_dim\n",
        "\n",
        "    if with_forward_match:\n",
        "        forward_relevancy_matrix = cal_relevancy_matrix(question_context_representation_fw, passage_context_representation_fw)\n",
        "        forward_relevancy_matrix = mask_relevancy_matrix(forward_relevancy_matrix, question_mask, mask)\n",
        "\n",
        "    if with_backward_match:\n",
        "        backward_relevancy_matrix = cal_relevancy_matrix(question_context_representation_bw, passage_context_representation_bw)\n",
        "        backward_relevancy_matrix = mask_relevancy_matrix(backward_relevancy_matrix, question_mask, mask)\n",
        "\n",
        "    if with_attentive_matching:\n",
        "        # forward attentive-matching\n",
        "        if with_forward_match:\n",
        "            att_question_fw_contexts = calculate_cosine_weighted_question_representation(question_context_representation_fw,\n",
        "                                                                                     forward_relevancy_matrix)\n",
        "            (fw_attentive_rep, matching_dim) = calculate_attentive_matching(passage_context_representation_fw,\n",
        "                                                    att_question_fw_contexts, context_lstm_dim, match_options, 'fw_attentive_match')\n",
        "            all_question_aware_representatins.append(fw_attentive_rep)\n",
        "            dim += matching_dim\n",
        "\n",
        "        # backward attentive-matching\n",
        "        if with_backward_match:\n",
        "            att_question_bw_contexts = calculate_cosine_weighted_question_representation(question_context_representation_bw,\n",
        "                                                                                     backward_relevancy_matrix)\n",
        "            (bw_attentive_rep, matching_dim) = calculate_attentive_matching(passage_context_representation_bw,\n",
        "                                                    att_question_bw_contexts, context_lstm_dim, match_options, 'bw_attentive_match')\n",
        "            all_question_aware_representatins.append(bw_attentive_rep)\n",
        "            dim += matching_dim\n",
        "\n",
        "    if with_max_attentive_matching:\n",
        "        # forward max attentive-matching\n",
        "        if with_forward_match:\n",
        "            max_att_fw = calculate_max_question_representation(question_context_representation_fw, forward_relevancy_matrix)\n",
        "            (fw_max_attentive_rep, matching_dim) = calculate_attentive_matching(passage_context_representation_fw,\n",
        "                                                    max_att_fw, context_lstm_dim, match_options, 'fw_max_attentive_match')\n",
        "            all_question_aware_representatins.append(fw_max_attentive_rep)\n",
        "            dim += matching_dim\n",
        "\n",
        "        # backward max attentive-matching\n",
        "        if with_backward_match:\n",
        "            max_att_bw = calculate_max_question_representation(question_context_representation_bw, backward_relevancy_matrix)\n",
        "            (bw_max_attentive_rep, matching_dim) = calculate_attentive_matching(passage_context_representation_bw,\n",
        "                                                    max_att_bw, context_lstm_dim, match_options, 'bw_max_attentive_match')\n",
        "            all_question_aware_representatins.append(bw_max_attentive_rep)\n",
        "            dim += matching_dim\n",
        "\n",
        "    if with_local_attentive_matching:\n",
        "        # forward max attentive-matching\n",
        "        if with_forward_match:\n",
        "            local_att_fw = calculate_local_question_representation(question_context_representation_fw, forward_relevancy_matrix, win_size)\n",
        "            (fw_local_attentive_rep, matching_dim) = calculate_attentive_matching(passage_context_representation_fw,\n",
        "                                                    local_att_fw, context_lstm_dim, match_options, 'fw_local_attentive_match')\n",
        "            all_question_aware_representatins.append(fw_local_attentive_rep)\n",
        "            dim += matching_dim\n",
        "\n",
        "        # backward max attentive-matching\n",
        "        if with_backward_match:\n",
        "            local_att_bw = calculate_local_question_representation(question_context_representation_bw, backward_relevancy_matrix, win_size)\n",
        "            (bw_local_attentive_rep, matching_dim) = calculate_attentive_matching(passage_context_representation_bw,\n",
        "                                                    local_att_bw, context_lstm_dim, match_options, 'bw_local_attentive_match')\n",
        "            all_question_aware_representatins.append(bw_local_attentive_rep)\n",
        "            dim += matching_dim\n",
        "\n",
        "    if with_forward_match:\n",
        "        all_question_aware_representatins.append(tf.reduce_max(forward_relevancy_matrix, axis=2,keep_dims=True))\n",
        "        all_question_aware_representatins.append(tf.reduce_mean(forward_relevancy_matrix, axis=2,keep_dims=True))\n",
        "        dim += 2\n",
        "\n",
        "    if with_backward_match:\n",
        "        all_question_aware_representatins.append(tf.reduce_max(backward_relevancy_matrix, axis=2,keep_dims=True))\n",
        "        all_question_aware_representatins.append(tf.reduce_mean(backward_relevancy_matrix, axis=2,keep_dims=True))\n",
        "        dim += 2\n",
        "    return (all_question_aware_representatins, dim)\n",
        "\n",
        "def cross_entropy(logits, truth, mask):\n",
        "    # logits: [batch_size, passage_len]\n",
        "    # truth: [batch_size, passage_len]\n",
        "    # mask: [batch_size, passage_len]\n",
        "\n",
        "#     xdev = x - x.max()\n",
        "#     return xdev - T.log(T.sum(T.exp(xdev)))\n",
        "    logits = tf.multiply(logits, mask)\n",
        "    xdev = tf.subtract(logits, tf.expand_dims(tf.reduce_max(logits, 1), -1))\n",
        "    log_predictions = tf.subtract(xdev, tf.expand_dims(tf.log(tf.reduce_sum(tf.exp(xdev),-1)),-1))\n",
        "#     return -T.sum(targets * log_predictions)\n",
        "    result = tf.multiply(tf.multiply(truth, log_predictions), mask) # [batch_size, passage_len]\n",
        "    return tf.multiply(-1.0,tf.reduce_sum(result, -1)) # [batch_size]\n",
        "\n",
        "def highway_layer(in_val, output_size, scope=None):\n",
        "    # in_val: [batch_size, passage_len, dim]\n",
        "    input_shape = tf.shape(in_val)\n",
        "    batch_size = input_shape[0]\n",
        "    passage_len = input_shape[1]\n",
        "#     feat_dim = input_shape[2]\n",
        "    in_val = tf.reshape(in_val, [batch_size * passage_len, output_size])\n",
        "    with tf.variable_scope(scope or \"highway_layer\"):\n",
        "        highway_w = tf.get_variable(\"highway_w\", [output_size, output_size], dtype=tf.float32)\n",
        "        highway_b = tf.get_variable(\"highway_b\", [output_size], dtype=tf.float32)\n",
        "        full_w = tf.get_variable(\"full_w\", [output_size, output_size], dtype=tf.float32)\n",
        "        full_b = tf.get_variable(\"full_b\", [output_size], dtype=tf.float32)\n",
        "        trans = tf.nn.tanh(tf.nn.xw_plus_b(in_val, full_w, full_b))\n",
        "        gate = tf.nn.sigmoid(tf.nn.xw_plus_b(in_val, highway_w, highway_b))\n",
        "        outputs = tf.add(tf.multiply(trans, gate), tf.multiply(in_val, tf.subtract(1.0, gate)), \"y\")\n",
        "    outputs = tf.reshape(outputs, [batch_size, passage_len, output_size])\n",
        "    return outputs\n",
        "\n",
        "def multi_highway_layer(in_val, output_size, num_layers, scope=None):\n",
        "    scope_name = 'highway_layer'\n",
        "    if scope is not None: scope_name = scope\n",
        "    for i in xrange(num_layers):\n",
        "        cur_scope_name = scope_name + \"-{}\".format(i)\n",
        "        in_val = highway_layer(in_val, output_size, scope=cur_scope_name)\n",
        "    return in_val\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9oIugRt116FP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def collect_neighbor_node_representations(representation, positions):\n",
        "    # representation: [batch_size, num_nodes, feature_dim]\n",
        "    # positions: [batch_size, num_nodes, num_neighbors]\n",
        "    feature_dim = tf.shape(representation)[2]\n",
        "    input_shape = tf.shape(positions)\n",
        "    batch_size = input_shape[0]\n",
        "    num_nodes = input_shape[1]\n",
        "    num_neighbors = input_shape[2]\n",
        "    positions_flat = tf.reshape(positions, [batch_size, num_nodes*num_neighbors])\n",
        "    def singel_instance(x):\n",
        "        # x[0]: [num_nodes, feature_dim]\n",
        "        # x[1]: [num_nodes*num_neighbors]\n",
        "        return tf.gather(x[0], x[1])\n",
        "    elems = (representation, positions_flat)\n",
        "    representations = tf.map_fn(singel_instance, elems, dtype=tf.float32)\n",
        "    return tf.reshape(representations, [batch_size, num_nodes, num_neighbors, feature_dim])\n",
        "\n",
        "def collect_node_representations(representation, positions):\n",
        "    # representation: [batch_size, num_nodes, feature_dim]\n",
        "    # positions: [batch_size, num_candidate_nodes]\n",
        "    def singel_instance(x):\n",
        "        # x[0]: [num_nodes, feature_dim]\n",
        "        # x[1]: [num_candidate_nodes]\n",
        "        return tf.gather(x[0], x[1])\n",
        "    elems = (representation, positions)\n",
        "    return tf.map_fn(singel_instance, elems, dtype=tf.float32) # [batch_size, num_candidate_nodes, feature_dim]\n",
        "\n",
        "\n",
        "def graph_match(in_question_repres, in_passage_repres, \n",
        "                question_mask, passage_mask, edge_embedding,\n",
        "                question_neighbor_indices, passage_neighbor_indices,\n",
        "                question_neighbor_edges, passage_neighbor_edges,\n",
        "                question_neighbor_size, passage_neighbor_size,\n",
        "                neighbor_vector_dim, input_dim, edge_dim, num_syntax_match_layer,\n",
        "                with_attentive_matching=True, with_max_attentive_matching=True,\n",
        "                with_maxpooling_matching=True, match_options=None): \n",
        "\n",
        "    all_matching_vectors = []\n",
        "    all_matching_dim = 0\n",
        "    \n",
        "    \n",
        "    input_shape = tf.shape(question_neighbor_indices)\n",
        "    batch_size = input_shape[0]\n",
        "    question_len = input_shape[1]\n",
        "    num_question_neighbors = input_shape[2]\n",
        "\n",
        "    input_shape = tf.shape(passage_neighbor_indices)\n",
        "#     batch_size = input_shape[0]\n",
        "    passage_len = input_shape[1]\n",
        "    num_passage_neighbors = input_shape[2]\n",
        "    \n",
        "    question_neighbor_mask = tf.sequence_mask(tf.reshape(question_neighbor_size, [-1]), num_question_neighbors, dtype=tf.float32)\n",
        "    question_neighbor_mask = tf.reshape(question_neighbor_mask, [batch_size, question_len, num_question_neighbors])\n",
        "\n",
        "    passage_neighbor_mask = tf.sequence_mask(tf.reshape(passage_neighbor_size, [-1]), num_passage_neighbors, dtype=tf.float32)\n",
        "    passage_neighbor_mask = tf.reshape(passage_neighbor_mask, [batch_size, passage_len, num_passage_neighbors])\n",
        "\n",
        "    question_neighbor_edge_representations = tf.nn.embedding_lookup(edge_embedding, question_neighbor_edges) \n",
        "    # [batch_size, question_len, num_question_neighbors, edge_dim]\n",
        "    passage_neighbor_edge_representations = tf.nn.embedding_lookup(edge_embedding, passage_neighbor_edges) \n",
        "    # [batch_size, passage_len, num_passage_neighbors, edge_dim]\n",
        "    question_neighbor_node_representations = collect_neighbor_node_representations(\n",
        "                                                    in_question_repres, question_neighbor_indices)\n",
        "    # [batch_size, question_len, num_question_neighbors, input_dim]\n",
        "    passage_neighbor_node_representations = collect_neighbor_node_representations(\n",
        "                                                    in_passage_repres, passage_neighbor_indices)\n",
        "    # [batch_size, passage_len, num_passage_neighbors, input_dim]\n",
        "                \n",
        "    question_neighbor_representations = tf.concat(3, [question_neighbor_node_representations, question_neighbor_edge_representations])\n",
        "    # [batch_size, question_len, num_question_neighbors, input_dim+ edge_dim]\n",
        "    passage_neighbor_representations = tf.concat(3, [passage_neighbor_node_representations, passage_neighbor_edge_representations])\n",
        "    # [batch_size, passage_len, num_passage_neighbors, input_dim + edge_dim]\n",
        "\n",
        "    # =====compress neighbor_representations \n",
        "    compress_vector_dim = neighbor_vector_dim\n",
        "    w_compress = tf.get_variable(\"w_compress\", [input_dim + edge_dim, compress_vector_dim], dtype=tf.float32)\n",
        "    b_compress = tf.get_variable(\"b_compress\", [compress_vector_dim], dtype=tf.float32)\n",
        "\n",
        "    question_neighbor_representations = tf.reshape(question_neighbor_representations, [-1, input_dim + edge_dim])\n",
        "    question_neighbor_representations = tf.matmul(question_neighbor_representations, w_compress) + b_compress\n",
        "    question_neighbor_representations = tf.tanh(question_neighbor_representations)\n",
        "    # [batch_size*question_len*num_question_neighbors, compress_vector_dim]\n",
        "\n",
        "    passage_neighbor_representations = tf.reshape(passage_neighbor_representations, [-1, input_dim + edge_dim])\n",
        "    passage_neighbor_representations = tf.matmul(passage_neighbor_representations, w_compress) + b_compress\n",
        "    passage_neighbor_representations = tf.tanh(passage_neighbor_representations)\n",
        "    # [batch_size*passage_len*num_passage_neighbors, compress_vector_dim]\n",
        "                \n",
        "    # assume each node has a neighbor vector, and it is None at the beginning\n",
        "    question_node_hidden = tf.zeros([batch_size, question_len, neighbor_vector_dim])\n",
        "    question_node_cell = tf.zeros([batch_size, question_len, neighbor_vector_dim])\n",
        "\n",
        "    passage_node_hidden = tf.zeros([batch_size, passage_len, neighbor_vector_dim])\n",
        "    passage_node_cell = tf.zeros([batch_size, passage_len, neighbor_vector_dim])\n",
        "        \n",
        "        \n",
        "    w_ingate = tf.get_variable(\"w_ingate\", [compress_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    u_ingate = tf.get_variable(\"u_ingate\", [neighbor_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    b_ingate = tf.get_variable(\"b_ingate\", [neighbor_vector_dim], dtype=tf.float32)\n",
        "\n",
        "    w_forgetgate = tf.get_variable(\"w_forgetgate\", [compress_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    u_forgetgate = tf.get_variable(\"u_forgetgate\", [neighbor_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    b_forgetgate = tf.get_variable(\"b_forgetgate\", [neighbor_vector_dim], dtype=tf.float32)\n",
        "\n",
        "    w_outgate = tf.get_variable(\"w_outgate\", [compress_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    u_outgate = tf.get_variable(\"u_outgate\", [neighbor_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    b_outgate = tf.get_variable(\"b_outgate\", [neighbor_vector_dim], dtype=tf.float32)\n",
        "\n",
        "    w_cell = tf.get_variable(\"w_cell\", [compress_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    u_cell = tf.get_variable(\"u_cell\", [neighbor_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    b_cell = tf.get_variable(\"b_cell\", [neighbor_vector_dim], dtype=tf.float32)\n",
        "\n",
        "    for i in xrange(num_syntax_match_layer):\n",
        "        with tf.variable_scope('syntax_match_layer-{}'.format(i)):\n",
        "            # ========for question============\n",
        "            question_edge_prev_hidden = collect_neighbor_node_representations(question_node_hidden, question_neighbor_indices)\n",
        "            # [batch_size, question_len, num_question_neighbors, neighbor_vector_dim]\n",
        "            question_edge_prev_cell = collect_neighbor_node_representations(question_node_cell, question_neighbor_indices)\n",
        "            # [batch_size, question_len, num_question_neighbors, neighbor_vector_dim]\n",
        "            question_edge_prev_hidden = tf.reshape(question_edge_prev_hidden, [-1, neighbor_vector_dim])\n",
        "            question_edge_prev_cell = tf.reshape(question_edge_prev_cell, [-1, neighbor_vector_dim])\n",
        "\n",
        "            question_edge_ingate = tf.sigmoid(tf.matmul(question_neighbor_representations,w_ingate) \n",
        "                                          + tf.matmul(question_edge_prev_hidden, u_ingate) + b_ingate)\n",
        "            question_edge_forgetgate = tf.sigmoid(tf.matmul(question_neighbor_representations,w_forgetgate) \n",
        "                                          + tf.matmul(question_edge_prev_hidden, u_forgetgate) + b_forgetgate)\n",
        "            question_edge_outgate = tf.sigmoid(tf.matmul(question_neighbor_representations,w_outgate) \n",
        "                                          + tf.matmul(question_edge_prev_hidden, u_outgate) + b_outgate)\n",
        "            question_edge_cell_input = tf.tanh(tf.matmul(question_neighbor_representations,w_cell) \n",
        "                                          + tf.matmul(question_edge_prev_hidden, u_cell) + b_cell)\n",
        "            question_edge_cell = question_edge_forgetgate * question_edge_prev_cell + question_edge_ingate * question_edge_cell_input\n",
        "            question_edge_hidden = question_edge_outgate * tf.tanh(question_edge_cell)\n",
        "            question_edge_cell = tf.reshape(question_edge_cell, [batch_size, question_len, num_question_neighbors, neighbor_vector_dim])\n",
        "            question_edge_hidden = tf.reshape(question_edge_hidden, [batch_size, question_len, num_question_neighbors, neighbor_vector_dim])\n",
        "            # edge mask\n",
        "            question_edge_cell = tf.mul(question_edge_cell, tf.expand_dims(question_neighbor_mask, axis=-1))\n",
        "            question_edge_hidden = tf.mul(question_edge_hidden, tf.expand_dims(question_neighbor_mask, axis=-1))\n",
        "            question_node_cell = tf.reduce_sum(question_edge_cell, axis=2)\n",
        "            question_node_hidden = tf.reduce_sum(question_edge_hidden, axis=2)\n",
        "            #[batch_size, question_len, neighbor_vector_dim]\n",
        "\n",
        "            # node mask\n",
        "            question_node_cell = question_node_cell * tf.expand_dims(question_mask, axis=-1)\n",
        "            question_node_hidden = question_node_hidden * tf.expand_dims(question_mask, axis=-1)\n",
        "\n",
        "            # ========for passage============\n",
        "            passage_edge_prev_hidden = collect_neighbor_node_representations(passage_node_hidden, passage_neighbor_indices)\n",
        "            passage_edge_prev_cell = collect_neighbor_node_representations(passage_node_cell, passage_neighbor_indices)\n",
        "            # [batch_size, passage_len, num_passage_neighbors, neighbor_vector_dim]\n",
        "            passage_edge_prev_hidden = tf.reshape(passage_edge_prev_hidden, [-1, neighbor_vector_dim])\n",
        "            passage_edge_prev_cell = tf.reshape(passage_edge_prev_cell, [-1, neighbor_vector_dim])\n",
        "\n",
        "            passage_edge_ingate = tf.sigmoid(tf.matmul(passage_neighbor_representations,w_ingate) \n",
        "                                          + tf.matmul(passage_edge_prev_hidden, u_ingate) + b_ingate)\n",
        "            passage_edge_forgetgate = tf.sigmoid(tf.matmul(passage_neighbor_representations,w_forgetgate) \n",
        "                                          + tf.matmul(passage_edge_prev_hidden, u_forgetgate) + b_forgetgate)\n",
        "            passage_edge_outgate = tf.sigmoid(tf.matmul(passage_neighbor_representations,w_outgate) \n",
        "                                          + tf.matmul(passage_edge_prev_hidden, u_outgate) + b_outgate)\n",
        "            passage_edge_cell_input = tf.tanh(tf.matmul(passage_neighbor_representations,w_cell) \n",
        "                                          + tf.matmul(passage_edge_prev_hidden, u_cell) + b_cell)\n",
        "            passage_edge_cell = passage_edge_forgetgate * passage_edge_prev_cell + passage_edge_ingate * passage_edge_cell_input\n",
        "            passage_edge_hidden = passage_edge_outgate * tf.tanh(passage_edge_cell)\n",
        "            passage_edge_cell = tf.reshape(passage_edge_cell, [batch_size, passage_len, num_passage_neighbors, neighbor_vector_dim])\n",
        "            passage_edge_hidden = tf.reshape(passage_edge_hidden, [batch_size, passage_len, num_passage_neighbors, neighbor_vector_dim])\n",
        "            # edge mask\n",
        "            passage_edge_cell = tf.mul(passage_edge_cell, tf.expand_dims(passage_neighbor_mask, axis=-1))\n",
        "            passage_edge_hidden = tf.mul(passage_edge_hidden, tf.expand_dims(passage_neighbor_mask, axis=-1))\n",
        "            passage_node_cell = tf.reduce_sum(passage_edge_cell, axis=2)\n",
        "            passage_node_hidden = tf.reduce_sum(passage_edge_hidden, axis=2)\n",
        "\n",
        "            # node mask\n",
        "            passage_node_cell = passage_node_cell * tf.expand_dims(passage_mask, axis=-1)\n",
        "            passage_node_hidden = passage_node_hidden * tf.expand_dims(passage_mask, axis=-1)\n",
        "                        \n",
        "            #=====matching\n",
        "            (node_matching_vectors, node_matching_dim) = match_passage_with_question(\n",
        "                                passage_node_hidden, None, passage_mask, question_node_hidden, None,question_mask, neighbor_vector_dim, \n",
        "                                with_full_matching=False, \n",
        "                                with_attentive_matching=with_attentive_matching, \n",
        "                                with_max_attentive_matching=with_max_attentive_matching, \n",
        "                                with_maxpooling_matching=with_maxpooling_matching, \n",
        "                                with_forward_match=True, with_backward_match=False, match_options=match_options)\n",
        "            all_matching_vectors.extend(node_matching_vectors) #[batch_size, passage_len, node_matching_dim]\n",
        "            all_matching_dim += node_matching_dim\n",
        "\n",
        "    all_matching_vectors = tf.concat(2, all_matching_vectors) # [batch_size, passage_len, all_matching_dim]\n",
        "    return (all_matching_vectors, all_matching_dim)\n",
        "\n",
        "def graph_matching_for_chunk_ranking(in_question_repres, in_passage_repres, \n",
        "                question_mask, passage_mask, edge_embedding,\n",
        "                question_neighbor_indices, passage_neighbor_indices,\n",
        "                question_neighbor_edges, passage_neighbor_edges,\n",
        "                question_neighbor_mask, passage_neighbor_mask,\n",
        "                question_word_idx, candidate_node_idx, candidate_mask,\n",
        "                neighbor_vector_dim, input_dim, edge_dim, num_syntax_match_layer,\n",
        "                with_attentive_matching=True, with_max_attentive_matching=True,\n",
        "                with_maxpooling_matching=True, with_local_attentive_matching=True,match_options=None): \n",
        "\n",
        "    all_matching_vectors = []\n",
        "    all_matching_dim = 0\n",
        "    \n",
        "    \n",
        "    input_shape = tf.shape(question_neighbor_indices)\n",
        "    batch_size = input_shape[0]\n",
        "    question_len = input_shape[1]\n",
        "    num_question_neighbors = input_shape[2]\n",
        "\n",
        "    input_shape = tf.shape(passage_neighbor_indices)\n",
        "#     batch_size = input_shape[0]\n",
        "    passage_len = input_shape[1]\n",
        "    num_passage_neighbors = input_shape[2]\n",
        "    \n",
        "#     question_neighbor_mask = tf.sequence_mask(tf.reshape(question_neighbor_size, [-1]), num_question_neighbors, dtype=tf.float32)\n",
        "#     question_neighbor_mask = tf.reshape(question_neighbor_mask, [batch_size, question_len, num_question_neighbors])\n",
        "\n",
        "#     passage_neighbor_mask = tf.sequence_mask(tf.reshape(passage_neighbor_size, [-1]), num_passage_neighbors, dtype=tf.float32)\n",
        "#     passage_neighbor_mask = tf.reshape(passage_neighbor_mask, [batch_size, passage_len, num_passage_neighbors])\n",
        "\n",
        "    question_neighbor_edge_representations = tf.nn.embedding_lookup(edge_embedding, question_neighbor_edges) \n",
        "    # [batch_size, question_len, num_question_neighbors, edge_dim]\n",
        "    passage_neighbor_edge_representations = tf.nn.embedding_lookup(edge_embedding, passage_neighbor_edges) \n",
        "    # [batch_size, passage_len, num_passage_neighbors, edge_dim]\n",
        "    question_neighbor_node_representations = collect_neighbor_node_representations(\n",
        "                                                    in_question_repres, question_neighbor_indices)\n",
        "    # [batch_size, question_len, num_question_neighbors, input_dim]\n",
        "    passage_neighbor_node_representations = collect_neighbor_node_representations(\n",
        "                                                    in_passage_repres, passage_neighbor_indices)\n",
        "    # [batch_size, passage_len, num_passage_neighbors, input_dim]\n",
        "                \n",
        "    question_neighbor_representations = tf.concat(3, [question_neighbor_node_representations, question_neighbor_edge_representations])\n",
        "    # [batch_size, question_len, num_question_neighbors, input_dim+ edge_dim]\n",
        "    passage_neighbor_representations = tf.concat(3, [passage_neighbor_node_representations, passage_neighbor_edge_representations])\n",
        "    # [batch_size, passage_len, num_passage_neighbors, input_dim + edge_dim]\n",
        "\n",
        "    # =====compress neighbor_representations \n",
        "    compress_vector_dim = neighbor_vector_dim\n",
        "    w_compress = tf.get_variable(\"w_compress\", [input_dim + edge_dim, compress_vector_dim], dtype=tf.float32)\n",
        "    b_compress = tf.get_variable(\"b_compress\", [compress_vector_dim], dtype=tf.float32)\n",
        "\n",
        "    question_neighbor_representations = tf.reshape(question_neighbor_representations, [-1, input_dim + edge_dim])\n",
        "    question_neighbor_representations = tf.matmul(question_neighbor_representations, w_compress) + b_compress\n",
        "    question_neighbor_representations = tf.tanh(question_neighbor_representations)\n",
        "    # [batch_size*question_len*num_question_neighbors, compress_vector_dim]\n",
        "\n",
        "    passage_neighbor_representations = tf.reshape(passage_neighbor_representations, [-1, input_dim + edge_dim])\n",
        "    passage_neighbor_representations = tf.matmul(passage_neighbor_representations, w_compress) + b_compress\n",
        "    passage_neighbor_representations = tf.tanh(passage_neighbor_representations)\n",
        "    # [batch_size*passage_len*num_passage_neighbors, compress_vector_dim]\n",
        "                \n",
        "    # assume each node has a neighbor vector, and it is None at the beginning\n",
        "    question_node_hidden = tf.zeros([batch_size, question_len, neighbor_vector_dim])\n",
        "    question_node_cell = tf.zeros([batch_size, question_len, neighbor_vector_dim])\n",
        "\n",
        "    passage_node_hidden = tf.zeros([batch_size, passage_len, neighbor_vector_dim])\n",
        "    passage_node_cell = tf.zeros([batch_size, passage_len, neighbor_vector_dim])\n",
        "        \n",
        "        \n",
        "    w_ingate = tf.get_variable(\"w_ingate\", [compress_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    u_ingate = tf.get_variable(\"u_ingate\", [neighbor_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    b_ingate = tf.get_variable(\"b_ingate\", [neighbor_vector_dim], dtype=tf.float32)\n",
        "\n",
        "    w_forgetgate = tf.get_variable(\"w_forgetgate\", [compress_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    u_forgetgate = tf.get_variable(\"u_forgetgate\", [neighbor_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    b_forgetgate = tf.get_variable(\"b_forgetgate\", [neighbor_vector_dim], dtype=tf.float32)\n",
        "\n",
        "    w_outgate = tf.get_variable(\"w_outgate\", [compress_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    u_outgate = tf.get_variable(\"u_outgate\", [neighbor_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    b_outgate = tf.get_variable(\"b_outgate\", [neighbor_vector_dim], dtype=tf.float32)\n",
        "\n",
        "    w_cell = tf.get_variable(\"w_cell\", [compress_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    u_cell = tf.get_variable(\"u_cell\", [neighbor_vector_dim, neighbor_vector_dim], dtype=tf.float32)\n",
        "    b_cell = tf.get_variable(\"b_cell\", [neighbor_vector_dim], dtype=tf.float32)\n",
        "    \n",
        "    # calculate question graph representation\n",
        "    all_question_graph_representations = []\n",
        "    for i in xrange(num_syntax_match_layer):\n",
        "        with tf.variable_scope('syntax_match_layer-{}'.format(i)):\n",
        "            # ========for question============\n",
        "            question_edge_prev_hidden = collect_neighbor_node_representations(question_node_hidden, question_neighbor_indices)\n",
        "            # [batch_size, question_len, num_question_neighbors, neighbor_vector_dim]\n",
        "            question_edge_prev_cell = collect_neighbor_node_representations(question_node_cell, question_neighbor_indices)\n",
        "            # [batch_size, question_len, num_question_neighbors, neighbor_vector_dim]\n",
        "            question_edge_prev_hidden = tf.reshape(question_edge_prev_hidden, [-1, neighbor_vector_dim])\n",
        "            question_edge_prev_cell = tf.reshape(question_edge_prev_cell, [-1, neighbor_vector_dim])\n",
        "\n",
        "            question_edge_ingate = tf.sigmoid(tf.matmul(question_neighbor_representations,w_ingate) \n",
        "                                          + tf.matmul(question_edge_prev_hidden, u_ingate) + b_ingate)\n",
        "            question_edge_forgetgate = tf.sigmoid(tf.matmul(question_neighbor_representations,w_forgetgate) \n",
        "                                          + tf.matmul(question_edge_prev_hidden, u_forgetgate) + b_forgetgate)\n",
        "            question_edge_outgate = tf.sigmoid(tf.matmul(question_neighbor_representations,w_outgate) \n",
        "                                          + tf.matmul(question_edge_prev_hidden, u_outgate) + b_outgate)\n",
        "            question_edge_cell_input = tf.tanh(tf.matmul(question_neighbor_representations,w_cell) \n",
        "                                          + tf.matmul(question_edge_prev_hidden, u_cell) + b_cell)\n",
        "            question_edge_cell = question_edge_forgetgate * question_edge_prev_cell + question_edge_ingate * question_edge_cell_input\n",
        "            question_edge_hidden = question_edge_outgate * tf.tanh(question_edge_cell)\n",
        "            question_edge_cell = tf.reshape(question_edge_cell, [batch_size, question_len, num_question_neighbors, neighbor_vector_dim])\n",
        "            question_edge_hidden = tf.reshape(question_edge_hidden, [batch_size, question_len, num_question_neighbors, neighbor_vector_dim])\n",
        "            # edge mask\n",
        "            question_edge_cell = tf.mul(question_edge_cell, tf.expand_dims(question_neighbor_mask, axis=-1))\n",
        "            question_edge_hidden = tf.mul(question_edge_hidden, tf.expand_dims(question_neighbor_mask, axis=-1))\n",
        "#             question_node_cell = tf.reduce_sum(question_edge_cell, axis=2)\n",
        "#             question_node_hidden = tf.reduce_sum(question_edge_hidden, axis=2)\n",
        "            question_node_cell = tf.reduce_max(question_edge_cell, axis=2)\n",
        "            question_node_hidden = tf.reduce_max(question_edge_hidden, axis=2) # TODO\n",
        "            #[batch_size, question_len, neighbor_vector_dim]\n",
        "\n",
        "            # node mask\n",
        "            question_node_cell = question_node_cell * tf.expand_dims(question_mask, axis=-1)\n",
        "            question_node_hidden = question_node_hidden * tf.expand_dims(question_mask, axis=-1)\n",
        "\n",
        "            question_word_representation = collect_node_representations(question_node_hidden, question_word_idx)\n",
        "            # [batch_size, neighbor_vector_dim]\n",
        "            all_question_graph_representations.append(tf.reshape(question_word_representation, [batch_size, 1, neighbor_vector_dim]))\n",
        "    all_question_graph_representations = tf.concat(1, all_question_graph_representations) # [batch_size, num_match_layer, neighbor_vector_dim]\n",
        "     \n",
        "    # calculate passage representation and match it with the question\n",
        "    for i in xrange(num_syntax_match_layer):\n",
        "        with tf.variable_scope('syntax_match_layer-{}'.format(i)):\n",
        "            passage_edge_prev_hidden = collect_neighbor_node_representations(passage_node_hidden, passage_neighbor_indices)\n",
        "            passage_edge_prev_cell = collect_neighbor_node_representations(passage_node_cell, passage_neighbor_indices)\n",
        "            # [batch_size, passage_len, num_passage_neighbors, neighbor_vector_dim]\n",
        "            passage_edge_prev_hidden = tf.reshape(passage_edge_prev_hidden, [-1, neighbor_vector_dim])\n",
        "            passage_edge_prev_cell = tf.reshape(passage_edge_prev_cell, [-1, neighbor_vector_dim])\n",
        "\n",
        "            passage_edge_ingate = tf.sigmoid(tf.matmul(passage_neighbor_representations,w_ingate) \n",
        "                                          + tf.matmul(passage_edge_prev_hidden, u_ingate) + b_ingate)\n",
        "            passage_edge_forgetgate = tf.sigmoid(tf.matmul(passage_neighbor_representations,w_forgetgate) \n",
        "                                          + tf.matmul(passage_edge_prev_hidden, u_forgetgate) + b_forgetgate)\n",
        "            passage_edge_outgate = tf.sigmoid(tf.matmul(passage_neighbor_representations,w_outgate) \n",
        "                                          + tf.matmul(passage_edge_prev_hidden, u_outgate) + b_outgate)\n",
        "            passage_edge_cell_input = tf.tanh(tf.matmul(passage_neighbor_representations,w_cell) \n",
        "                                          + tf.matmul(passage_edge_prev_hidden, u_cell) + b_cell)\n",
        "            passage_edge_cell = passage_edge_forgetgate * passage_edge_prev_cell + passage_edge_ingate * passage_edge_cell_input\n",
        "            passage_edge_hidden = passage_edge_outgate * tf.tanh(passage_edge_cell)\n",
        "            passage_edge_cell = tf.reshape(passage_edge_cell, [batch_size, passage_len, num_passage_neighbors, neighbor_vector_dim])\n",
        "            passage_edge_hidden = tf.reshape(passage_edge_hidden, [batch_size, passage_len, num_passage_neighbors, neighbor_vector_dim])\n",
        "            # edge mask\n",
        "            passage_edge_cell = tf.mul(passage_edge_cell, tf.expand_dims(passage_neighbor_mask, axis=-1))\n",
        "            passage_edge_hidden = tf.mul(passage_edge_hidden, tf.expand_dims(passage_neighbor_mask, axis=-1))\n",
        "#             passage_node_cell = tf.reduce_sum(passage_edge_cell, axis=2)\n",
        "#             passage_node_hidden = tf.reduce_sum(passage_edge_hidden, axis=2)\n",
        "            passage_node_cell = tf.reduce_max(passage_edge_cell, axis=2)\n",
        "            passage_node_hidden = tf.reduce_max(passage_edge_hidden, axis=2) # TODO\n",
        "\n",
        "            # node mask\n",
        "            passage_node_cell = passage_node_cell * tf.expand_dims(passage_mask, axis=-1)\n",
        "            passage_node_hidden = passage_node_hidden * tf.expand_dims(passage_mask, axis=-1)\n",
        "                        \n",
        "            #=====matching\n",
        "            canidate_node_representation = collect_node_representations(passage_node_hidden, candidate_node_idx)\n",
        "            # [batch_size, num_candidate_nodes, neighbor_vector_dim]\n",
        "            (node_matching_vectors, node_matching_dim) = match_passage_with_question(\n",
        "                                canidate_node_representation, None, candidate_mask, \n",
        "                                all_question_graph_representations, None,None, \n",
        "                                neighbor_vector_dim, with_full_matching=False, \n",
        "                                with_attentive_matching=with_attentive_matching, \n",
        "                                with_max_attentive_matching=with_max_attentive_matching, \n",
        "                                with_maxpooling_matching=with_maxpooling_matching, \n",
        "                                with_local_attentive_matching=with_local_attentive_matching,\n",
        "                                with_forward_match=True, with_backward_match=False, match_options=match_options)\n",
        "            all_matching_vectors.extend(node_matching_vectors) #[batch_size, num_candidate_nodes, node_matching_dim]\n",
        "            all_matching_dim += node_matching_dim\n",
        "\n",
        "    all_matching_vectors = tf.concat(2, all_matching_vectors) # [batch_size, num_candidate_nodes, all_matching_dim]\n",
        "    return (all_matching_vectors, all_matching_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M-XNcePV2EP1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def collect_neighbor_node_representations(representation, positions):\n",
        "    # representation: [batch_size, num_nodes, feature_dim]\n",
        "    # positions: [batch_size, num_nodes, num_neighbors]\n",
        "    feature_dim = tf.shape(representation)[2]\n",
        "    input_shape = tf.shape(positions)\n",
        "    batch_size = input_shape[0]\n",
        "    num_nodes = input_shape[1]\n",
        "    num_neighbors = input_shape[2]\n",
        "    positions_flat = tf.reshape(positions, [batch_size, num_nodes*num_neighbors])\n",
        "    def singel_instance(x):\n",
        "        # x[0]: [num_nodes, feature_dim]\n",
        "        # x[1]: [num_nodes*num_neighbors]\n",
        "        return tf.gather(x[0], x[1])\n",
        "    elems = (representation, positions_flat)\n",
        "    representations = tf.map_fn(singel_instance, elems, dtype=tf.float32)\n",
        "    return tf.reshape(representations, [batch_size, num_nodes, num_neighbors, feature_dim])\n",
        "\n",
        "def collect_final_step_lstm(lstm_rep, lens):\n",
        "    lens = tf.maximum(lens, tf.zeros_like(lens, dtype=tf.int32)) # [batch,]\n",
        "    idxs = tf.range(0, limit=tf.shape(lens)[0]) # [batch,]\n",
        "    indices = tf.stack((idxs,lens,), axis=1) # [batch_size, 2]\n",
        "    return tf.gather_nd(lstm_rep, indices, name='lstm-forward-last')\n",
        "\n",
        "class GraphEncoder(object):\n",
        "    def __init__(self, word_vocab=None, edge_label_vocab=None, char_vocab=None, is_training=True, options=None):\n",
        "        assert options != None\n",
        "\n",
        "        self.passage_nodes_size = tf.placeholder(tf.int32, [None]) # [batch_size]\n",
        "        self.passage_nodes = tf.placeholder(tf.int32, [None, None]) # [batch_size, passage_nodes_size_max]\n",
        "        if options.with_char:\n",
        "            self.passage_nodes_chars_size = tf.placeholder(tf.int32, [None, None])\n",
        "            self.passage_nodes_chars = tf.placeholder(tf.int32, [None, None, None])\n",
        "\n",
        "        # [batch_size, passage_nodes_size_max, passage_neighbors_size_max]\n",
        "        self.passage_in_neighbor_indices = tf.placeholder(tf.int32, [None, None, None])\n",
        "        self.passage_in_neighbor_edges = tf.placeholder(tf.int32, [None, None, None])\n",
        "        self.passage_in_neighbor_mask = tf.placeholder(tf.float32, [None, None, None])\n",
        "\n",
        "        # [batch_size, passage_nodes_size_max, passage_neighbors_size_max]\n",
        "        self.passage_out_neighbor_indices = tf.placeholder(tf.int32, [None, None, None])\n",
        "        self.passage_out_neighbor_edges = tf.placeholder(tf.int32, [None, None, None])\n",
        "        self.passage_out_neighbor_mask = tf.placeholder(tf.float32, [None, None, None])\n",
        "\n",
        "        # shapes\n",
        "        input_shape = tf.shape(self.passage_in_neighbor_indices)\n",
        "        batch_size = input_shape[0]\n",
        "        passage_nodes_size_max = input_shape[1]\n",
        "        passage_in_neighbors_size_max = input_shape[2]\n",
        "        passage_out_neighbors_size_max = tf.shape(self.passage_out_neighbor_indices)[2]\n",
        "        if options.with_char:\n",
        "            passage_nodes_chars_size_max = tf.shape(self.passage_nodes_chars)[2]\n",
        "\n",
        "        # masks\n",
        "        # [batch_size, passage_nodes_size_max]\n",
        "        self.passage_nodes_mask = tf.sequence_mask(self.passage_nodes_size, passage_nodes_size_max, dtype=tf.float32)\n",
        "\n",
        "        # embeddings\n",
        "        if options.fix_word_vec:\n",
        "            word_vec_trainable = False\n",
        "            cur_device = '/cpu:0'\n",
        "        else:\n",
        "            word_vec_trainable = True\n",
        "            cur_device = '/gpu:0'\n",
        "        with tf.device(cur_device):\n",
        "            self.word_embedding = tf.get_variable(\"word_embedding\", trainable=word_vec_trainable,\n",
        "                                                  initializer=tf.constant(word_vocab.word_vecs), dtype=tf.float32)\n",
        "\n",
        "        self.edge_embedding = tf.get_variable(\"edge_embedding\",\n",
        "                initializer=tf.constant(edge_label_vocab.word_vecs), dtype=tf.float32)\n",
        "\n",
        "        word_dim = word_vocab.word_dim\n",
        "        edge_dim = edge_label_vocab.word_dim\n",
        "\n",
        "        if options.with_char:\n",
        "            self.char_embedding = tf.get_variable(\"char_embedding\",\n",
        "                    initializer=tf.constant(char_vocab.word_vecs), dtype=tf.float32)\n",
        "            char_dim = char_vocab.word_dim\n",
        "\n",
        "        # word representation for nodes, where each node only includes one word\n",
        "        # [batch_size, passage_nodes_size_max, word_dim]\n",
        "        passage_node_representation = tf.nn.embedding_lookup(self.word_embedding, self.passage_nodes)\n",
        "\n",
        "        if options.with_char:\n",
        "            # [batch_size, passage_nodes_size_max, passage_nodes_chars_size_max, char_dim]\n",
        "            passage_nodes_chars_representation = tf.nn.embedding_lookup(self.char_embedding, self.passage_nodes_chars)\n",
        "            passage_nodes_chars_representation = tf.reshape(passage_nodes_chars_representation,\n",
        "                    shape=[batch_size*passage_nodes_size_max, passage_nodes_chars_size_max, char_dim])\n",
        "            passage_nodes_chars_size = tf.reshape(self.passage_nodes_chars_size, [batch_size*passage_nodes_size_max])\n",
        "            with tf.variable_scope('node_char_lstm'):\n",
        "                node_char_lstm_cell = tf.contrib.rnn.LSTMCell(options.char_lstm_dim)\n",
        "                node_char_lstm_cell = tf.contrib.rnn.MultiRNNCell([node_char_lstm_cell])\n",
        "                # [batch_size*node_num, char_num, char_lstm_dim]\n",
        "                node_char_outputs = tf.nn.dynamic_rnn(node_char_lstm_cell, passage_nodes_chars_representation,\n",
        "                        sequence_length=passage_nodes_chars_size, dtype=tf.float32)[0]\n",
        "                node_char_outputs = collect_final_step_lstm(node_char_outputs, passage_nodes_chars_size-1)\n",
        "                # [batch_size, node_num, char_lstm_dim]\n",
        "                node_char_outputs = tf.reshape(node_char_outputs, [batch_size, passage_nodes_size_max, options.char_lstm_dim])\n",
        "\n",
        "        if options.with_char:\n",
        "            input_dim = word_dim + options.char_lstm_dim\n",
        "            passage_node_representation = tf.concat([passage_node_representation, node_char_outputs], 2)\n",
        "        else:\n",
        "            input_dim = word_dim\n",
        "            passage_node_representation = passage_node_representation\n",
        "\n",
        "        # apply the mask\n",
        "        passage_node_representation = passage_node_representation * tf.expand_dims(self.passage_nodes_mask, axis=-1)\n",
        "\n",
        "        if options.compress_input: # compress input word vector into smaller vectors\n",
        "            w_compress = tf.get_variable(\"w_compress_input\", [input_dim, options.compress_input_dim], dtype=tf.float32)\n",
        "            b_compress = tf.get_variable(\"b_compress_input\", [options.compress_input_dim], dtype=tf.float32)\n",
        "\n",
        "            passage_node_representation = tf.reshape(passage_node_representation, [-1, input_dim])\n",
        "            passage_node_representation = tf.matmul(passage_node_representation, w_compress) + b_compress\n",
        "            passage_node_representation = tf.tanh(passage_node_representation)\n",
        "            passage_node_representation = tf.reshape(passage_node_representation, \\\n",
        "                    [batch_size, passage_nodes_size_max, options.compress_input_dim])\n",
        "            input_dim = options.compress_input_dim\n",
        "\n",
        "        if is_training:\n",
        "            passage_node_representation = tf.nn.dropout(passage_node_representation, (1 - options.dropout_rate))\n",
        "\n",
        "        # ======Highway layer======\n",
        "        if options.with_highway:\n",
        "            with tf.variable_scope(\"input_highway\"):\n",
        "                passage_node_representation = multi_highway_layer(passage_node_representation,\n",
        "                        input_dim, options.highway_layer_num)\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        with tf.variable_scope('graph_encoder'):\n",
        "            # =========== in neighbor\n",
        "            # [batch_size, passage_len, passage_neighbors_size_max, edge_dim]\n",
        "            passage_in_neighbor_edge_representations = tf.nn.embedding_lookup(self.edge_embedding,\n",
        "                    self.passage_in_neighbor_edges)\n",
        "            # [batch_size, passage_len, passage_neighbors_size_max, node_dim]\n",
        "            passage_in_neighbor_node_representations = collect_neighbor_node_representations(\n",
        "                    passage_node_representation, self.passage_in_neighbor_indices)\n",
        "\n",
        "            passage_in_neighbor_representations = tf.concat( \\\n",
        "                    [passage_in_neighbor_node_representations, passage_in_neighbor_edge_representations], 3)\n",
        "            passage_in_neighbor_representations = tf.multiply(passage_in_neighbor_representations,\n",
        "                    tf.expand_dims(self.passage_in_neighbor_mask, axis=-1))\n",
        "            # [batch_size, passage_len, node_dim + edge_dim]\n",
        "            passage_in_neighbor_representations = tf.reduce_sum(passage_in_neighbor_representations, axis=2)\n",
        "\n",
        "            # ============ out neighbor\n",
        "            # [batch_size, passage_len, passage_neighbors_size_max, edge_dim]\n",
        "            passage_out_neighbor_edge_representations = tf.nn.embedding_lookup(self.edge_embedding,\n",
        "                    self.passage_out_neighbor_edges)\n",
        "            # [batch_size, passage_len, passage_neighbors_size_max, node_dim]\n",
        "            passage_out_neighbor_node_representations = collect_neighbor_node_representations(\n",
        "                    passage_node_representation, self.passage_out_neighbor_indices)\n",
        "\n",
        "            passage_out_neighbor_representations = tf.concat( \\\n",
        "                    [passage_out_neighbor_node_representations, passage_out_neighbor_edge_representations], 3)\n",
        "            passage_out_neighbor_representations = tf.multiply(passage_out_neighbor_representations,\n",
        "                    tf.expand_dims(self.passage_out_neighbor_mask, axis=-1))\n",
        "            # [batch_size, passage_len, node_dim + edge_dim]\n",
        "            passage_out_neighbor_representations = tf.reduce_sum(passage_out_neighbor_representations, axis=2)\n",
        "\n",
        "            # =====transpose neighbor_representations\n",
        "            grn_hidden_dim = options.neighbor_vector_dim\n",
        "            w_trans = tf.get_variable(\"w_trans\", [input_dim + edge_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "            b_trans = tf.get_variable(\"b_trans\", [grn_hidden_dim], dtype=tf.float32)\n",
        "\n",
        "            passage_in_neighbor_representations = tf.reshape(passage_in_neighbor_representations,\n",
        "                    [-1, input_dim + edge_dim])\n",
        "            passage_in_neighbor_representations = tf.matmul(passage_in_neighbor_representations, w_trans) + b_trans\n",
        "            passage_in_neighbor_representations = tf.tanh(passage_in_neighbor_representations)\n",
        "\n",
        "            passage_out_neighbor_representations = tf.reshape(passage_out_neighbor_representations,\n",
        "                    [-1, input_dim + edge_dim])\n",
        "            passage_out_neighbor_representations = tf.matmul(passage_out_neighbor_representations, w_trans) + b_trans\n",
        "            passage_out_neighbor_representations = tf.tanh(passage_out_neighbor_representations)\n",
        "\n",
        "            # assume each node has a neighbor vector, and it is None at the beginning\n",
        "            passage_node_hidden = tf.zeros([batch_size, passage_nodes_size_max, grn_hidden_dim])\n",
        "            passage_node_cell = tf.zeros([batch_size, passage_nodes_size_max, grn_hidden_dim])\n",
        "\n",
        "            w_in_ingate = tf.get_variable(\"w_in_ingate\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "            u_in_ingate = tf.get_variable(\"u_in_ingate\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "            b_ingate = tf.get_variable(\"b_in_ingate\",\n",
        "                    [grn_hidden_dim], dtype=tf.float32)\n",
        "            w_out_ingate = tf.get_variable(\"w_out_ingate\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "            u_out_ingate = tf.get_variable(\"u_out_ingate\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "\n",
        "            w_in_forgetgate = tf.get_variable(\"w_in_forgetgate\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "            u_in_forgetgate = tf.get_variable(\"u_in_forgetgate\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "            b_forgetgate = tf.get_variable(\"b_in_forgetgate\",\n",
        "                    [grn_hidden_dim], dtype=tf.float32)\n",
        "            w_out_forgetgate = tf.get_variable(\"w_out_forgetgate\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "            u_out_forgetgate = tf.get_variable(\"u_out_forgetgate\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "\n",
        "            w_in_outgate = tf.get_variable(\"w_in_outgate\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "            u_in_outgate = tf.get_variable(\"u_in_outgate\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "            b_outgate = tf.get_variable(\"b_in_outgate\",\n",
        "                    [grn_hidden_dim], dtype=tf.float32)\n",
        "            w_out_outgate = tf.get_variable(\"w_out_outgate\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "            u_out_outgate = tf.get_variable(\"u_out_outgate\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "\n",
        "            w_in_cell = tf.get_variable(\"w_in_cell\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "            u_in_cell = tf.get_variable(\"u_in_cell\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "            b_cell = tf.get_variable(\"b_in_cell\",\n",
        "                    [grn_hidden_dim], dtype=tf.float32)\n",
        "            w_out_cell = tf.get_variable(\"w_out_cell\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "            u_out_cell = tf.get_variable(\"u_out_cell\",\n",
        "                    [grn_hidden_dim, grn_hidden_dim], dtype=tf.float32)\n",
        "\n",
        "            # calculate question graph representation\n",
        "            graph_representations = []\n",
        "            for i in xrange(options.num_syntax_match_layer):\n",
        "                # =============== in edge hidden\n",
        "                # h_{ij} [batch_size, node_len, neighbors_size, neighbor_vector_dim]\n",
        "                passage_in_edge_prev_hidden = collect_neighbor_node_representations(passage_node_hidden,\n",
        "                        self.passage_in_neighbor_indices)\n",
        "                passage_in_edge_prev_hidden = tf.multiply(passage_in_edge_prev_hidden,\n",
        "                    tf.expand_dims(self.passage_in_neighbor_mask, axis=-1))\n",
        "                # [batch_size, node_len, neighbor_vector_dim]\n",
        "                passage_in_edge_prev_hidden = tf.reduce_sum(passage_in_edge_prev_hidden, axis=2)\n",
        "                passage_in_edge_prev_hidden = tf.multiply(passage_in_edge_prev_hidden,\n",
        "                        tf.expand_dims(self.passage_nodes_mask, axis=-1))\n",
        "                passage_in_edge_prev_hidden = tf.reshape(passage_in_edge_prev_hidden,\n",
        "                        [-1, grn_hidden_dim])\n",
        "\n",
        "                # =============== out edge hidden\n",
        "                # h_{jk} [batch_size, node_len, neighbors_size, neighbor_vector_dim]\n",
        "                passage_out_edge_prev_hidden = collect_neighbor_node_representations(passage_node_hidden,\n",
        "                        self.passage_out_neighbor_indices)\n",
        "                passage_out_edge_prev_hidden = tf.multiply(passage_out_edge_prev_hidden,\n",
        "                    tf.expand_dims(self.passage_out_neighbor_mask, axis=-1))\n",
        "                # [batch_size, node_len, neighbor_vector_dim]\n",
        "                passage_out_edge_prev_hidden = tf.reduce_sum(passage_out_edge_prev_hidden, axis=2)\n",
        "                passage_out_edge_prev_hidden = tf.multiply(passage_out_edge_prev_hidden,\n",
        "                        tf.expand_dims(self.passage_nodes_mask, axis=-1))\n",
        "                passage_out_edge_prev_hidden = tf.reshape(passage_out_edge_prev_hidden,\n",
        "                        [-1, grn_hidden_dim])\n",
        "\n",
        "                ## ig\n",
        "                passage_edge_ingate = tf.sigmoid(tf.matmul(passage_in_neighbor_representations, w_in_ingate)\n",
        "                                          + tf.matmul(passage_in_edge_prev_hidden, u_in_ingate)\n",
        "                                          + tf.matmul(passage_out_neighbor_representations, w_out_ingate)\n",
        "                                          + tf.matmul(passage_out_edge_prev_hidden, u_out_ingate)\n",
        "                                          + b_ingate)\n",
        "                passage_edge_ingate = tf.reshape(passage_edge_ingate,\n",
        "                        [batch_size, passage_nodes_size_max, grn_hidden_dim])\n",
        "                ## fg\n",
        "                passage_edge_forgetgate = tf.sigmoid(tf.matmul(passage_in_neighbor_representations, w_in_forgetgate)\n",
        "                                          + tf.matmul(passage_in_edge_prev_hidden, u_in_forgetgate)\n",
        "                                          + tf.matmul(passage_out_neighbor_representations, w_out_forgetgate)\n",
        "                                          + tf.matmul(passage_out_edge_prev_hidden, u_out_forgetgate)\n",
        "                                          + b_forgetgate)\n",
        "                passage_edge_forgetgate = tf.reshape(passage_edge_forgetgate,\n",
        "                        [batch_size, passage_nodes_size_max, grn_hidden_dim])\n",
        "                ## og\n",
        "                passage_edge_outgate = tf.sigmoid(tf.matmul(passage_in_neighbor_representations, w_in_outgate)\n",
        "                                          + tf.matmul(passage_in_edge_prev_hidden, u_in_outgate)\n",
        "                                          + tf.matmul(passage_out_neighbor_representations, w_out_outgate)\n",
        "                                          + tf.matmul(passage_out_edge_prev_hidden, u_out_outgate)\n",
        "                                          + b_outgate)\n",
        "                passage_edge_outgate = tf.reshape(passage_edge_outgate,\n",
        "                        [batch_size, passage_nodes_size_max, grn_hidden_dim])\n",
        "                ## input\n",
        "                passage_edge_cell_input = tf.tanh(tf.matmul(passage_in_neighbor_representations, w_in_cell)\n",
        "                                          + tf.matmul(passage_in_edge_prev_hidden, u_in_cell)\n",
        "                                          + tf.matmul(passage_out_neighbor_representations, w_out_cell)\n",
        "                                          + tf.matmul(passage_out_edge_prev_hidden, u_out_cell)\n",
        "                                          + b_cell)\n",
        "                passage_edge_cell_input = tf.reshape(passage_edge_cell_input,\n",
        "                        [batch_size, passage_nodes_size_max, grn_hidden_dim])\n",
        "\n",
        "                passage_edge_cell = passage_edge_forgetgate * passage_node_cell + passage_edge_ingate * passage_edge_cell_input\n",
        "                passage_edge_hidden = passage_edge_outgate * tf.tanh(passage_edge_cell)\n",
        "                # node mask\n",
        "                # [batch_size, passage_len, neighbor_vector_dim]\n",
        "                passage_node_cell = tf.multiply(passage_edge_cell, tf.expand_dims(self.passage_nodes_mask, axis=-1))\n",
        "                passage_node_hidden = tf.multiply(passage_edge_hidden, tf.expand_dims(self.passage_nodes_mask, axis=-1))\n",
        "\n",
        "                graph_representations.append(passage_node_hidden)\n",
        "\n",
        "            # decide how to use graph_representations\n",
        "            self.graph_representations = graph_representations\n",
        "            self.node_representations = passage_node_representation\n",
        "            self.graph_hiddens = passage_node_hidden\n",
        "            self.graph_cells = passage_node_cell\n",
        "\n",
        "            self.batch_size = batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oZbcmKha2OLT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cc = SmoothingFunction()\n",
        "ROUGE_path = '/usr/local/lib/python2.7/dist-packages/pythonrouge/RELEASE-1.5.5/ROUGE-1.5.5.pl'\n",
        "data_path = '/usr/local/lib/python2.7/dist-packages/pythonrouge/RELEASE-1.5.5/data'\n",
        "\n",
        "def _clip_and_normalize(word_probs, epsilon):\n",
        "    '''\n",
        "    word_probs: 1D tensor of [vsize]\n",
        "    '''\n",
        "    word_probs = tf.clip_by_value(word_probs, epsilon, 1.0 - epsilon)\n",
        "    return word_probs / tf.reduce_sum(word_probs, axis=-1, keep_dims=True) # scale preds so that the class probas of each sample sum to 1\n",
        "\n",
        "def collect_by_indices(memory, indices): # [batch, node_num, dim], [batch, 3, 5]\n",
        "    batch_size = tf.shape(indices)[0]\n",
        "    entity_num = tf.shape(indices)[1]\n",
        "    entity_size = tf.shape(indices)[2]\n",
        "    idxs = tf.range(0, limit=batch_size) # [batch]\n",
        "    idxs = tf.reshape(idxs, [-1, 1, 1]) # [batch, 1, 1]\n",
        "    idxs = tf.tile(idxs, [1, entity_num, entity_size])\n",
        "    indices = tf.maximum(indices, tf.zeros_like(indices, dtype=tf.int32))\n",
        "    indices = tf.stack((idxs,indices), axis=3) # [batch,3,5,2]\n",
        "    return tf.gather_nd(memory, indices)\n",
        "\n",
        "def sentence_rouge(reflex, genlex):\n",
        "    #rouge = Pythonrouge(n_gram=2, ROUGE_SU4=True, ROUGE_L=True, stemming=True, stopwords=True, word_level=True, length_limit=True, \\\n",
        "     #       length=50, use_cf=False, cf=95, scoring_formula=\"average\", resampling=True, samples=1000, favor=True, p=0.5)\n",
        "    genlex = [[genlex,]]\n",
        "    reflex = [[[reflex]]]\n",
        "    #setting_file = rouge.setting(files=False, summary=genlex, reference=reflex)\n",
        "    #result = rouge.eval_rouge(setting_file, recall_only=False, ROUGE_path=ROUGE_path, data_path=data_path)\n",
        "    rouge = Pythonrouge(summary_file_exist=False,\n",
        "                    summary=genlex, reference=reflex,\n",
        "                    n_gram=2, ROUGE_SU4=True, ROUGE_L=False,\n",
        "                    recall_only=True, stemming=True, stopwords=True,\n",
        "                    word_level=True, length_limit=True, length=50,\n",
        "                    use_cf=False, cf=95, scoring_formula='average',\n",
        "                    resampling=True, samples=1000, favor=True, p=0.5)\n",
        "    #rouge.ROUGE_path = ROUGE_path\n",
        "    #rouge.data_path = data_path\n",
        "    result = rouge.calc_score()\n",
        "    print(result)\n",
        "    return result['ROUGE-SU4']\n",
        "\n",
        "class ModelGraph(object):\n",
        "    def __init__(self, word_vocab, char_vocab, Edgelabel_vocab, options=None, mode='train'):\n",
        "        # the value of 'mode' can be:\n",
        "        #  'train',\n",
        "        #  'evaluate'\n",
        "        self.mode = mode\n",
        "\n",
        "        # is_training controls whether to use dropout\n",
        "        is_training = True if mode in ('train', ) else False\n",
        "\n",
        "        self.options = options\n",
        "        self.word_vocab = word_vocab\n",
        "\n",
        "        # encode the input instance\n",
        "        # encoder.graph_hidden [batch, node_num, vsize]\n",
        "        # encoder.graph_cell [batch, node_num, vsize]\n",
        "        self.encoder = GraphEncoder(\n",
        "                word_vocab = word_vocab,\n",
        "                edge_label_vocab = Edgelabel_vocab,\n",
        "                char_vocab = char_vocab,\n",
        "                is_training = is_training, options = options)\n",
        "\n",
        "        # ============== Choices of attention memory ================\n",
        "        if options.attention_type == 'hidden':\n",
        "            self.encoder_dim = options.neighbor_vector_dim\n",
        "            self.encoder_states = self.encoder.graph_hiddens\n",
        "        elif options.attention_type == 'hidden_cell':\n",
        "            self.encoder_dim = options.neighbor_vector_dim * 2\n",
        "            self.encoder_states = tf.concat([self.encoder.graph_hiddens, self.encoder.graph_cells], 2)\n",
        "        elif options.attention_type == 'hidden_embed':\n",
        "            self.encoder_dim = options.neighbor_vector_dim + self.encoder.input_dim\n",
        "            self.encoder_states = tf.concat([self.encoder.graph_hiddens, self.encoder.node_representations], 2)\n",
        "        else:\n",
        "            assert False, '%s not supported yet' % options.attention_type\n",
        "\n",
        "        self.nodes = self.encoder.passage_nodes\n",
        "        self.nodes_num = self.encoder.passage_nodes_size\n",
        "        if options.with_char:\n",
        "            self.nodes_chars = self.encoder.passage_nodes_chars\n",
        "            self.nodes_chars_num = self.encoder.passage_nodes_chars_size\n",
        "        self.nodes_mask = self.encoder.passage_nodes_mask\n",
        "\n",
        "        self.in_neigh_indices = self.encoder.passage_in_neighbor_indices\n",
        "        self.in_neigh_edges = self.encoder.passage_in_neighbor_edges\n",
        "        self.in_neigh_mask = self.encoder.passage_in_neighbor_mask\n",
        "\n",
        "        self.out_neigh_indices = self.encoder.passage_out_neighbor_indices\n",
        "        self.out_neigh_edges = self.encoder.passage_out_neighbor_edges\n",
        "        self.out_neigh_mask = self.encoder.passage_out_neighbor_mask\n",
        "\n",
        "        ## generating prediction results\n",
        "        self.entity_indices = tf.placeholder(tf.int32, [None, None, None],\n",
        "                name=\"entity_indices\")\n",
        "        self.entity_indices_mask = tf.placeholder(tf.float32, [None, None, None],\n",
        "                name=\"entity_indices_mask\")\n",
        "        batch_size = tf.shape(self.encoder_states)[0]\n",
        "        node_num = tf.shape(self.encoder_states)[1]\n",
        "        dim = tf.shape(self.encoder_states)[2]\n",
        "        entity_num = tf.shape(self.entity_indices)[1]\n",
        "        entity_size = tf.shape(self.entity_indices)[2]\n",
        "\n",
        "        # self.encoder_states [batch, node_num, encoder_dim]\n",
        "        # entity_states [batch, 3, 5, dim]\n",
        "        entity_states = collect_by_indices(self.encoder_states, self.entity_indices)\n",
        "        # applying mask\n",
        "        entity_states = entity_states * tf.expand_dims(self.entity_indices_mask, axis=-1)\n",
        "        # average within each entity: [batch, 3, encoder_dim]\n",
        "        entity_states = tf.reduce_mean(entity_states, axis=2)\n",
        "        # flatten: [batch, 3*encoder_dim]\n",
        "        entity_states = tf.reshape(entity_states, [batch_size, entity_num*dim])\n",
        "\n",
        "        w_linear = tf.get_variable(\"w_linear\",\n",
        "                [options.entity_num*self.encoder_dim, options.class_num], dtype=tf.float32)\n",
        "        b_linear = tf.get_variable(\"b_linear\",\n",
        "                [options.class_num], dtype=tf.float32)\n",
        "        # [batch, class_num]\n",
        "        prediction = tf.nn.softmax(tf.matmul(entity_states, w_linear) + b_linear)\n",
        "        prediction = _clip_and_normalize(prediction, 1.0e-6)\n",
        "        self.output = tf.argmax(prediction,axis=-1,output_type=tf.int32)\n",
        "\n",
        "        ## calculating accuracy\n",
        "        self.refs = tf.placeholder(tf.int32, [None,])\n",
        "        self.accu = tf.reduce_sum(tf.cast(tf.equal(self.output,self.refs),dtype=tf.float32))\n",
        "\n",
        "        ## calculating loss\n",
        "        # xent: [batch]\n",
        "        xent = -tf.reduce_sum(\n",
        "                tf.one_hot(self.refs,options.class_num)*tf.log(prediction),\n",
        "                axis=-1)\n",
        "        self.loss = tf.reduce_mean(xent)\n",
        "\n",
        "        if mode != 'train':\n",
        "            print('Return from here, just evaluate')\n",
        "            return\n",
        "\n",
        "        if options.optimize_type == 'adadelta':\n",
        "            clipper = 50\n",
        "            optimizer = tf.train.AdadeltaOptimizer(learning_rate=options.learning_rate)\n",
        "            tvars = tf.trainable_variables()\n",
        "            if options.lambda_l2>0.0:\n",
        "                l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tvars if v.get_shape().ndims > 1])\n",
        "                self.loss = self.loss + options.lambda_l2 * l2_loss\n",
        "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), clipper)\n",
        "            self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
        "        elif options.optimize_type == 'adam':\n",
        "            clipper = 50\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=options.learning_rate)\n",
        "            tvars = tf.trainable_variables()\n",
        "            if options.lambda_l2>0.0:\n",
        "                l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tvars if v.get_shape().ndims > 1])\n",
        "                self.loss = self.loss + options.lambda_l2 * l2_loss\n",
        "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), clipper)\n",
        "            self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
        "\n",
        "        extra_train_ops = []\n",
        "        train_ops = [self.train_op] + extra_train_ops\n",
        "        self.train_op = tf.group(*train_ops)\n",
        "\n",
        "\n",
        "    def execute(self, sess, batch, options, is_train=False):\n",
        "        feed_dict = {}\n",
        "        feed_dict[self.nodes] = batch.nodes\n",
        "        feed_dict[self.nodes_num] = batch.node_num\n",
        "        if options.with_char:\n",
        "            feed_dict[self.nodes_chars] = batch.nodes_chars\n",
        "            feed_dict[self.nodes_chars_num] = batch.nodes_chars_num\n",
        "\n",
        "        feed_dict[self.in_neigh_indices] = batch.in_neigh_indices\n",
        "        feed_dict[self.in_neigh_edges] = batch.in_neigh_edges\n",
        "        feed_dict[self.in_neigh_mask] = batch.in_neigh_mask\n",
        "\n",
        "        feed_dict[self.out_neigh_indices] = batch.out_neigh_indices\n",
        "        feed_dict[self.out_neigh_edges] = batch.out_neigh_edges\n",
        "        feed_dict[self.out_neigh_mask] = batch.out_neigh_mask\n",
        "\n",
        "        feed_dict[self.entity_indices] = batch.entity_indices\n",
        "        feed_dict[self.entity_indices_mask] = batch.entity_indices_mask\n",
        "        feed_dict[self.refs] = batch.y\n",
        "        if is_train:\n",
        "            return sess.run([self.accu, self.loss, self.train_op], feed_dict)\n",
        "        else:\n",
        "            return sess.run([self.accu, self.loss, self.output], feed_dict)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bf0jXTYiWVgI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm '/usr/local/lib/python2.7/dist-packages/pythonrouge/RELEASE-1.5.5/data/WordNet-2.0.exc.db'\n",
        "!rm '/usr/local/lib/python2.7/dist-packages/pythonrouge/RELEASE-1.5.5/data/WordNet-2.0-Exceptions/WordNet-2.0.exc.db'\n",
        "!cpan XML::DOM\n",
        "!perl '/usr/local/lib/python2.7/dist-packages/pythonrouge/RELEASE-1.5.5/data/WordNet-2.0-Exceptions/buildExeptionDB.pl' '/usr/local/lib/python2.7/dist-packages/pythonrouge/RELEASE-1.5.5/data/WordNet-2.0-Exceptions' '/usr/local/lib/python2.7/dist-packages/pythonrouge/RELEASE-1.5.5/data/smart_common_words.txt' '/usr/local/lib/python2.7/dist-packages/pythonrouge/RELEASE-1.5.5/data/WordNet-2.0.exc.db'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KeyqcHm6YJ7i",
        "colab_type": "code",
        "outputId": "6f7a0014-2fff-4859-97d5-6e500e2cf67c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "!pip uninstall pythonrouge\n",
        "!pip install git+https://github.com/tagucci/pythonrouge.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling pythonrouge-0.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python2.7/dist-packages/pythonrouge-0.2.dist-info/*\n",
            "    /usr/local/lib/python2.7/dist-packages/pythonrouge/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled pythonrouge-0.2\n",
            "Collecting git+https://github.com/tagucci/pythonrouge.git\n",
            "  Cloning https://github.com/tagucci/pythonrouge.git to /tmp/pip-req-build-Xd9aQ9\n",
            "  Running command git clone -q https://github.com/tagucci/pythonrouge.git /tmp/pip-req-build-Xd9aQ9\n",
            "Building wheels for collected packages: pythonrouge\n",
            "  Building wheel for pythonrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5l463C/wheels/fd/ff/be/6716935d513fa8656ab185cb0aa70aed382b72dda42bf09c95\n",
            "Successfully built pythonrouge\n",
            "Installing collected packages: pythonrouge\n",
            "Successfully installed pythonrouge-0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pythonrouge"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "utLgduwHY1-R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_text_file(text_file):\n",
        "    lines = []\n",
        "    with open(text_file, \"rt\") as f:\n",
        "        for line in f:\n",
        "            line = line.decode('utf-8')\n",
        "            lines.append(line.strip())\n",
        "    return lines\n",
        "\n",
        "def read_nary_file(inpath, options):\n",
        "    all_words = []\n",
        "    all_lemmas = []\n",
        "    all_poses = []\n",
        "    all_in_neigh = []\n",
        "    all_in_label = []\n",
        "    all_out_neigh = [] # [batch, node, neigh]\n",
        "    all_out_label = [] # [batch, node, neigh]\n",
        "    all_entity_indices = [] # [batch, 3, entity_size]\n",
        "    all_y = []\n",
        "    if options.class_num == 2:\n",
        "        relation_set = {'resistance or non-response':0, 'sensitivity':0, 'response':0, 'resistance':0, 'None':1, }\n",
        "    elif options.class_num == 5:\n",
        "        relation_set = {'resistance or non-response':0, 'sensitivity':1, 'response':2, 'resistance':3, 'None':4, }\n",
        "    else:\n",
        "        assert False, 'Illegal class num'\n",
        "    max_words = 0\n",
        "    max_in_neigh = 0\n",
        "    max_out_neigh = 0\n",
        "    max_entity_size = 0\n",
        "    with codecs.open(inpath, 'rU', 'utf-8') as f:\n",
        "        for inst in json.load(f):\n",
        "            words = []\n",
        "            lemmas = []\n",
        "            poses = []\n",
        "            if options.only_single_sent and len(inst['sentences']) > 1:\n",
        "                continue\n",
        "            for sentence in inst['sentences']:\n",
        "                for node in sentence['nodes']:\n",
        "                    words.append(node['label'])\n",
        "                    lemmas.append(node['lemma'])\n",
        "                    poses.append(node['postag'])\n",
        "            max_words = max(max_words, len(words))\n",
        "            all_words.append(words)\n",
        "            all_lemmas.append(lemmas)\n",
        "            all_poses.append(poses)\n",
        "            in_neigh = [[i,] for i,_ in enumerate(words)]\n",
        "            in_label = [['self',] for i,_ in enumerate(words)]\n",
        "            out_neigh = [[i,] for i,_ in enumerate(words)]\n",
        "            out_label = [['self',] for i,_ in enumerate(words)]\n",
        "            for sentence in inst['sentences']:\n",
        "                for node in sentence['nodes']:\n",
        "                    i = node['index']\n",
        "                    for arc in node['arcs']:\n",
        "                        j = arc['toIndex']\n",
        "                        l = arc['label']\n",
        "                        l = l.split('::')[0]\n",
        "                        l = l.split('_')[0]\n",
        "                        l = l.split('(')[0]\n",
        "                        if j == -1 or l == '':\n",
        "                            continue\n",
        "                        in_neigh[j].append(i)\n",
        "                        in_label[j].append(l)\n",
        "                        out_neigh[i].append(j)\n",
        "                        out_label[i].append(l)\n",
        "            for _i in in_neigh:\n",
        "                max_in_neigh = max(max_in_neigh, len(_i))\n",
        "            for _o in out_neigh:\n",
        "                max_out_neigh = max(max_out_neigh, len(_o))\n",
        "            all_in_neigh.append(in_neigh)\n",
        "            all_in_label.append(in_label)\n",
        "            all_out_neigh.append(out_neigh)\n",
        "            all_out_label.append(out_label)\n",
        "            entity_indices = []\n",
        "            for entity in inst['entities']:\n",
        "                entity_indices.append(entity['indices'])\n",
        "                max_entity_size = max(max_entity_size, len(entity['indices']))\n",
        "            assert len(entity_indices) == options.entity_num\n",
        "            all_entity_indices.append(entity_indices)\n",
        "            all_y.append(relation_set[inst['relationLabel'].strip()])\n",
        "    all_lex = all_lemmas if options.word_format == 'lemma' else all_words\n",
        "    return zip(all_lex, all_poses, all_in_neigh, all_in_label, all_out_neigh, all_out_label, all_entity_indices, all_y), \\\n",
        "            max_words, max_in_neigh, max_out_neigh, max_entity_size\n",
        "\n",
        "\n",
        "def read_nary_from_fof(fofpath, options):\n",
        "    all_paths = read_text_file(fofpath)\n",
        "    all_instances = []\n",
        "    max_words = 0\n",
        "    max_in_neigh = 0\n",
        "    max_out_neigh = 0\n",
        "    max_entity_size = 0\n",
        "    for cur_path in all_paths:\n",
        "        print(cur_path)\n",
        "        cur_instances, cur_words, cur_in_neigh, cur_out_neigh, cur_entity_size = read_nary_file(cur_path, options)\n",
        "        all_instances.extend(cur_instances)\n",
        "        max_words = max(max_words, cur_words)\n",
        "        max_in_neigh = max(max_in_neigh, cur_in_neigh)\n",
        "        max_out_neigh = max(max_out_neigh, cur_out_neigh)\n",
        "        max_entity_size = max(max_entity_size, cur_entity_size)\n",
        "    return all_instances, max_words, max_in_neigh, max_out_neigh, max_entity_size\n",
        "\n",
        "\n",
        "def collect_vocabs(all_instances):\n",
        "    all_words = set()\n",
        "    all_chars = set()\n",
        "    all_edgelabels = set()\n",
        "    for (lex, poses, in_neigh, in_label, out_neigh, out_label, entity_indices, y) in all_instances:\n",
        "        all_words.update(lex)\n",
        "        for l in lex:\n",
        "            if l.isspace() == False: all_chars.update(l)\n",
        "        for edges in in_label:\n",
        "            all_edgelabels.update(edges)\n",
        "        for edges in out_label:\n",
        "            all_edgelabels.update(edges)\n",
        "    return (all_words, all_chars, all_edgelabels)\n",
        "\n",
        "class G2SDataStream(object):\n",
        "    def __init__(self, all_instances, word_vocab=None, char_vocab=None, edgelabel_vocab=None, options=None,\n",
        "                 isShuffle=False, isLoop=False, isSort=True, batch_size=-1):\n",
        "        self.options = options\n",
        "        if batch_size ==-1: batch_size=options.batch_size\n",
        "        # index tokens and filter the dataset\n",
        "        instances = []\n",
        "        for (lex, poses, in_neigh, in_label, out_neigh, out_label, entity_indices, y) in all_instances:\n",
        "            if options.max_node_num != -1 and len(lex) > options.max_node_num:\n",
        "                continue # remove very long passages\n",
        "            in_neigh = [x[:options.max_in_neigh_num] for x in in_neigh]\n",
        "            in_label = [x[:options.max_in_neigh_num] for x in in_label]\n",
        "            out_neigh = [x[:options.max_out_neigh_num] for x in out_neigh]\n",
        "            out_label = [x[:options.max_out_neigh_num] for x in out_label]\n",
        "\n",
        "            lex_idx = word_vocab.to_index_sequence_for_list(lex)\n",
        "            lex_chars_idx = None\n",
        "            if options.with_char:\n",
        "                lex_chars_idx = char_vocab.to_character_matrix_for_list(lex, max_char_per_word=options.max_char_per_word)\n",
        "            in_label_idx = [edgelabel_vocab.to_index_sequence_for_list(edges) for edges in in_label]\n",
        "            out_label_idx = [edgelabel_vocab.to_index_sequence_for_list(edges) for edges in out_label]\n",
        "            instances.append((lex_idx, lex_chars_idx, in_neigh, in_label_idx, out_neigh, out_label_idx, entity_indices, y))\n",
        "\n",
        "        all_instances = instances\n",
        "        instances = None\n",
        "\n",
        "        # sort instances based on length\n",
        "        if isSort:\n",
        "            all_instances = sorted(all_instances, key=lambda inst: len(inst[0]))\n",
        "        if isShuffle:\n",
        "            random.shuffle(all_instances)\n",
        "            random.shuffle(all_instances)\n",
        "        self.num_instances = len(all_instances)\n",
        "\n",
        "        # distribute questions into different buckets\n",
        "        batch_spans = make_batches(self.num_instances, batch_size)\n",
        "        self.batches = []\n",
        "        for batch_index, (batch_start, batch_end) in enumerate(batch_spans):\n",
        "            cur_instances = []\n",
        "            for i in xrange(batch_start, batch_end):\n",
        "                cur_instances.append(all_instances[i])\n",
        "            cur_batch = G2SBatch(cur_instances, options, word_vocab=word_vocab)\n",
        "            self.batches.append(cur_batch)\n",
        "\n",
        "        self.num_batch = len(self.batches)\n",
        "        self.index_array = np.arange(self.num_batch)\n",
        "        self.isShuffle = isShuffle\n",
        "        if self.isShuffle: np.random.shuffle(self.index_array)\n",
        "        self.isLoop = isLoop\n",
        "        self.cur_pointer = 0\n",
        "\n",
        "    def nextBatch(self):\n",
        "        if self.cur_pointer>=self.num_batch:\n",
        "            if not self.isLoop: return None\n",
        "            self.cur_pointer = 0\n",
        "            if self.isShuffle: np.random.shuffle(self.index_array)\n",
        "        cur_batch = self.batches[self.index_array[self.cur_pointer]]\n",
        "        self.cur_pointer += 1\n",
        "        return cur_batch\n",
        "\n",
        "    def reset(self):\n",
        "        if self.isShuffle: np.random.shuffle(self.index_array)\n",
        "        self.cur_pointer = 0\n",
        "\n",
        "    def get_num_batch(self):\n",
        "        return self.num_batch\n",
        "\n",
        "    def get_num_instance(self):\n",
        "        return self.num_instances\n",
        "\n",
        "    def get_batch(self, i):\n",
        "        if i>= self.num_batch: return None\n",
        "        return self.batches[i]\n",
        "\n",
        "class G2SBatch(object):\n",
        "    def __init__(self, instances, options, word_vocab=None):\n",
        "        self.options = options\n",
        "\n",
        "        self.instances = instances # list of tuples\n",
        "        self.batch_size = len(instances)\n",
        "        self.vocab = word_vocab\n",
        "\n",
        "        # node num\n",
        "        self.node_num = [] # [batch_size]\n",
        "        for (lex_idx, lex_chars_idx, in_neigh, in_label_idx, out_neigh, out_label_idx, entity_indices, y) in instances:\n",
        "            self.node_num.append(len(lex_idx))\n",
        "        self.node_num = np.array(self.node_num, dtype=np.int32)\n",
        "\n",
        "        # node char num\n",
        "        if options.with_char:\n",
        "            self.nodes_chars_num = [[len(lex_chars_idx) for lex_chars_idx in instance[1]] for instance in instances]\n",
        "            self.nodes_chars_num = pad_2d_vals_no_size(self.nodes_chars_num)\n",
        "\n",
        "        # neigh mask\n",
        "        self.in_neigh_mask = [] # [batch_size, node_num, neigh_num]\n",
        "        self.out_neigh_mask = []\n",
        "        self.entity_indices_mask = []\n",
        "        for instance in instances:\n",
        "            ins = []\n",
        "            for in_neighs in instance[2]:\n",
        "                ins.append([1 for _ in in_neighs])\n",
        "            self.in_neigh_mask.append(ins)\n",
        "            outs = []\n",
        "            for out_neighs in instance[4]:\n",
        "                outs.append([1 for _ in out_neighs])\n",
        "            self.out_neigh_mask.append(outs)\n",
        "            idxs = []\n",
        "            for entity_indices in instance[6]:\n",
        "                idxs.append([1 for _ in entity_indices])\n",
        "            self.entity_indices_mask.append(idxs)\n",
        "        self.in_neigh_mask = pad_3d_vals_no_size(self.in_neigh_mask)\n",
        "        self.out_neigh_mask = pad_3d_vals_no_size(self.out_neigh_mask)\n",
        "        self.entity_indices_mask = pad_3d_vals_no_size(self.entity_indices_mask)\n",
        "\n",
        "        # the actual contents\n",
        "        self.nodes = [x[0] for x in instances]\n",
        "        if options.with_char:\n",
        "            self.nodes_chars = [x[1] for x in instances] # [batch_size, sent_len, char_num]\n",
        "        self.in_neigh_indices = [x[2] for x in instances]\n",
        "        self.in_neigh_edges = [x[3] for x in instances]\n",
        "        self.out_neigh_indices = [x[4] for x in instances]\n",
        "        self.out_neigh_edges = [x[5] for x in instances]\n",
        "        self.entity_indices = [x[6] for x in instances]\n",
        "        self.y = [x[7] for x in instances]\n",
        "\n",
        "        # making ndarray\n",
        "        self.nodes = pad_2d_vals_no_size(self.nodes)\n",
        "        if options.with_char:\n",
        "            self.nodes_chars = pad_3d_vals_no_size(self.nodes_chars)\n",
        "        self.in_neigh_indices = pad_3d_vals_no_size(self.in_neigh_indices)\n",
        "        self.in_neigh_edges = pad_3d_vals_no_size(self.in_neigh_edges)\n",
        "        self.out_neigh_indices = pad_3d_vals_no_size(self.out_neigh_indices)\n",
        "        self.out_neigh_edges = pad_3d_vals_no_size(self.out_neigh_edges)\n",
        "        self.entity_indices = pad_3d_vals_no_size(self.entity_indices)\n",
        "        self.y = np.asarray(self.y, dtype='int32')\n",
        "\n",
        "        assert self.in_neigh_mask.shape == self.in_neigh_indices.shape\n",
        "        assert self.in_neigh_mask.shape == self.in_neigh_edges.shape\n",
        "        assert self.out_neigh_mask.shape == self.out_neigh_indices.shape\n",
        "        assert self.out_neigh_mask.shape == self.out_neigh_edges.shape\n",
        "        assert self.entity_indices_mask.shape == self.entity_indices.shape\n",
        "\n",
        "        assert self.entity_indices.shape[1] == options.entity_num\n",
        "        assert self.entity_indices_mask.shape[1] == options.entity_num\n",
        "\n",
        "    def get_amrside_anonyids(self, anony_ids):\n",
        "        assert self.batch_size == 1 # only for beam search\n",
        "        if self.options.__dict__.has_key(\"enc_word_vec_path\"):\n",
        "            assert self.options.enc_word_vec_path == self.options.dec_word_vec_path # only when enc_vocab == dec_vocab\n",
        "        self.amr_anony_ids = set(self.instances[0][0]) & anony_ids # sent1 of inst_0\n",
        "\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "    \"\"\"all_instances, max_node_num, max_in_neigh_num, max_out_neigh_num, max_entity_size = read_nary_from_fof(\n",
        "            './data/data_list', 'lemma')\n",
        "    print sum(len(x[0]) for x in all_instances)/len(all_instances)\n",
        "    print(max_in_neigh_num)\n",
        "    print(max_out_neigh_num)\n",
        "    print(max_node_num)\n",
        "    print(max_entity_size)\n",
        "    print('DONE!')\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5u3aSrGNZw-G",
        "colab_type": "code",
        "outputId": "821a9159-802b-45af-ccc0-2f6d3f8b6a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6929
        }
      },
      "cell_type": "code",
      "source": [
        "cc = SmoothingFunction()\n",
        "FLAGS = None\n",
        "\n",
        "def get_machine_name():\n",
        "    return platform.node()\n",
        "\n",
        "def vec2string(val):\n",
        "    result = \"\"\n",
        "    for v in val:\n",
        "        result += \" {}\".format(v)\n",
        "    return result.strip()\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "\n",
        "def document_bleu(vocab, gen, ref, suffix=''):\n",
        "    genlex = [vocab.getLexical(x)[1] for x in gen]\n",
        "    reflex = [[vocab.getLexical(x)[1],] for x in ref]\n",
        "    genlst = [x.split() for x in genlex]\n",
        "    reflst = [[x[0].split()] for x in reflex]\n",
        "    f = codecs.open('gen.txt'+suffix,'w','utf-8')\n",
        "    for line in genlex:\n",
        "        print(line, end='\\n', file=f)\n",
        "    f.close()\n",
        "    f = codecs.open('ref.txt'+suffix,'w','utf-8')\n",
        "    for line in reflex:\n",
        "        print(line[0], end='\\n', file=f)\n",
        "    f.close()\n",
        "    return corpus_bleu(reflst, genlst, smoothing_function=cc.method3)\n",
        "\n",
        "\n",
        "def evaluate(sess, valid_graph, devDataStream, options=None, suffix=''):\n",
        "    devDataStream.reset()\n",
        "    instances = []\n",
        "    outputs = []\n",
        "    dev_loss = 0.0\n",
        "    dev_right = 0.0\n",
        "    dev_total = 0.0\n",
        "    for batch_index in xrange(devDataStream.get_num_batch()): # for each batch\n",
        "        cur_batch = devDataStream.get_batch(batch_index)\n",
        "        accu_value, loss_value, output_value = valid_graph.execute(sess, cur_batch, options, is_train=False)\n",
        "        instances += cur_batch.instances\n",
        "        outputs += output_value.flatten().tolist()\n",
        "        dev_loss += loss_value\n",
        "        dev_right += accu_value\n",
        "        dev_total += cur_batch.batch_size\n",
        "\n",
        "    return {'dev_loss':dev_loss, 'dev_accu':1.0*dev_right/dev_total, 'dev_right':dev_right, 'dev_total':dev_total, 'data':(instances,outputs)}\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    print('Configurations:')\n",
        "    print(FLAGS)\n",
        "\n",
        "    log_dir = FLAGS.model_dir\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    path_prefix = log_dir + \"/G2S.{}\".format(FLAGS.suffix)\n",
        "    log_file_path = path_prefix + \".log\"\n",
        "    print('Log file path: {}'.format(log_file_path))\n",
        "    log_file = open(log_file_path, 'wt')\n",
        "    log_file.write(\"{}\\n\".format(FLAGS))\n",
        "    log_file.flush()\n",
        "\n",
        "    # save configuration\n",
        "    save_namespace(FLAGS, path_prefix + \".config.json\")\n",
        "\n",
        "    print('Loading train set.')\n",
        "    if FLAGS.infile_format == 'fof':\n",
        "        trainset, trn_node, trn_in_neigh, trn_out_neigh, trn_sent = read_nary_from_fof(FLAGS.train_path, FLAGS)\n",
        "    else:\n",
        "        trainset, trn_node, trn_in_neigh, trn_out_neigh, trn_sent = read_nary_file(FLAGS.train_path, FLAGS)\n",
        "\n",
        "    random.shuffle(trainset)\n",
        "    devset = trainset[:200]\n",
        "    trainset = trainset[200:]\n",
        "\n",
        "    print('Number of training samples: {}'.format(len(trainset)))\n",
        "    print('Number of dev samples: {}'.format(len(devset)))\n",
        "\n",
        "    max_node = trn_node\n",
        "    max_in_neigh = trn_in_neigh\n",
        "    max_out_neigh = trn_out_neigh\n",
        "    max_sent = trn_sent\n",
        "    print('Max node number: {}, while max allowed is {}'.format(max_node, FLAGS.max_node_num))\n",
        "    print('Max parent number: {}, truncated to {}'.format(max_in_neigh, FLAGS.max_in_neigh_num))\n",
        "    print('Max children number: {}, truncated to {}'.format(max_out_neigh, FLAGS.max_out_neigh_num))\n",
        "    print('Max entity size: {}, truncated to {}'.format(max_sent, FLAGS.max_entity_size))\n",
        "\n",
        "    word_vocab = None\n",
        "    char_vocab = None\n",
        "    edgelabel_vocab = None\n",
        "    has_pretrained_model = False\n",
        "    best_path = path_prefix + \".best.model\"\n",
        "    if os.path.exists(best_path + \".index\"):\n",
        "        has_pretrained_model = True\n",
        "        print('!!Existing pretrained model. Loading vocabs.')\n",
        "        word_vocab = Vocab(FLAGS.word_vec_path, fileformat='txt2')\n",
        "        print('word_vocab: {}'.format(word_vocab.word_vecs.shape))\n",
        "        char_vocab = None\n",
        "        if FLAGS.with_char:\n",
        "            char_vocab = Vocab(path_prefix + \".char_vocab\", fileformat='txt2')\n",
        "            print('char_vocab: {}'.format(char_vocab.word_vecs.shape))\n",
        "        edgelabel_vocab = Vocab(path_prefix + \".edgelabel_vocab\", fileformat='txt2')\n",
        "    else:\n",
        "        print('Collecting vocabs.')\n",
        "        (allWords, allChars, allEdgelabels) = collect_vocabs(trainset)\n",
        "        print('Number of words: {}'.format(len(allWords)))\n",
        "        print('Number of allChars: {}'.format(len(allChars)))\n",
        "        print('Number of allEdgelabels: {}'.format(len(allEdgelabels)))\n",
        "\n",
        "        word_vocab = Vocab(FLAGS.word_vec_path, fileformat='txt2')\n",
        "        char_vocab = None\n",
        "        if FLAGS.with_char:\n",
        "            char_vocab = Vocab(voc=allChars, dim=FLAGS.char_dim, fileformat='build')\n",
        "            char_vocab.dump_to_txt2(path_prefix + \".char_vocab\")\n",
        "        edgelabel_vocab = Vocab(voc=allEdgelabels, dim=FLAGS.edgelabel_dim, fileformat='build')\n",
        "        edgelabel_vocab.dump_to_txt2(path_prefix + \".edgelabel_vocab\")\n",
        "\n",
        "    print('word vocab size {}'.format(word_vocab.vocab_size))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    print('Build DataStream ... ')\n",
        "    trainDataStream = G2SDataStream(trainset, word_vocab, char_vocab, edgelabel_vocab, options=FLAGS,\n",
        "                 isShuffle=True, isLoop=True, isSort=False)\n",
        "\n",
        "    devDataStream = G2SDataStream(devset, word_vocab, char_vocab, edgelabel_vocab, options=FLAGS,\n",
        "                 isShuffle=False, isLoop=False, isSort=False)\n",
        "    print('Number of instances in trainDataStream: {}'.format(trainDataStream.get_num_instance()))\n",
        "    print('Number of instances in devDataStream: {}'.format(devDataStream.get_num_instance()))\n",
        "    print('Number of batches in trainDataStream: {}'.format(trainDataStream.get_num_batch()))\n",
        "    print('Number of batches in devDataStream: {}'.format(devDataStream.get_num_batch()))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    # initialize the best bleu and accu scores for current training session\n",
        "    best_accu = FLAGS.best_accu if FLAGS.__dict__.has_key('best_accu') else 0.0\n",
        "    if best_accu > 0.0:\n",
        "        print('With initial dev accuracy {}'.format(best_accu))\n",
        "\n",
        "    init_scale = 0.01\n",
        "    with tf.Graph().as_default():\n",
        "        initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
        "        with tf.name_scope(\"Train\"):\n",
        "            with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
        "                train_graph = ModelGraph(word_vocab=word_vocab, Edgelabel_vocab=edgelabel_vocab,\n",
        "                                         char_vocab=char_vocab, options=FLAGS, mode='train')\n",
        "\n",
        "        with tf.name_scope(\"Valid\"):\n",
        "            with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
        "                valid_graph = ModelGraph(word_vocab=word_vocab, Edgelabel_vocab=edgelabel_vocab,\n",
        "                                         char_vocab=char_vocab, options=FLAGS, mode='evaluate')\n",
        "\n",
        "        initializer = tf.global_variables_initializer()\n",
        "\n",
        "        vars_ = {}\n",
        "        for var in tf.all_variables():\n",
        "            if \"word_embedding\" in var.name: continue\n",
        "            if not var.name.startswith(\"Model\"): continue\n",
        "            vars_[var.name.split(\":\")[0]] = var\n",
        "        saver = tf.train.Saver(vars_)\n",
        "\n",
        "        sess = tf.Session()\n",
        "        sess.run(initializer)\n",
        "        if has_pretrained_model:\n",
        "            print(\"Restoring model from \" + best_path)\n",
        "            saver.restore(sess, best_path)\n",
        "            print(\"DONE!\")\n",
        "\n",
        "            if abs(best_accu) < 0.00001:\n",
        "                print(\"Getting ACCU score for the model\")\n",
        "                best_accu = evaluate(sess, valid_graph, devDataStream, options=FLAGS)['dev_accu']\n",
        "                FLAGS.best_accu = best_accu\n",
        "                save_namespace(FLAGS, path_prefix + \".config.json\")\n",
        "                print('ACCU = %.4f' % best_accu)\n",
        "                log_file.write('ACCU = %.4f\\n' % best_accu)\n",
        "\n",
        "        print('Start the training loop.')\n",
        "        train_size = trainDataStream.get_num_batch()\n",
        "        max_steps = train_size * FLAGS.max_epochs\n",
        "        last_step = 0\n",
        "        total_loss = 0.0\n",
        "        start_time = time.time()\n",
        "        for step in xrange(max_steps):\n",
        "            cur_batch = trainDataStream.nextBatch()\n",
        "            _, loss_value, _ = train_graph.execute(sess, cur_batch, FLAGS, is_train=True)\n",
        "            total_loss += loss_value\n",
        "\n",
        "            if step % 100==0:\n",
        "                print('{} '.format(step), end=\"\")\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            # Save a checkpoint and evaluate the model periodically.\n",
        "            if (step + 1) % trainDataStream.get_num_batch() == 0 or (step + 1) == max_steps:\n",
        "                print()\n",
        "                duration = time.time() - start_time\n",
        "                print('Step %d: loss = %.2f (%.3f sec)' % (step, total_loss/(step-last_step), duration))\n",
        "                log_file.write('Step %d: loss = %.2f (%.3f sec)\\n' % (step, total_loss/(step-last_step), duration))\n",
        "                sys.stdout.flush()\n",
        "                log_file.flush()\n",
        "                last_step = step\n",
        "                total_loss = 0.0\n",
        "\n",
        "                # Evaluate against the validation set.\n",
        "                start_time = time.time()\n",
        "                print('Validation Data Eval:')\n",
        "                res_dict = evaluate(sess, valid_graph, devDataStream, options=FLAGS, suffix=str(step))\n",
        "                dev_loss = res_dict['dev_loss']\n",
        "                dev_accu = res_dict['dev_accu']\n",
        "                dev_right = int(res_dict['dev_right'])\n",
        "                dev_total = int(res_dict['dev_total'])\n",
        "                print('Dev loss = %.4f' % dev_loss)\n",
        "                log_file.write('Dev loss = %.4f\\n' % dev_loss)\n",
        "                print('Dev accu = %.4f %d/%d' % (dev_accu, dev_right, dev_total))\n",
        "                log_file.write('Dev accu = %.4f %d/%d\\n' % (dev_accu, dev_right, dev_total))\n",
        "                log_file.flush()\n",
        "                if best_accu < dev_accu:\n",
        "                    print('Saving weights, ACCU {} (prev_best) < {} (cur)'.format(best_accu, dev_accu))\n",
        "                    saver.save(sess, best_path)\n",
        "                    best_accu = dev_accu\n",
        "                    FLAGS.best_accu = dev_accu\n",
        "                    save_namespace(FLAGS, path_prefix + \".config.json\")\n",
        "                    json.dump(res_dict['data'], open(FLAGS.output_path,'w'))\n",
        "                duration = time.time() - start_time\n",
        "                print('Duration %.3f sec' % (duration))\n",
        "                sys.stdout.flush()\n",
        "\n",
        "                log_file.write('Duration %.3f sec\\n' % (duration))\n",
        "                log_file.flush()\n",
        "                print(\"(step + 1) == max_steps\",step+1,max_steps)\n",
        "        \n",
        "    log_file.close()\n",
        "\n",
        "\n",
        "def enrich_options(options):\n",
        "    if not options.__dict__.has_key(\"infile_format\"):\n",
        "        options.__dict__[\"infile_format\"] = \"fof\"\n",
        "\n",
        "    return options\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config_path', type=str,nargs='?',default ='/content/drive/My Drive/nary-grn-master/gs_lstm/config.json', help='Configuration file.')\n",
        "    \n",
        "    #os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
        "    #os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
        "    \n",
        "    #print(\"CUDA_VISIBLE_DEVICES \" + os.environ['CUDA_VISIBLE_DEVICES'])\n",
        "    FLAGS, unparsed = parser.parse_known_args()\n",
        "\n",
        "    print(FLAGS.config_path)\n",
        "    unparsed=None\n",
        "    if FLAGS.config_path is not None:\n",
        "        print('Loading the configuration from ' + FLAGS.config_path)\n",
        "        FLAGS = load_namespace(FLAGS.config_path)\n",
        "    \n",
        "    FLAGS = enrich_options(FLAGS)\n",
        "    \n",
        "\n",
        "    sys.stdout.flush()\n",
        "    tf.app.run(main=main,argv=None)\n",
        "    #If running through command line\n",
        "    #tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/nary-grn-master/gs_lstm/config.json\n",
            "Loading the configuration from /content/drive/My Drive/nary-grn-master/gs_lstm/config.json\n",
            "Configurations:\n",
            "<__main__.Bunch object at 0x7fab6a49e390>\n",
            "Log file path: logs/G2S.cross_mul_0.log\n",
            "Loading train set.\n",
            "/content/drive/My Drive/nary-grn-master/peng_data/data/drug_gene_var/1/data_graph_1\n",
            "Number of training samples: 505\n",
            "Number of dev samples: 200\n",
            "Max node number: 343, while max allowed is 450\n",
            "Max parent number: 34, truncated to 45\n",
            "Max children number: 34, truncated to 45\n",
            "Max entity size: 5, truncated to 5\n",
            "Collecting vocabs.\n",
            "Number of words: 2665\n",
            "Number of allChars: 98\n",
            "Number of allEdgelabels: 109\n",
            "word vocab size 15736\n",
            "Build DataStream ... \n",
            "Number of instances in trainDataStream: 505\n",
            "Number of instances in devDataStream: 200\n",
            "Number of batches in trainDataStream: 64\n",
            "Number of batches in devDataStream: 25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Return from here, just evaluate\n",
            "Start the training loop.\n",
            "0 \n",
            "Step 63: loss = 1.52 (7.623 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 31.0420\n",
            "Dev accu = 0.3850 77/200\n",
            "Saving weights, ACCU 0.0 (prev_best) < 0.385 (cur)\n",
            "Duration 3.013 sec\n",
            "(step + 1) == max_steps 64 3200\n",
            "100 \n",
            "Step 127: loss = 1.21 (8.224 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 30.2684\n",
            "Dev accu = 0.3850 77/200\n",
            "Duration 0.881 sec\n",
            "(step + 1) == max_steps 128 3200\n",
            "\n",
            "Step 191: loss = 1.19 (6.049 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 29.9925\n",
            "Dev accu = 0.4000 80/200\n",
            "Saving weights, ACCU 0.385 (prev_best) < 0.4 (cur)\n",
            "Duration 1.929 sec\n",
            "(step + 1) == max_steps 192 3200\n",
            "200 \n",
            "Step 255: loss = 1.18 (7.174 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 29.8386\n",
            "Dev accu = 0.4450 89/200\n",
            "Saving weights, ACCU 0.4 (prev_best) < 0.445 (cur)\n",
            "Duration 1.969 sec\n",
            "(step + 1) == max_steps 256 3200\n",
            "300 \n",
            "Step 319: loss = 1.17 (7.151 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 29.6396\n",
            "Dev accu = 0.4050 81/200\n",
            "Duration 0.966 sec\n",
            "(step + 1) == max_steps 320 3200\n",
            "\n",
            "Step 383: loss = 1.17 (6.648 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 29.4373\n",
            "Dev accu = 0.4000 80/200\n",
            "Duration 0.961 sec\n",
            "(step + 1) == max_steps 384 3200\n",
            "400 \n",
            "Step 447: loss = 1.15 (6.330 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 29.1633\n",
            "Dev accu = 0.4150 83/200\n",
            "Duration 0.867 sec\n",
            "(step + 1) == max_steps 448 3200\n",
            "500 \n",
            "Step 511: loss = 1.14 (6.177 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 29.0393\n",
            "Dev accu = 0.4200 84/200\n",
            "Duration 0.865 sec\n",
            "(step + 1) == max_steps 512 3200\n",
            "\n",
            "Step 575: loss = 1.13 (6.368 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 28.7546\n",
            "Dev accu = 0.4850 97/200\n",
            "Saving weights, ACCU 0.445 (prev_best) < 0.485 (cur)\n",
            "Duration 2.146 sec\n",
            "(step + 1) == max_steps 576 3200\n",
            "600 \n",
            "Step 639: loss = 1.11 (7.589 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 28.5366\n",
            "Dev accu = 0.4600 92/200\n",
            "Duration 0.854 sec\n",
            "(step + 1) == max_steps 640 3200\n",
            "700 \n",
            "Step 703: loss = 1.10 (6.019 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 28.2814\n",
            "Dev accu = 0.4750 95/200\n",
            "Duration 0.853 sec\n",
            "(step + 1) == max_steps 704 3200\n",
            "\n",
            "Step 767: loss = 1.09 (6.053 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 28.1346\n",
            "Dev accu = 0.4800 96/200\n",
            "Duration 0.861 sec\n",
            "(step + 1) == max_steps 768 3200\n",
            "800 \n",
            "Step 831: loss = 1.08 (6.064 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 28.0100\n",
            "Dev accu = 0.5000 100/200\n",
            "Saving weights, ACCU 0.485 (prev_best) < 0.5 (cur)\n",
            "Duration 2.406 sec\n",
            "(step + 1) == max_steps 832 3200\n",
            "\n",
            "Step 895: loss = 1.07 (7.538 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 27.6572\n",
            "Dev accu = 0.4850 97/200\n",
            "Duration 0.860 sec\n",
            "(step + 1) == max_steps 896 3200\n",
            "900 \n",
            "Step 959: loss = 1.06 (5.950 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 27.5137\n",
            "Dev accu = 0.4850 97/200\n",
            "Duration 0.867 sec\n",
            "(step + 1) == max_steps 960 3200\n",
            "1000 \n",
            "Step 1023: loss = 1.06 (5.932 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 27.3841\n",
            "Dev accu = 0.4850 97/200\n",
            "Duration 0.847 sec\n",
            "(step + 1) == max_steps 1024 3200\n",
            "\n",
            "Step 1087: loss = 1.04 (5.949 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 27.2998\n",
            "Dev accu = 0.5050 101/200\n",
            "Saving weights, ACCU 0.5 (prev_best) < 0.505 (cur)\n",
            "Duration 1.949 sec\n",
            "(step + 1) == max_steps 1088 3200\n",
            "1100 \n",
            "Step 1151: loss = 1.04 (7.062 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 26.7006\n",
            "Dev accu = 0.5250 105/200\n",
            "Saving weights, ACCU 0.505 (prev_best) < 0.525 (cur)\n",
            "Duration 1.953 sec\n",
            "(step + 1) == max_steps 1152 3200\n",
            "1200 \n",
            "Step 1215: loss = 1.01 (7.082 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 26.6593\n",
            "Dev accu = 0.4850 97/200\n",
            "Duration 0.873 sec\n",
            "(step + 1) == max_steps 1216 3200\n",
            "\n",
            "Step 1279: loss = 1.01 (5.940 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 26.5771\n",
            "Dev accu = 0.5150 103/200\n",
            "Duration 0.848 sec\n",
            "(step + 1) == max_steps 1280 3200\n",
            "1300 \n",
            "Step 1343: loss = 1.01 (6.081 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 26.2459\n",
            "Dev accu = 0.5000 100/200\n",
            "Duration 0.940 sec\n",
            "(step + 1) == max_steps 1344 3200\n",
            "1400 \n",
            "Step 1407: loss = 0.99 (6.525 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 26.2595\n",
            "Dev accu = 0.5100 102/200\n",
            "Duration 0.937 sec\n",
            "(step + 1) == max_steps 1408 3200\n",
            "\n",
            "Step 1471: loss = 0.99 (6.059 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 25.9463\n",
            "Dev accu = 0.5100 102/200\n",
            "Duration 0.854 sec\n",
            "(step + 1) == max_steps 1472 3200\n",
            "1500 \n",
            "Step 1535: loss = 0.98 (5.966 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 26.0704\n",
            "Dev accu = 0.4950 99/200\n",
            "Duration 0.864 sec\n",
            "(step + 1) == max_steps 1536 3200\n",
            "\n",
            "Step 1599: loss = 0.96 (5.975 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 25.5360\n",
            "Dev accu = 0.5200 104/200\n",
            "Duration 0.843 sec\n",
            "(step + 1) == max_steps 1600 3200\n",
            "1600 \n",
            "Step 1663: loss = 0.96 (5.895 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 25.1936\n",
            "Dev accu = 0.5450 109/200\n",
            "Saving weights, ACCU 0.525 (prev_best) < 0.545 (cur)\n",
            "Duration 2.009 sec\n",
            "(step + 1) == max_steps 1664 3200\n",
            "1700 \n",
            "Step 1727: loss = 0.95 (7.164 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 25.1929\n",
            "Dev accu = 0.5350 107/200\n",
            "Duration 0.842 sec\n",
            "(step + 1) == max_steps 1728 3200\n",
            "\n",
            "Step 1791: loss = 0.93 (5.899 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 25.6874\n",
            "Dev accu = 0.5400 108/200\n",
            "Duration 0.874 sec\n",
            "(step + 1) == max_steps 1792 3200\n",
            "1800 \n",
            "Step 1855: loss = 0.97 (6.431 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 25.0377\n",
            "Dev accu = 0.5350 107/200\n",
            "Duration 0.921 sec\n",
            "(step + 1) == max_steps 1856 3200\n",
            "1900 \n",
            "Step 1919: loss = 0.93 (6.002 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 24.1483\n",
            "Dev accu = 0.5800 116/200\n",
            "Saving weights, ACCU 0.545 (prev_best) < 0.58 (cur)\n",
            "Duration 1.984 sec\n",
            "(step + 1) == max_steps 1920 3200\n",
            "\n",
            "Step 1983: loss = 0.91 (7.105 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 24.1420\n",
            "Dev accu = 0.6050 121/200\n",
            "Saving weights, ACCU 0.58 (prev_best) < 0.605 (cur)\n",
            "Duration 1.996 sec\n",
            "(step + 1) == max_steps 1984 3200\n",
            "2000 \n",
            "Step 2047: loss = 0.89 (7.170 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 24.2618\n",
            "Dev accu = 0.5850 117/200\n",
            "Duration 0.849 sec\n",
            "(step + 1) == max_steps 2048 3200\n",
            "2100 \n",
            "Step 2111: loss = 0.88 (5.901 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 23.7458\n",
            "Dev accu = 0.5900 118/200\n",
            "Duration 0.844 sec\n",
            "(step + 1) == max_steps 2112 3200\n",
            "\n",
            "Step 2175: loss = 0.86 (6.221 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 23.5855\n",
            "Dev accu = 0.6050 121/200\n",
            "Duration 0.927 sec\n",
            "(step + 1) == max_steps 2176 3200\n",
            "2200 \n",
            "Step 2239: loss = 0.85 (6.423 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 23.4965\n",
            "Dev accu = 0.6050 121/200\n",
            "Duration 0.854 sec\n",
            "(step + 1) == max_steps 2240 3200\n",
            "2300 \n",
            "Step 2303: loss = 0.84 (5.912 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 23.4289\n",
            "Dev accu = 0.5950 119/200\n",
            "Duration 0.867 sec\n",
            "(step + 1) == max_steps 2304 3200\n",
            "\n",
            "Step 2367: loss = 0.83 (5.872 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 23.5392\n",
            "Dev accu = 0.5950 119/200\n",
            "Duration 0.845 sec\n",
            "(step + 1) == max_steps 2368 3200\n",
            "2400 \n",
            "Step 2431: loss = 0.83 (5.871 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 22.9232\n",
            "Dev accu = 0.6050 121/200\n",
            "Duration 0.850 sec\n",
            "(step + 1) == max_steps 2432 3200\n",
            "\n",
            "Step 2495: loss = 0.83 (5.909 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 23.1744\n",
            "Dev accu = 0.6100 122/200\n",
            "Saving weights, ACCU 0.605 (prev_best) < 0.61 (cur)\n",
            "Duration 2.392 sec\n",
            "(step + 1) == max_steps 2496 3200\n",
            "2500 \n",
            "Step 2559: loss = 0.83 (7.548 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 22.9477\n",
            "Dev accu = 0.6000 120/200\n",
            "Duration 0.830 sec\n",
            "(step + 1) == max_steps 2560 3200\n",
            "2600 \n",
            "Step 2623: loss = 0.82 (5.920 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 23.3199\n",
            "Dev accu = 0.6150 123/200\n",
            "Saving weights, ACCU 0.61 (prev_best) < 0.615 (cur)\n",
            "Duration 2.004 sec\n",
            "(step + 1) == max_steps 2624 3200\n",
            "\n",
            "Step 2687: loss = 0.81 (6.966 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 22.8161\n",
            "Dev accu = 0.6100 122/200\n",
            "Duration 0.839 sec\n",
            "(step + 1) == max_steps 2688 3200\n",
            "2700 \n",
            "Step 2751: loss = 0.81 (5.856 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 23.4369\n",
            "Dev accu = 0.6000 120/200\n",
            "Duration 0.856 sec\n",
            "(step + 1) == max_steps 2752 3200\n",
            "2800 \n",
            "Step 2815: loss = 0.80 (5.840 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 22.8448\n",
            "Dev accu = 0.6000 120/200\n",
            "Duration 0.833 sec\n",
            "(step + 1) == max_steps 2816 3200\n",
            "\n",
            "Step 2879: loss = 0.80 (5.834 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 23.3396\n",
            "Dev accu = 0.5800 116/200\n",
            "Duration 0.849 sec\n",
            "(step + 1) == max_steps 2880 3200\n",
            "2900 \n",
            "Step 2943: loss = 0.82 (5.864 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 22.6448\n",
            "Dev accu = 0.6150 123/200\n",
            "Duration 0.876 sec\n",
            "(step + 1) == max_steps 2944 3200\n",
            "3000 \n",
            "Step 3007: loss = 0.79 (6.194 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 22.2368\n",
            "Dev accu = 0.6250 125/200\n",
            "Saving weights, ACCU 0.615 (prev_best) < 0.625 (cur)\n",
            "Duration 2.151 sec\n",
            "(step + 1) == max_steps 3008 3200\n",
            "\n",
            "Step 3071: loss = 0.77 (7.719 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 22.6195\n",
            "Dev accu = 0.5800 116/200\n",
            "Duration 0.830 sec\n",
            "(step + 1) == max_steps 3072 3200\n",
            "3100 \n",
            "Step 3135: loss = 0.79 (5.863 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 22.0723\n",
            "Dev accu = 0.6250 125/200\n",
            "Duration 0.837 sec\n",
            "(step + 1) == max_steps 3136 3200\n",
            "\n",
            "Step 3199: loss = 0.77 (5.851 sec)\n",
            "Validation Data Eval:\n",
            "Dev loss = 22.0481\n",
            "Dev accu = 0.6150 123/200\n",
            "Duration 0.844 sec\n",
            "(step + 1) == max_steps 3200 3200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "DwGtIwQ9T0P1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9SZE1UAxT3Ts",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ENlmxvTNB05A",
        "colab_type": "code",
        "outputId": "81037494-7a10-478c-ca2d-f023e992c3e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--model_prefix', type=str,default='/content/logs/G2S.cross_mul_0', help='Prefix to the models.')\n",
        "    parser.add_argument('--in_path', type=str,  help='The path to the test file.',default='/content/drive/My Drive/nary-grn-master/gs_lstm/data/test_list_0')\n",
        "    parser.add_argument('--out_path', type=str, help='The path to the test file.',default='/content/logs/results_c_b_0.json')\n",
        "\n",
        "    args, unparsed = parser.parse_known_args()\n",
        "\n",
        "    model_prefix = args.model_prefix\n",
        "    in_path = args.in_path\n",
        "    out_path = args.out_path\n",
        "\n",
        "    #os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
        "    #os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "    #print(\"CUDA_VISIBLE_DEVICES \" + os.environ['CUDA_VISIBLE_DEVICES'])\n",
        "\n",
        "    # load the configuration file\n",
        "    print('Loading configurations from ' + model_prefix + \".config.json\")\n",
        "    FLAGS = load_namespace(model_prefix + \".config.json\")\n",
        "    FLAGS = enrich_options(FLAGS)\n",
        "\n",
        "    # load vocabs\n",
        "    print('Loading vocabs.')\n",
        "    word_vocab = Vocab(FLAGS.word_vec_path, fileformat='txt2')\n",
        "    print('word_vocab: {}'.format(word_vocab.word_vecs.shape))\n",
        "    edgelabel_vocab = Vocab(model_prefix + \".edgelabel_vocab\", fileformat='txt2')\n",
        "    print('edgelabel_vocab: {}'.format(edgelabel_vocab.word_vecs.shape))\n",
        "    char_vocab = None\n",
        "    if FLAGS.with_char:\n",
        "        char_vocab = Vocab(model_prefix + \".char_vocab\", fileformat='txt2')\n",
        "        print('char_vocab: {}'.format(char_vocab.word_vecs.shape))\n",
        "\n",
        "    print('Loading test set from {}.'.format(in_path))\n",
        "    if FLAGS.infile_format == 'fof':\n",
        "        testset, _, _, _, _ = read_nary_from_fof(in_path, FLAGS)\n",
        "    else:\n",
        "        testset, _, _, _, _ = read_nary_file(in_path, FLAGS)\n",
        "    print('Number of samples: {}'.format(len(testset)))\n",
        "\n",
        "    print('Build DataStream ... ')\n",
        "    batch_size=-1\n",
        "    devDataStream = G2SDataStream(testset, word_vocab, char_vocab, edgelabel_vocab, options=FLAGS,\n",
        "                 isShuffle=False, isLoop=False, isSort=False, batch_size=batch_size)\n",
        "    print('Number of instances in testDataStream: {}'.format(devDataStream.get_num_instance()))\n",
        "    print('Number of batches in testDataStream: {}'.format(devDataStream.get_num_batch()))\n",
        "\n",
        "    best_path = model_prefix + \".best.model\"\n",
        "    with tf.Graph().as_default():\n",
        "        initializer = tf.random_uniform_initializer(-0.01, 0.01)\n",
        "        with tf.name_scope(\"Valid\"):\n",
        "            with tf.variable_scope(\"Model\", reuse=False, initializer=initializer):\n",
        "                valid_graph = ModelGraph(word_vocab=word_vocab, char_vocab=char_vocab, Edgelabel_vocab=edgelabel_vocab,\n",
        "                                         options=FLAGS, mode=\"evaluate\")\n",
        "\n",
        "        ## remove word _embedding\n",
        "        vars_ = {}\n",
        "        for var in tf.all_variables():\n",
        "            if \"word_embedding\" in var.name: continue\n",
        "            if not var.name.startswith(\"Model\"): continue\n",
        "            vars_[var.name.split(\":\")[0]] = var\n",
        "        saver = tf.train.Saver(vars_)\n",
        "\n",
        "        initializer = tf.global_variables_initializer()\n",
        "        sess = tf.Session()\n",
        "        sess.run(initializer)\n",
        "\n",
        "        saver.restore(sess, best_path) # restore the model\n",
        "\n",
        "        devDataStream.reset()\n",
        "        instances = []\n",
        "        outputs = []\n",
        "        test_loss = 0.0\n",
        "        test_right = 0.0\n",
        "        test_total = 0.0\n",
        "        start_time = time.time()\n",
        "        for batch_index in xrange(devDataStream.get_num_batch()): # for each batch\n",
        "            cur_batch = devDataStream.get_batch(batch_index)\n",
        "            accu_value, loss_value, output_value = valid_graph.execute(sess, cur_batch, FLAGS, is_train=False)\n",
        "            instances += cur_batch.instances\n",
        "            outputs += output_value.flatten().tolist()\n",
        "            test_loss += loss_value\n",
        "            test_right += accu_value\n",
        "            test_total += cur_batch.batch_size\n",
        "        duration = time.time() - start_time\n",
        "        print('Decoding time %.3f sec' % (duration))\n",
        "\n",
        "        assert len(instances) == len(outputs)\n",
        "        json.dump((instances,outputs,testset), open(out_path,'w'))\n",
        "\n",
        "        print('Test accu {}, right {}, total {}'.format(1.0*test_right/test_total, test_right, test_total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading configurations from /content/logs/G2S.cross_mul_0.config.json\n",
            "Loading vocabs.\n",
            "word_vocab: (15736, 100)\n",
            "edgelabel_vocab: (111, 3)\n",
            "Loading test set from /content/drive/My Drive/nary-grn-master/gs_lstm/data/test_list_0.\n",
            "/content/drive/My Drive/nary-grn-master/peng_data/data/drug_gene_var/0/data_graph_1\n",
            "Number of samples: 758\n",
            "Build DataStream ... \n",
            "Number of instances in testDataStream: 758\n",
            "Number of batches in testDataStream: 95\n",
            "Return from here, just evaluate\n",
            "Decoding time 2.938 sec\n",
            "Test accu 0.627968337731, right 476.0, total 758.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KxxStB04KTVp",
        "colab_type": "code",
        "outputId": "7dc84e01-9064-479e-8f38-534537739a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "cell_type": "code",
      "source": [
        "#import abc\n",
        "import RelationExtraction\n",
        "\n",
        "#class RelationExtraction1(RelationExtraction):\n",
        "class RelationExtraction1:\n",
        "  cc = SmoothingFunction()\n",
        "  FLAGS = None\n",
        "\n",
        "  def get_machine_name():\n",
        "      return platform.node()\n",
        "\n",
        "  def vec2string(val):\n",
        "      result = \"\"\n",
        "      for v in val:\n",
        "          result += \" {}\".format(v)\n",
        "      return result.strip()\n",
        "\n",
        "\n",
        "  def softmax(x):\n",
        "      \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "      e_x = np.exp(x - np.max(x))\n",
        "      return e_x / e_x.sum()\n",
        "\n",
        "\n",
        "  def document_bleu(vocab, gen, ref, suffix=''):\n",
        "      genlex = [vocab.getLexical(x)[1] for x in gen]\n",
        "      reflex = [[vocab.getLexical(x)[1],] for x in ref]\n",
        "      genlst = [x.split() for x in genlex]\n",
        "      reflst = [[x[0].split()] for x in reflex]\n",
        "      f = codecs.open('gen.txt'+suffix,'w','utf-8')\n",
        "      for line in genlex:\n",
        "          print(line, end='\\n', file=f)\n",
        "      f.close()\n",
        "      f = codecs.open('ref.txt'+suffix,'w','utf-8')\n",
        "      for line in reflex:\n",
        "          print(line[0], end='\\n', file=f)\n",
        "      f.close()\n",
        "      return corpus_bleu(reflst, genlst, smoothing_function=cc.method3)\n",
        "\n",
        "\n",
        "  def evaluate(sess, valid_graph, devDataStream, options=None, suffix=''):\n",
        "      devDataStream.reset()\n",
        "      instances = []\n",
        "      outputs = []\n",
        "      dev_loss = 0.0\n",
        "      dev_right = 0.0\n",
        "      dev_total = 0.0\n",
        "      for batch_index in xrange(devDataStream.get_num_batch()): # for each batch\n",
        "          cur_batch = devDataStream.get_batch(batch_index)\n",
        "          accu_value, loss_value, output_value = valid_graph.execute(sess, cur_batch, options, is_train=False)\n",
        "          instances += cur_batch.instances\n",
        "          outputs += output_value.flatten().tolist()\n",
        "          dev_loss += loss_value\n",
        "          dev_right += accu_value\n",
        "          dev_total += cur_batch.batch_size\n",
        "\n",
        "      return {'dev_loss':dev_loss, 'dev_accu':1.0*dev_right/dev_total, 'dev_right':dev_right, 'dev_total':dev_total, 'data':(instances,outputs)}\n",
        "\n",
        "\n",
        "  def main(_):\n",
        "      print('Configurations:')\n",
        "      print(FLAGS)\n",
        "\n",
        "      log_dir = FLAGS.model_dir\n",
        "      if not os.path.exists(log_dir):\n",
        "          os.makedirs(log_dir)\n",
        "\n",
        "      path_prefix = log_dir + \"/G2S.{}\".format(FLAGS.suffix)\n",
        "      log_file_path = path_prefix + \".log\"\n",
        "      print('Log file path: {}'.format(log_file_path))\n",
        "      log_file = open(log_file_path, 'wt')\n",
        "      log_file.write(\"{}\\n\".format(FLAGS))\n",
        "      log_file.flush()\n",
        "\n",
        "      # save configuration\n",
        "      save_namespace(FLAGS, path_prefix + \".config.json\")\n",
        "\n",
        "      print('Loading train set.')\n",
        "      if FLAGS.infile_format == 'fof':\n",
        "          trainset, trn_node, trn_in_neigh, trn_out_neigh, trn_sent = read_nary_from_fof(FLAGS.train_path, FLAGS)\n",
        "      else:\n",
        "          trainset, trn_node, trn_in_neigh, trn_out_neigh, trn_sent = read_nary_file(FLAGS.train_path, FLAGS)\n",
        "\n",
        "      random.shuffle(trainset)\n",
        "      devset = trainset[:200]\n",
        "      trainset = trainset[200:]\n",
        "\n",
        "      print('Number of training samples: {}'.format(len(trainset)))\n",
        "      print('Number of dev samples: {}'.format(len(devset)))\n",
        "\n",
        "      max_node = trn_node\n",
        "      max_in_neigh = trn_in_neigh\n",
        "      max_out_neigh = trn_out_neigh\n",
        "      max_sent = trn_sent\n",
        "      print('Max node number: {}, while max allowed is {}'.format(max_node, FLAGS.max_node_num))\n",
        "      print('Max parent number: {}, truncated to {}'.format(max_in_neigh, FLAGS.max_in_neigh_num))\n",
        "      print('Max children number: {}, truncated to {}'.format(max_out_neigh, FLAGS.max_out_neigh_num))\n",
        "      print('Max entity size: {}, truncated to {}'.format(max_sent, FLAGS.max_entity_size))\n",
        "\n",
        "      word_vocab = None\n",
        "      char_vocab = None\n",
        "      edgelabel_vocab = None\n",
        "      has_pretrained_model = False\n",
        "      best_path = path_prefix + \".best.model\"\n",
        "      if os.path.exists(best_path + \".index\"):\n",
        "          has_pretrained_model = True\n",
        "          print('!!Existing pretrained model. Loading vocabs.')\n",
        "          word_vocab = Vocab(FLAGS.word_vec_path, fileformat='txt2')\n",
        "          print('word_vocab: {}'.format(word_vocab.word_vecs.shape))\n",
        "          char_vocab = None\n",
        "          if FLAGS.with_char:\n",
        "              char_vocab = Vocab(path_prefix + \".char_vocab\", fileformat='txt2')\n",
        "              print('char_vocab: {}'.format(char_vocab.word_vecs.shape))\n",
        "          edgelabel_vocab = Vocab(path_prefix + \".edgelabel_vocab\", fileformat='txt2')\n",
        "      else:\n",
        "          print('Collecting vocabs.')\n",
        "          (allWords, allChars, allEdgelabels) = collect_vocabs(trainset)\n",
        "          print('Number of words: {}'.format(len(allWords)))\n",
        "          print('Number of allChars: {}'.format(len(allChars)))\n",
        "          print('Number of allEdgelabels: {}'.format(len(allEdgelabels)))\n",
        "\n",
        "          word_vocab = Vocab(FLAGS.word_vec_path, fileformat='txt2')\n",
        "          char_vocab = None\n",
        "          if FLAGS.with_char:\n",
        "              char_vocab = Vocab(voc=allChars, dim=FLAGS.char_dim, fileformat='build')\n",
        "              char_vocab.dump_to_txt2(path_prefix + \".char_vocab\")\n",
        "          edgelabel_vocab = Vocab(voc=allEdgelabels, dim=FLAGS.edgelabel_dim, fileformat='build')\n",
        "          edgelabel_vocab.dump_to_txt2(path_prefix + \".edgelabel_vocab\")\n",
        "\n",
        "      print('word vocab size {}'.format(word_vocab.vocab_size))\n",
        "      sys.stdout.flush()\n",
        "\n",
        "      print('Build DataStream ... ')\n",
        "      trainDataStream = G2SDataStream(trainset, word_vocab, char_vocab, edgelabel_vocab, options=FLAGS,\n",
        "                   isShuffle=True, isLoop=True, isSort=False)\n",
        "\n",
        "      devDataStream = G2SDataStream(devset, word_vocab, char_vocab, edgelabel_vocab, options=FLAGS,\n",
        "                   isShuffle=False, isLoop=False, isSort=False)\n",
        "      print('Number of instances in trainDataStream: {}'.format(trainDataStream.get_num_instance()))\n",
        "      print('Number of instances in devDataStream: {}'.format(devDataStream.get_num_instance()))\n",
        "      print('Number of batches in trainDataStream: {}'.format(trainDataStream.get_num_batch()))\n",
        "      print('Number of batches in devDataStream: {}'.format(devDataStream.get_num_batch()))\n",
        "      sys.stdout.flush()\n",
        "\n",
        "      # initialize the best bleu and accu scores for current training session\n",
        "      best_accu = FLAGS.best_accu if FLAGS.__dict__.has_key('best_accu') else 0.0\n",
        "      if best_accu > 0.0:\n",
        "          print('With initial dev accuracy {}'.format(best_accu))\n",
        "\n",
        "      init_scale = 0.01\n",
        "      with tf.Graph().as_default():\n",
        "          initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
        "          with tf.name_scope(\"Train\"):\n",
        "              with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
        "                  train_graph = ModelGraph(word_vocab=word_vocab, Edgelabel_vocab=edgelabel_vocab,\n",
        "                                           char_vocab=char_vocab, options=FLAGS, mode='train')\n",
        "\n",
        "          with tf.name_scope(\"Valid\"):\n",
        "              with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
        "                  valid_graph = ModelGraph(word_vocab=word_vocab, Edgelabel_vocab=edgelabel_vocab,\n",
        "                                           char_vocab=char_vocab, options=FLAGS, mode='evaluate')\n",
        "\n",
        "          initializer = tf.global_variables_initializer()\n",
        "\n",
        "          vars_ = {}\n",
        "          for var in tf.all_variables():\n",
        "              if \"word_embedding\" in var.name: continue\n",
        "              if not var.name.startswith(\"Model\"): continue\n",
        "              vars_[var.name.split(\":\")[0]] = var\n",
        "          saver = tf.train.Saver(vars_)\n",
        "\n",
        "          sess = tf.Session()\n",
        "          sess.run(initializer)\n",
        "          if has_pretrained_model:\n",
        "              print(\"Restoring model from \" + best_path)\n",
        "              saver.restore(sess, best_path)\n",
        "              print(\"DONE!\")\n",
        "\n",
        "              if abs(best_accu) < 0.00001:\n",
        "                  print(\"Getting ACCU score for the model\")\n",
        "                  best_accu = evaluate(sess, valid_graph, devDataStream, options=FLAGS)['dev_accu']\n",
        "                  FLAGS.best_accu = best_accu\n",
        "                  save_namespace(FLAGS, path_prefix + \".config.json\")\n",
        "                  print('ACCU = %.4f' % best_accu)\n",
        "                  log_file.write('ACCU = %.4f\\n' % best_accu)\n",
        "\n",
        "          print('Start the training loop.')\n",
        "          train_size = trainDataStream.get_num_batch()\n",
        "          max_steps = train_size * FLAGS.max_epochs\n",
        "          last_step = 0\n",
        "          total_loss = 0.0\n",
        "          start_time = time.time()\n",
        "          for step in xrange(max_steps):\n",
        "              cur_batch = trainDataStream.nextBatch()\n",
        "              _, loss_value, _ = train_graph.execute(sess, cur_batch, FLAGS, is_train=True)\n",
        "              total_loss += loss_value\n",
        "\n",
        "              if step % 100==0:\n",
        "                  print('{} '.format(step), end=\"\")\n",
        "                  sys.stdout.flush()\n",
        "\n",
        "              # Save a checkpoint and evaluate the model periodically.\n",
        "              if (step + 1) % trainDataStream.get_num_batch() == 0 or (step + 1) == max_steps:\n",
        "                  print()\n",
        "                  duration = time.time() - start_time\n",
        "                  print('Step %d: loss = %.2f (%.3f sec)' % (step, total_loss/(step-last_step), duration))\n",
        "                  log_file.write('Step %d: loss = %.2f (%.3f sec)\\n' % (step, total_loss/(step-last_step), duration))\n",
        "                  sys.stdout.flush()\n",
        "                  log_file.flush()\n",
        "                  last_step = step\n",
        "                  total_loss = 0.0\n",
        "\n",
        "                  # Evaluate against the validation set.\n",
        "                  start_time = time.time()\n",
        "                  print('Validation Data Eval:')\n",
        "                  res_dict = evaluate(sess, valid_graph, devDataStream, options=FLAGS, suffix=str(step))\n",
        "                  dev_loss = res_dict['dev_loss']\n",
        "                  dev_accu = res_dict['dev_accu']\n",
        "                  dev_right = int(res_dict['dev_right'])\n",
        "                  dev_total = int(res_dict['dev_total'])\n",
        "                  print('Dev loss = %.4f' % dev_loss)\n",
        "                  log_file.write('Dev loss = %.4f\\n' % dev_loss)\n",
        "                  print('Dev accu = %.4f %d/%d' % (dev_accu, dev_right, dev_total))\n",
        "                  log_file.write('Dev accu = %.4f %d/%d\\n' % (dev_accu, dev_right, dev_total))\n",
        "                  log_file.flush()\n",
        "                  if best_accu < dev_accu:\n",
        "                      print('Saving weights, ACCU {} (prev_best) < {} (cur)'.format(best_accu, dev_accu))\n",
        "                      saver.save(sess, best_path)\n",
        "                      best_accu = dev_accu\n",
        "                      FLAGS.best_accu = dev_accu\n",
        "                      save_namespace(FLAGS, path_prefix + \".config.json\")\n",
        "                      json.dump(res_dict['data'], open(FLAGS.output_path,'w'))\n",
        "                  duration = time.time() - start_time\n",
        "                  print('Duration %.3f sec' % (duration))\n",
        "                  sys.stdout.flush()\n",
        "\n",
        "                  log_file.write('Duration %.3f sec\\n' % (duration))\n",
        "                  log_file.flush()\n",
        "                  print(\"(step + 1) == max_steps\",step+1,max_steps)\n",
        "\n",
        "      log_file.close()\n",
        "\n",
        "\n",
        "  def enrich_options(options):\n",
        "      if not options.__dict__.has_key(\"infile_format\"):\n",
        "          options.__dict__[\"infile_format\"] = \"fof\"\n",
        "\n",
        "      return options\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def predict(self):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--model_prefix', type=str,default='/content/logs/G2S.cross_mul_0', help='Prefix to the models.')\n",
        "    parser.add_argument('--in_path', type=str,  help='The path to the test file.',default='/content/drive/My Drive/nary-grn-master/gs_lstm/data/test_list_0')\n",
        "    parser.add_argument('--out_path', type=str, help='The path to the test file.',default='/content/logs/results_c_b_0.json')\n",
        "\n",
        "    args, unparsed = parser.parse_known_args()\n",
        "\n",
        "    model_prefix = args.model_prefix\n",
        "    in_path = args.in_path\n",
        "    out_path = args.out_path\n",
        "\n",
        "    #os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
        "    #os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "    #print(\"CUDA_VISIBLE_DEVICES \" + os.environ['CUDA_VISIBLE_DEVICES'])\n",
        "\n",
        "    # load the configuration file\n",
        "    print('Loading configurations from ' + model_prefix + \".config.json\")\n",
        "    FLAGS = load_namespace(model_prefix + \".config.json\")\n",
        "    FLAGS = enrich_options(FLAGS)\n",
        "\n",
        "    # load vocabs\n",
        "    print('Loading vocabs.')\n",
        "    word_vocab = Vocab(FLAGS.word_vec_path, fileformat='txt2')\n",
        "    print('word_vocab: {}'.format(word_vocab.word_vecs.shape))\n",
        "    edgelabel_vocab = Vocab(model_prefix + \".edgelabel_vocab\", fileformat='txt2')\n",
        "    print('edgelabel_vocab: {}'.format(edgelabel_vocab.word_vecs.shape))\n",
        "    char_vocab = None\n",
        "    if FLAGS.with_char:\n",
        "        char_vocab = Vocab(model_prefix + \".char_vocab\", fileformat='txt2')\n",
        "        print('char_vocab: {}'.format(char_vocab.word_vecs.shape))\n",
        "\n",
        "    print('Loading test set from {}.'.format(in_path))\n",
        "    if FLAGS.infile_format == 'fof':\n",
        "        testset, _, _, _, _ = read_nary_from_fof(in_path, FLAGS)\n",
        "    else:\n",
        "        testset, _, _, _, _ = read_nary_file(in_path, FLAGS)\n",
        "    print('Number of samples: {}'.format(len(testset)))\n",
        "\n",
        "    print('Build DataStream ... ')\n",
        "    batch_size=-1\n",
        "    devDataStream = G2SDataStream(testset, word_vocab, char_vocab, edgelabel_vocab, options=FLAGS,\n",
        "                 isShuffle=False, isLoop=False, isSort=False, batch_size=batch_size)\n",
        "    print('Number of instances in testDataStream: {}'.format(devDataStream.get_num_instance()))\n",
        "    print('Number of batches in testDataStream: {}'.format(devDataStream.get_num_batch()))\n",
        "\n",
        "    best_path = model_prefix + \".best.model\"\n",
        "    with tf.Graph().as_default():\n",
        "        initializer = tf.random_uniform_initializer(-0.01, 0.01)\n",
        "        with tf.name_scope(\"Valid\"):\n",
        "            with tf.variable_scope(\"Model\", reuse=False, initializer=initializer):\n",
        "                valid_graph = ModelGraph(word_vocab=word_vocab, char_vocab=char_vocab, Edgelabel_vocab=edgelabel_vocab,\n",
        "                                         options=FLAGS, mode=\"evaluate\")\n",
        "\n",
        "        ## remove word _embedding\n",
        "        vars_ = {}\n",
        "        for var in tf.all_variables():\n",
        "            if \"word_embedding\" in var.name: continue\n",
        "            if not var.name.startswith(\"Model\"): continue\n",
        "            vars_[var.name.split(\":\")[0]] = var\n",
        "        saver = tf.train.Saver(vars_)\n",
        "\n",
        "        initializer = tf.global_variables_initializer()\n",
        "        sess = tf.Session()\n",
        "        sess.run(initializer)\n",
        "\n",
        "        saver.restore(sess, best_path) # restore the model\n",
        "\n",
        "        devDataStream.reset()\n",
        "        instances = []\n",
        "        outputs = []\n",
        "        test_loss = 0.0\n",
        "        test_right = 0.0\n",
        "        test_total = 0.0\n",
        "        start_time = time.time()\n",
        "        for batch_index in xrange(devDataStream.get_num_batch()): # for each batch\n",
        "            cur_batch = devDataStream.get_batch(batch_index)\n",
        "            accu_value, loss_value, output_value = valid_graph.execute(sess, cur_batch, FLAGS, is_train=False)\n",
        "            instances += cur_batch.instances\n",
        "            outputs += output_value.flatten().tolist()\n",
        "            test_loss += loss_value\n",
        "            test_right += accu_value\n",
        "            test_total += cur_batch.batch_size\n",
        "        duration = time.time() - start_time\n",
        "        print('Decoding time %.3f sec' % (duration))\n",
        "\n",
        "        assert len(instances) == len(outputs)\n",
        "        json.dump((instances,outputs,testset), open(out_path,'w'))\n",
        "\n",
        "        print('Test accu {}, right {}, total {}'.format(1.0*test_right/test_total, test_right, test_total))\n",
        "    pass\n",
        "  \n",
        "  def train(self):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config_path', type=str,nargs='?',default ='/content/drive/My Drive/nary-grn-master/gs_lstm/config.json', help='Configuration file.')\n",
        "    \n",
        "    #os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
        "    #os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
        "    \n",
        "    #print(\"CUDA_VISIBLE_DEVICES \" + os.environ['CUDA_VISIBLE_DEVICES'])\n",
        "    FLAGS, unparsed = parser.parse_known_args()\n",
        "\n",
        "    print(FLAGS.config_path)\n",
        "    unparsed=None\n",
        "    if FLAGS.config_path is not None:\n",
        "        print('Loading the configuration from ' + FLAGS.config_path)\n",
        "        FLAGS = load_namespace(FLAGS.config_path)\n",
        "    \n",
        "    \n",
        "    FLAGS = enrich_options(FLAGS)\n",
        "    \n",
        "\n",
        "    sys.stdout.flush()\n",
        "    tf.app.run()\n",
        "    #If running through command line\n",
        "    #tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n",
        "if __name__ == '__main__': \n",
        "  x=RelationExtraction1()\n",
        "  x.train()\n",
        "  x.predict()\n",
        "    "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/nary-grn-master/gs_lstm/config.json\n",
            "Loading the configuration from /content/drive/My Drive/nary-grn-master/gs_lstm/config.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-63531ed47826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m   \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRelationExtraction1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m   \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m   \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-63531ed47826>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menrich_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: global name 'enrich_options' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "2_dqv7g0T5la",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "773b8b8c-ea9e-44b3-f532-79fd73888d68"
      },
      "cell_type": "code",
      "source": [
        "x=RelationExtraction1()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading configurations from /content/logs/G2S.cross_mul_0.config.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IOError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-1c38675de95f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRelationExtraction1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-c322a19fb1e9>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# load the configuration file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading configurations from '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".config.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".config.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menrich_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-303c5b563508>\u001b[0m in \u001b[0;36mload_namespace\u001b[0;34m(in_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mFLAGS_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/content/logs/G2S.cross_mul_0.config.json'"
          ]
        }
      ]
    }
  ]
}